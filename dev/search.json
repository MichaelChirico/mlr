[{"path":"https://mlr.mlr-org.com/dev/ISSUE_TEMPLATE.html","id":"bug-report","dir":"","previous_headings":"","what":"Bug report","title":"NA","text":"Start new R session Install latest version mlr: update.packages(oldPkgs=\"mlr\", ask=FALSE) use GitHub install mlr: devtools::install_github(c(\"BBmisc\", \"ParamHelpers\", \"mlr\")) run sessionInfo() Give minimal reproducible example","code":""},{"path":"https://mlr.mlr-org.com/dev/ISSUE_TEMPLATE.html","id":"writing-a-good-bug-report","dir":"","previous_headings":"","what":"Writing a good bug report","title":"NA","text":"[ Slightly adapted version () [https://www.r-project.org/bugs.html#writing--good-bug-report] ] Bug reports include way reproducing bug. simple possible. person trying fix bug can’t work make appear, jump lot unnecessary hoops make appear, ’re going waste lot time. mlr maintained number people, ’s best make sure bug report clear well-written. ’s , suck energy maintainers take longer bug get fixed - may end getting handled . particular, : Write clear unique summary bug. “Running model data set constant feature causes following exception” good; “software crashes” . Include, description, steps reproduce bug mentioned . Focus facts happened, course helps can already give us (informed!) guesses bug comes .","code":""},{"path":"https://mlr.mlr-org.com/dev/ISSUE_TEMPLATE.html","id":"minimal-reproducible-example","dir":"","previous_headings":"","what":"Minimal reproducible example","title":"NA","text":"minimal reproducible example consists following items: minimal dataset, necessary reproduce error minimal runnable code necessary reproduce error, can run given dataset. necessary information used packages, R version system run . case random processes, seed (set set.seed()) reproducibility want learn , read https://stackoverflow.com/questions/5963269/--make--great-r-reproducible-example]","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/advanced_tune.html","id":"tuning-across-whole-model-spaces-with-modelmultiplexer","dir":"Articles > Tutorial","previous_headings":"","what":"Tuning across whole model spaces with ModelMultiplexer","title":"Iterated F-Racing for mixed spaces and dependencies","text":"can now take following example even one step . use makeModelMultiplexer() can tune different model classes , just SVM kernels . Function makeModelMultiplexerParamSet() offers simple way construct parameter set tuning: parameter names prefixed automatically requires element set, , make parameters subordinate selected.learner.","code":"base.learners = list(   makeLearner(\"classif.ksvm\"),   makeLearner(\"classif.randomForest\") ) lrn = makeModelMultiplexer(base.learners) ps = makeModelMultiplexerParamSet(lrn,   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x),   makeIntegerParam(\"ntree\", lower = 1L, upper = 500L) ) print(ps) ##                                Type len Def                            Constr ## selected.learner           discrete   -   - classif.ksvm,classif.randomForest ## classif.ksvm.sigma          numeric   -   -                         -12 to 12 ## classif.randomForest.ntree  integer   -   -                          1 to 500 ##                            Req Tunable Trafo ## selected.learner             -    TRUE     - ## classif.ksvm.sigma           Y    TRUE     Y ## classif.randomForest.ntree   Y    TRUE     -  rdesc = makeResampleDesc(\"CV\", iters = 2L) ctrl = makeTuneControlIrace(maxExperiments = 200L) res = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl,   show.info = FALSE) df = as.data.frame(res$opt.path) print(head(df[, -ncol(df)])) ##       selected.learner classif.ksvm.sigma classif.randomForest.ntree ## 1         classif.ksvm           5.459183                         NA ## 2         classif.ksvm         -11.194386                         NA ## 3         classif.ksvm           9.999280                         NA ## 4 classif.randomForest                 NA                        383 ## 5 classif.randomForest                 NA                        248 ## 6         classif.ksvm          -9.148848                         NA ##   mmce.test.mean dob eol error.message ## 1     0.52000000   1  NA          <NA> ## 2     0.70000000   1  NA          <NA> ## 3     0.70000000   1  NA          <NA> ## 4     0.07333333   1  NA          <NA> ## 5     0.06666667   1  NA          <NA> ## 6     0.53333333   1  NA          <NA>"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/advanced_tune.html","id":"multi-criteria-evaluation-and-optimization","dir":"Articles > Tutorial","previous_headings":"","what":"Multi-criteria evaluation and optimization","title":"Iterated F-Racing for mixed spaces and dependencies","text":"tuning might want optimize multiple, potentially conflicting, performance measures simultaneously. following example aim minimize , false positive false negative rates (fpr fnr). tune hyperparameters SVM (function kernlab::ksvm()) radial basis kernel use sonar.task() illustration. search strategy choose random search. available multi-criteria tuning algorithms see TuneMultiCritControl(). results can visualized function plotTuneMultiCritResult(). plot shows false positive false negative rates parameter settings evaluated tuning. Points Pareto front slightly increased.","code":"ps = makeParamSet(   makeNumericParam(\"C\", lower = -12, upper = 12, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x) ) ctrl = makeTuneMultiCritControlRandom(maxit = 30L) rdesc = makeResampleDesc(\"Holdout\") res = tuneParamsMultiCrit(\"classif.ksvm\", task = sonar.task,    resampling = rdesc, par.set = ps,   measures = list(fpr, fnr), control = ctrl, show.info = FALSE) res ## Tune multicrit result: ## Points on front: 5  print(head(df[, -ncol(df)])) ##       selected.learner classif.ksvm.sigma classif.randomForest.ntree ## 1         classif.ksvm           5.459183                         NA ## 2         classif.ksvm         -11.194386                         NA ## 3         classif.ksvm           9.999280                         NA ## 4 classif.randomForest                 NA                        383 ## 5 classif.randomForest                 NA                        248 ## 6         classif.ksvm          -9.148848                         NA ##   mmce.test.mean dob eol error.message ## 1     0.52000000   1  NA          <NA> ## 2     0.70000000   1  NA          <NA> ## 3     0.70000000   1  NA          <NA> ## 4     0.07333333   1  NA          <NA> ## 5     0.06666667   1  NA          <NA> ## 6     0.53333333   1  NA          <NA> plotTuneMultiCritResult(res)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/bagging.html","id":"changing-the-type-of-prediction","dir":"Articles > Tutorial","previous_headings":"","what":"Changing the type of prediction","title":"Generic Bagging","text":"case classification problem predicted class labels determined majority voting predictions individual models. Additionally, posterior probabilities can estimated relative proportions predicted class labels. purpose change predict type bagging learner follows. Note relevant base learner can predict probabilities reason predict type base learner always \"response\". regression mean value across predictions computed. Moreover, standard deviation across predictions estimated predict type bagging learner changed \"se\". , give small example regression. function getLearnerModel(), can access models fitted individual iterations. Predict response calculate standard deviation: column labelled se standard deviation prediction given. Let’s visualise bit using ggplot2::ggplot2(). plot percentage lower status population (lstat) prediction.","code":"bag.lrn = setPredictType(bag.lrn, predict.type = \"prob\") n = getTaskSize(bh.task) train.inds = seq(1, n, 3) test.inds  = setdiff(1:n, train.inds) lrn = makeLearner(\"regr.rpart\") bag.lrn = makeBaggingWrapper(lrn) bag.lrn = setPredictType(bag.lrn, predict.type = \"se\") mod = train(learner = bag.lrn, task = bh.task, subset = train.inds) head(getLearnerModel(mod), 2) ## [[1]] ## Model for learner.id=regr.rpart; learner.class=regr.rpart ## Trained on: task.id = BostonHousing-example; obs = 169; features = 13 ## Hyperparameters: xval=0 ##  ## [[2]] ## Model for learner.id=regr.rpart; learner.class=regr.rpart ## Trained on: task.id = BostonHousing-example; obs = 169; features = 13 ## Hyperparameters: xval=0 pred = predict(mod, task = bh.task, subset = test.inds) head(as.data.frame(pred)) ##   id truth response       se ## 2  2  21.6 21.53997 2.727530 ## 3  3  34.7 34.54394 3.524780 ## 5  5  36.2 33.47336 1.704140 ## 6  6  28.7 23.79443 1.282300 ## 8  8  27.1 14.60342 2.269138 ## 9  9  16.5 13.67014 3.147195 library(\"ggplot2\") library(\"reshape2\") data = cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds)) g = ggplot(data, aes(x = lstat, y = response, ymin = response - se,   ymax = response + se, col = age)) g + geom_point() + geom_linerange(alpha = 0.5)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"conducting-benchmark-experiments","dir":"Articles > Tutorial","previous_headings":"","what":"Conducting benchmark experiments","title":"Benchmark Experiments","text":"start small example. Two learners, MASS::lda() rpart::rpart(), applied one classification problem (sonar.task()). resampling strategy choose \"Holdout\". performance thus calculated single randomly sampled test data set. example create resample description (makeResampleDesc()), automatically instantiated benchmark(). instantiation done per Task(), .e., training test sets used learners. also possible directly pass makeResampleInstance(). like use fixed test data set instead randomly selected one, can create suitable makeResampleInstance() function makeFixedHoldoutInstance(). convenience, don’t want pass additional arguments makeLearner(), don’t need generate makeLearner()s explicitly, ’s sufficient provide learner name. example also written: also need set models = TRUE explicitly want take look later. default, models stored benchmark result object. printed summary table every row corresponds one pair Task() makeLearner(). entries show mean misclassification error, default performance measure classification, test data set. result bmr object class BenchmarkResult(). Basically, contains base::list() lists ResampleResult() objects, first ordered Task() makeLearner().","code":"# Two learners to be compared lrns = list(makeLearner(\"classif.lda\"), makeLearner(\"classif.rpart\"))  # Choose the resampling strategy rdesc = makeResampleDesc(\"Holdout\")  # Conduct the benchmark experiment bmr = benchmark(lrns, sonar.task, rdesc) ## Task: Sonar-example, Learner: classif.lda ## Resampling: holdout ## Measures:             mmce ## [Resample] iter 1:    0.3142857 ##  ## Aggregated Result: mmce.test.mean=0.3142857 ##  ## Task: Sonar-example, Learner: classif.rpart ## Resampling: holdout ## Measures:             mmce ## [Resample] iter 1:    0.3285714 ##  ## Aggregated Result: mmce.test.mean=0.3285714 ##   bmr ##         task.id    learner.id mmce.test.mean ## 1 Sonar-example   classif.lda      0.3142857 ## 2 Sonar-example classif.rpart      0.3285714 # Vector of strings lrns = c(\"classif.lda\", \"classif.rpart\")  # A mixed list of Learner objects and strings works, too lrns = list(makeLearner(\"classif.lda\", predict.type = \"prob\"), \"classif.rpart\")  bmr = benchmark(lrns, sonar.task, rdesc, models = TRUE) ## Task: Sonar-example, Learner: classif.lda ## Resampling: holdout ## Measures:             mmce ## [Resample] iter 1:    0.3285714 ##  ## Aggregated Result: mmce.test.mean=0.3285714 ##  ## Task: Sonar-example, Learner: classif.rpart ## Resampling: holdout ## Measures:             mmce ## [Resample] iter 1:    0.3714286 ##  ## Aggregated Result: mmce.test.mean=0.3714286 ##   bmr ##         task.id    learner.id mmce.test.mean ## 1 Sonar-example   classif.lda      0.3285714 ## 2 Sonar-example classif.rpart      0.3714286"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"making-experiments-reproducible","dir":"Articles > Tutorial","previous_headings":"","what":"Making experiments reproducible","title":"Benchmark Experiments","text":"Typically, want experiment results reproducible. mlr obeys set.seed function, make sure use set.seed beginning script like results reproducible. Note using parallel computing, may need adjust call set.seed depending usecase. One possibility use set.seed(123, \"L'Ecuyer\") order ensure results reproducible child process. See examples parallel::mclapply() information reproducibility parallel computing.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"accessing-benchmark-results","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing benchmark results","title":"Benchmark Experiments","text":"mlr provides several accessor functions, named getBMR<WhatToExtract>, permit retrieve information analyses. includes example performances predictions learning algorithms consideration.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"learner-performances","dir":"Articles > Tutorial","previous_headings":"","what":"Learner performances","title":"Benchmark Experiments","text":"Let’s look benchmark result . getBMRPerformances() returns individual performances resampling runs, getBMRAggrPerformances() gives aggregated values. Since used holdout resampling strategy, individual aggregated performance values coincide. default, nearly “getter” functions return nested base::list(), first level indicating task second level indicating learner. single learner , case single task considered, setting drop = TRUE simplifies result flat base::list(). Often convenient work base::data.frame()s. can easily convert result structure setting .df = TRUE.","code":"getBMRPerformances(bmr) ## $`Sonar-example` ## $`Sonar-example`$classif.lda ##   iter      mmce ## 1    1 0.3285714 ##  ## $`Sonar-example`$classif.rpart ##   iter      mmce ## 1    1 0.3714286  getBMRAggrPerformances(bmr) ## $`Sonar-example` ## $`Sonar-example`$classif.lda ## mmce.test.mean  ##      0.3285714  ##  ## $`Sonar-example`$classif.rpart ## mmce.test.mean  ##      0.3714286 getBMRPerformances(bmr, drop = TRUE) ## $classif.lda ##   iter      mmce ## 1    1 0.3285714 ##  ## $classif.rpart ##   iter      mmce ## 1    1 0.3714286 getBMRPerformances(bmr, as.df = TRUE) ##         task.id    learner.id iter      mmce ## 1 Sonar-example   classif.lda    1 0.3285714 ## 2 Sonar-example classif.rpart    1 0.3714286  getBMRAggrPerformances(bmr, as.df = TRUE) ##         task.id    learner.id mmce.test.mean ## 1 Sonar-example   classif.lda      0.3285714 ## 2 Sonar-example classif.rpart      0.3714286"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"predictions","dir":"Articles > Tutorial","previous_headings":"","what":"Predictions","title":"Benchmark Experiments","text":"Per default, BenchmarkResult() contains learner predictions. want keep , e.g., conserve memory, set keep.pred = FALSE calling benchmark(). can access predictions using function getBMRPredictions(). Per default, get nested base::list() ResamplePrediction()objects. , can use drop .df options simplify result. also easily possible access results certain learners tasks via IDs. purpose many “getter” functions learner.ids task.ids argument. don’t like default IDs, can set IDs learners tasks via id option makeLearner() makeTask(). Moreover, can conveniently change ID makeLearner() via function setLearnerid().","code":"getBMRPredictions(bmr)  head(getBMRPredictions(bmr, as.df = TRUE)) getBMRPredictions(bmr)  ## $`Sonar-example` ## $`Sonar-example`$classif.lda ## Resampled Prediction for: ## Resample description: holdout with 0.67 split rate. ## Predict: test ## Stratification: FALSE ## predict.type: prob ## threshold: M=0.50,R=0.50 ## time (mean): 0.00 ##    id truth      prob.M       prob.R response iter  set ## 1 194     M 0.001914379 9.980856e-01        R    1 test ## 2  18     R 0.474564117 5.254359e-01        R    1 test ## 3 196     M 0.996712551 3.287449e-03        M    1 test ## 4  10     R 0.001307244 9.986928e-01        R    1 test ## 5 134     M 0.999999755 2.445735e-07        M    1 test ## 6  34     R 0.999761364 2.386361e-04        M    1 test ## ... (#rows: 70, #cols: 7) ## ## $`Sonar-example`$classif.rpart ## Resampled Prediction for: ## Resample description: holdout with 0.67 split rate. ## Predict: test ## Stratification: FALSE ## predict.type: response ## threshold: ## time (mean): 0.00 ##    id truth response iter  set ## 1 194     M        M    1 test ## 2  18     R        R    1 test ## 3 196     M        M    1 test ## 4  10     R        R    1 test ## 5 134     M        M    1 test ## 6  34     R        M    1 test ## ... (#rows: 70, #cols: 5)  head(getBMRPredictions(bmr, as.df = TRUE))  ##         task.id  learner.id  id truth      prob.M       prob.R response ## 1 Sonar-example classif.lda 194     M 0.001914379 9.980856e-01        R ## 2 Sonar-example classif.lda  18     R 0.474564117 5.254359e-01        R ## 3 Sonar-example classif.lda 196     M 0.996712551 3.287449e-03        M ## 4 Sonar-example classif.lda  10     R 0.001307244 9.986928e-01        R ## 5 Sonar-example classif.lda 134     M 0.999999755 2.445735e-07        M ## 6 Sonar-example classif.lda  34     R 0.999761364 2.386361e-04        M ##   iter  set ## 1    1 test ## 2    1 test ## 3    1 test ## 4    1 test ## 5    1 test ## 6    1 test head(getBMRPredictions(bmr, learner.ids = \"classif.rpart\", as.df = TRUE)) ##         task.id    learner.id  id truth response iter  set ## 1 Sonar-example classif.rpart 112     M        R    1 test ## 2 Sonar-example classif.rpart 191     M        M    1 test ## 3 Sonar-example classif.rpart 208     M        M    1 test ## 4 Sonar-example classif.rpart 203     M        M    1 test ## 5 Sonar-example classif.rpart  23     R        M    1 test ## 6 Sonar-example classif.rpart 162     M        M    1 test"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"ids","dir":"Articles > Tutorial","previous_headings":"","what":"IDs","title":"Benchmark Experiments","text":"IDs makeLearner()s, Task()s Measure’s (makeMeasure()) benchmark experiment can retrieved follows:","code":"getBMRTaskIds(bmr) ## [1] \"Sonar-example\"  getBMRLearnerIds(bmr) ## [1] \"classif.lda\"   \"classif.rpart\"  getBMRMeasureIds(bmr) ## [1] \"mmce\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"fitted-models","dir":"Articles > Tutorial","previous_headings":"","what":"Fitted models","title":"Benchmark Experiments","text":"default BenchmarkResult() contain fitted models learners tasks. Since used models = TRUE calling benchmark(), can also inspect created models. fitted models can retrieved function getBMRModels(). returns (possibly nested) base::list() WrappedModel (makeWrappedModel()) objects.","code":"getBMRModels(bmr) ## $`Sonar-example` ## $`Sonar-example`$classif.lda ## $`Sonar-example`$classif.lda[[1]] ## Model for learner.id=classif.lda; learner.class=classif.lda ## Trained on: task.id = Sonar-example; obs = 138; features = 60 ## Hyperparameters:  ##  ##  ## $`Sonar-example`$classif.rpart ## $`Sonar-example`$classif.rpart[[1]] ## Model for learner.id=classif.rpart; learner.class=classif.rpart ## Trained on: task.id = Sonar-example; obs = 138; features = 60 ## Hyperparameters: xval=0  getBMRModels(bmr, drop = TRUE) ## $classif.lda ## $classif.lda[[1]] ## Model for learner.id=classif.lda; learner.class=classif.lda ## Trained on: task.id = Sonar-example; obs = 138; features = 60 ## Hyperparameters:  ##  ##  ## $classif.rpart ## $classif.rpart[[1]] ## Model for learner.id=classif.rpart; learner.class=classif.rpart ## Trained on: task.id = Sonar-example; obs = 138; features = 60 ## Hyperparameters: xval=0  getBMRModels(bmr, learner.ids = \"classif.lda\") ## $`Sonar-example` ## $`Sonar-example`$classif.lda ## $`Sonar-example`$classif.lda[[1]] ## Model for learner.id=classif.lda; learner.class=classif.lda ## Trained on: task.id = Sonar-example; obs = 138; features = 60 ## Hyperparameters:"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"learners-and-measures","dir":"Articles > Tutorial","previous_headings":"","what":"Learners and measures","title":"Benchmark Experiments","text":"Moreover, can extract employed makeLearner()s Measure’s (makeMeasure()).","code":"getBMRLearners(bmr) ## $classif.lda ## Learner classif.lda from package MASS ## Type: classif ## Name: Linear Discriminant Analysis; Short name: lda ## Class: classif.lda ## Properties: twoclass,multiclass,numerics,factors,prob ## Predict-Type: prob ## Hyperparameters:  ##  ##  ## $classif.rpart ## Learner classif.rpart from package rpart ## Type: classif ## Name: Decision Tree; Short name: rpart ## Class: classif.rpart ## Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp ## Predict-Type: response ## Hyperparameters: xval=0  getBMRMeasures(bmr) ## [[1]] ## Name: Mean misclassification error ## Performance measure: mmce ## Properties: classif,classif.multi,req.pred,req.truth ## Minimize: TRUE ## Best: 0; Worst: 1 ## Aggregated by: test.mean ## Arguments:  ## Note: Defined as: mean(response != truth)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"merging-benchmark-results","dir":"Articles > Tutorial","previous_headings":"","what":"Merging benchmark results","title":"Benchmark Experiments","text":"Sometimes completing benchmark experiment turns want extend another makeLearner() another Task(). case can perform additional benchmark experiment use function mergeBenchmarkResults() combine results single BenchmarkResult() object can accessed analyzed usual. example benchmark experiment applied MASS::lda() rpart::rpart() sonar.task(). now perform second experiment using randomForest::randomForest() quadratic discriminant analysis MASS::qda() merge results. Note examples case resample description (makeResampleDesc()) passed benchmark() function. reason MASS::lda() rpart::rpart() likely evaluated different training/test set pair randomForest::randomForest() MASS::qda(). Differing training/test set pairs across learners pose additional source variation results, can make harder detect actual performance differences learners. Therefore, suspect extend benchmark experiment another makeLearner() later ’s probably easiest work makeResampleInstance()s start. can stored used additional experiments. Alternatively, used resample description first benchmark experiment also extract makeResampleInstance()s BenchmarkResult() bmr pass benchmark() calls.","code":"# First benchmark result bmr ##         task.id    learner.id mmce.test.mean ## 1 Sonar-example   classif.lda      0.3285714 ## 2 Sonar-example classif.rpart      0.3714286  # Benchmark experiment for the additional learners lrns2 = list(makeLearner(\"classif.randomForest\"), makeLearner(\"classif.qda\")) bmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE) bmr2 ##         task.id           learner.id mmce.test.mean ## 1 Sonar-example classif.randomForest      0.1285714 ## 2 Sonar-example          classif.qda      0.4000000  # Merge the results mergeBenchmarkResults(list(bmr, bmr2)) ##         task.id           learner.id mmce.test.mean ## 1 Sonar-example          classif.lda      0.3285714 ## 2 Sonar-example        classif.rpart      0.3714286 ## 3 Sonar-example classif.randomForest      0.1285714 ## 4 Sonar-example          classif.qda      0.4000000 rin = getBMRPredictions(bmr)[[1]][[1]]$instance rin ## Resample instance for 208 cases. ## Resample description: holdout with 0.67 split rate. ## Predict: test ## Stratification: FALSE  # Benchmark experiment for the additional random forest bmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE) bmr3 ##         task.id           learner.id mmce.test.mean ## 1 Sonar-example classif.randomForest      0.1428571 ## 2 Sonar-example          classif.qda      0.3285714  # Merge the results mergeBenchmarkResults(list(bmr, bmr3)) ##         task.id           learner.id mmce.test.mean ## 1 Sonar-example          classif.lda      0.3285714 ## 2 Sonar-example        classif.rpart      0.3714286 ## 3 Sonar-example classif.randomForest      0.1428571 ## 4 Sonar-example          classif.qda      0.3285714"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"benchmark-analysis-and-visualization","dir":"Articles > Tutorial","previous_headings":"","what":"Benchmark analysis and visualization","title":"Benchmark Experiments","text":"mlr offers several ways analyze results benchmark experiment. includes visualization, ranking learning algorithms hypothesis tests assess performance differences learners. order demonstrate functionality conduct slightly larger benchmark experiment three learning algorithms applied five classification tasks.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"example-comparing-lda-rpart-and-random-forest","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Comparing lda, rpart and random Forest","title":"Benchmark Experiments","text":"consider MASS::lda(), classification trees rpart::rpart(), random forests randomForest::randomForest(). Since default learner IDs little long, choose shorter names R code . use five classification tasks. Three already provided mlr, two data sets taken package mlbench::mlbench() converted Task()s function convertMLBenchObjToTask(). tasks 10-fold cross-validation chosen resampling strategy. achieved passing single resample description (makeResampleDesc()) benchmark(), instantiated automatically Task(). way, instance used learners applied single task. also possible choose different resampling strategy Task() passing base::list() length number tasks can contain resample descriptions (makeResampleDesc()) resample instances (makeResampleInstance()). use mean misclassification error mmce primary performance measure, also calculate balanced error rate ber training time timetrain. aggregated performance values can see iris- PimaIndiansDiabetes-example linear discriminant analysis (MASS::lda()) performs well tasks randomForest::randomForest() seems superior. Training takes longer randomForest::randomForest() learners. order draw conclusions average performances least variability taken account , preferably, distribution performance values across resampling iterations. individual performances 10 folds every task, learner, measure retrieved . closer look result reveals randomForest::randomForest() outperforms classification tree (rpart::rpart()) every instance, linear discriminant analysis (MASS::lda()) performs better rpart::rpart() time. Additionally MASS::lda() sometimes even beats randomForest::randomForest(). increasing size benchmark() experiments, tables become almost unreadable hard comprehend. mlr features plotting functions visualize results benchmark experiments might find useful. Moreover, mlr offers statistical hypothesis tests assess performance differences learners.","code":"## Loading required package: mlbench # Create a list of learners lrns = list(   makeLearner(\"classif.lda\", id = \"lda\"),   makeLearner(\"classif.rpart\", id = \"rpart\"),   makeLearner(\"classif.randomForest\", id = \"randomForest\") )  # Get additional Tasks from package mlbench ring.task = convertMLBenchObjToTask(\"mlbench.ringnorm\", n = 600) wave.task = convertMLBenchObjToTask(\"mlbench.waveform\", n = 600)  tasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task) rdesc = makeResampleDesc(\"CV\", iters = 10) meas = list(mmce, ber, timetrain) bmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE)  ##                        task.id   learner.id mmce.test.mean ber.test.mean ## 1                 iris-example          lda     0.02000000    0.02222222 ## 2                 iris-example        rpart     0.08000000    0.07555556 ## 3                 iris-example randomForest     0.05333333    0.05250000 ## 4             mlbench.ringnorm          lda     0.35000000    0.34605671 ## 5             mlbench.ringnorm        rpart     0.17333333    0.17313632 ## 6             mlbench.ringnorm randomForest     0.05666667    0.05724405 ## 7             mlbench.waveform          lda     0.19000000    0.18257244 ## 8             mlbench.waveform        rpart     0.28833333    0.28765247 ## 9             mlbench.waveform randomForest     0.17500000    0.17418440 ## 10 PimaIndiansDiabetes-example          lda     0.22778537    0.27148893 ## 11 PimaIndiansDiabetes-example        rpart     0.25133288    0.28967870 ## 12 PimaIndiansDiabetes-example randomForest     0.23427888    0.27464510 ## 13               Sonar-example          lda     0.24619048    0.23986694 ## 14               Sonar-example        rpart     0.30785714    0.31153361 ## 15               Sonar-example randomForest     0.18738095    0.18359363 ##    timetrain.test.mean ## 1               0.0039 ## 2               0.0048 ## 3               0.0330 ## 4               0.0092 ## 5               0.0116 ## 6               0.3548 ## 7               0.0084 ## 8               0.0100 ## 9               0.3675 ## 10              0.0424 ## 11              0.0066 ## 12              0.4108 ## 13              0.0147 ## 14              0.0128 ## 15              0.2335 perf = getBMRPerformances(bmr, as.df = TRUE) head(perf[, -ncol(perf)]) ##        task.id learner.id iter       mmce        ber ## 1 iris-example        lda    1 0.06666667 0.05555556 ## 2 iris-example        lda    2 0.00000000 0.00000000 ## 3 iris-example        lda    3 0.06666667 0.04761905 ## 4 iris-example        lda    4 0.00000000 0.00000000 ## 5 iris-example        lda    5 0.06666667 0.04166667 ## 6 iris-example        lda    6 0.00000000 0.00000000"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"integrated-plots","dir":"Articles > Tutorial","previous_headings":"","what":"Integrated plots","title":"Benchmark Experiments","text":"Plots generated using ggplot2::ggplot2(). customization, renaming plot elements changing colors, easily possible.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"visualizing-performances","dir":"Articles > Tutorial","previous_headings":"Integrated plots","what":"Visualizing performances","title":"Benchmark Experiments","text":"plotBMRBoxplots() creates box violin plots show distribution performance values across resampling iterations one performance measure learners tasks (thus visualize output getBMRPerformances()). variants, box violin plots. first plot shows mmce second plot ber. Moreover, second plot color boxes according learner.ids.   Note default measure names learner short.names used axis labels. prefer ids like, e.g., mmce ber set pretty.names = FALSE (done second plot). course can also use ggplot2::ggplot2() functionality like ggplot2::labs() function choose completely different labels. One question comes quite often change panel headers (default Task() IDs) learner names x-axis. example looking plots like remove “example” suffixes “mlbench” prefixes panel headers. Moreover, want uppercase learner labels. Currently, probably simplest solution change factor levels plotted data shown .","code":"plotBMRBoxplots(bmr, measure = mmce, order.lrn = getBMRLearnerIds(bmr)) plotBMRBoxplots(bmr,   measure = ber, style = \"violin\", pretty.names = FALSE,   order.lrn = getBMRLearnerIds(bmr)) +   aes(color = learner.id) +   theme(strip.text.x = element_text(size = 8)) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.ymin` is deprecated. Use `fun.min` instead. ## Warning: `fun.ymax` is deprecated. Use `fun.max` instead. mmce$name ## [1] \"Mean misclassification error\"  mmce$id ## [1] \"mmce\"  getBMRLearnerIds(bmr) ## [1] \"lda\"          \"rpart\"        \"randomForest\"  getBMRLearnerShortNames(bmr) ## [1] \"lda\"   \"rpart\" \"rf\" plt = plotBMRBoxplots(bmr, measure = mmce, order.lrn = getBMRLearnerIds(bmr)) head(plt$data[, -ncol(plt$data)]) ##        task.id learner.id iter       mmce        ber ## 1 iris-example        lda    1 0.06666667 0.05555556 ## 2 iris-example        lda    2 0.00000000 0.00000000 ## 3 iris-example        lda    3 0.06666667 0.04761905 ## 4 iris-example        lda    4 0.00000000 0.00000000 ## 5 iris-example        lda    5 0.06666667 0.04166667 ## 6 iris-example        lda    6 0.00000000 0.00000000  levels(plt$data$task.id) = c(\"Iris\", \"Ringnorm\", \"Waveform\", \"Diabetes\", \"Sonar\") levels(plt$data$learner.id) = c(\"LDA\", \"CART\", \"RF\")  plt + ylab(\"Error rate\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"visualizing-aggregated-performances","dir":"Articles > Tutorial","previous_headings":"Integrated plots","what":"Visualizing aggregated performances","title":"Benchmark Experiments","text":"aggregated performance values (resulting getBMRAggrPerformances()) can visualized function plotBMRSummary(). plot draws one line task aggregated values one performance measure learners displayed. default, first measure base::list() Measure’s (makeMeasure()) passed benchmark() used, example mmce. Moreover, small vertical jitter added prevent overplotting.","code":"plotBMRSummary(bmr)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"calculating-and-visualizing-ranks","dir":"Articles > Tutorial","previous_headings":"Integrated plots","what":"Calculating and visualizing ranks","title":"Benchmark Experiments","text":"Additional absolute performance, relative performance, .e., ranking learners usually interest might provide valuable additional insight. Function convertBMRToRankMatrix() calculates ranks based aggregated learner performances one measure. choose mean misclassification error. rank structure can visualized plotBMRRanksAsBarChart(). Methods best performance, .e., lowest mmce, assigned lowest rank. Linear discriminant analysis (MASS::lda()) best iris PimaIndiansDiabetes-examples randomForest::randomForest() shows best results remaining tasks. plotBMRRanksAsBarChart() option pos = \"tile\" shows corresponding heat map. ranks displayed x-axis learners color-coded.  similar plot can also obtained via plotBMRSummary(). option trafo = \"rank\" ranks displayed instead aggregated performances.  Alternatively, can draw stacked bar charts (default) bar charts juxtaposed bars (pos = \"dodge\") better suited compare frequencies learners within across ranks.","code":"m = convertBMRToRankMatrix(bmr, mmce) m ##              PimaIndiansDiabetes-example Sonar-example iris-example ## lda                                    1             2            1 ## randomForest                           2             1            2 ## rpart                                  3             3            3 ##              mlbench.ringnorm mlbench.waveform ## lda                         3                2 ## randomForest                1                1 ## rpart                       2                3 plotBMRRanksAsBarChart(bmr, pos = \"tile\", order.lrn = getBMRLearnerIds(bmr)) plotBMRSummary(bmr, trafo = \"rank\", jitter = 0) plotBMRRanksAsBarChart(bmr, order.lrn = getBMRLearnerIds(bmr)) plotBMRRanksAsBarChart(bmr, pos = \"dodge\", order.lrn = getBMRLearnerIds(bmr))"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"comparing-learners-using-hypothesis-tests","dir":"Articles > Tutorial","previous_headings":"","what":"Comparing learners using hypothesis tests","title":"Benchmark Experiments","text":"Many researchers feel need display algorithm’s superiority employing sort hypothesis testing. non-parametric tests seem better suited benchmark results tests provided mlr Overall Friedman test Friedman-Nemenyi post hoc test. ad hoc friedmanTestBMR() based stats::friedman.test() testing hypothesis whether significant difference employed learners, post hoc friedmanPostHocTestBMR() tests significant differences pairs learners. Non parametric tests often less power parametric counterparts less assumptions underlying distributions made. often means many data sets needed order able show significant differences reasonable significance levels. example, want compare three learners selected data sets. First might want test hypothesis whether difference learners. order keep computation time tutorial small, makeLearner()s evaluated five tasks. also means operate relatively low significance level \\(\\alpha = 0.1\\). can reject null hypothesis Friedman test reasonable significance level might now want test differences lie exactly. level significance, can reject null hypothesis exists performance difference decision tree (rpart::rpart()) randomForest::randomForest().","code":"friedmanTestBMR(bmr) ##  ##  Friedman rank sum test ##  ## data:  mmce.test.mean and learner.id and task.id ## Friedman chi-squared = 5.2, df = 2, p-value = 0.07427 friedmanPostHocTestBMR(bmr, p.value = 0.1) ##              lda   randomForest ## randomForest 0.802 -            ## rpart        0.254 0.069"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"critical-differences-diagram","dir":"Articles > Tutorial","previous_headings":"","what":"Critical differences diagram","title":"Benchmark Experiments","text":"order visualize differently performing learners, critical differences diagramcan plotted, using either Nemenyi test (test = \"nemenyi\") Bonferroni-Dunn test (test = \"bd\"). mean rank learners displayed x-axis. Choosing test = \"nemenyi\" compares pairs makeLearner()s , thus output groups significantly different learners. diagram connects groups learners mean ranks differ critical differences. Learners connected bar significantly different, learner(s) lower mean rank can considered “better” chosen significance level. Choosing test = \"bd\" performs pairwise comparison baseline. interval extends given critical difference directions drawn around makeLearner() chosen baseline, though comparisons baseline possible. learners within interval significantly different, baseline can considered better worse given learner outside interval. critical difference \\(\\mathit{CD}\\) calculated \\[\\mathit{CD} = q_\\alpha \\cdot \\sqrt{\\frac{k(k+1)}{6N}},\\] \\(N\\) denotes number tasks, \\(k\\) number learners, \\(q_\\alpha\\) comes studentized range statistic divided \\(\\sqrt{2}\\). details see Demsar (2006). Function generateCritDifferencesData() necessary calculations function plotCritDifferences() draws plot. See tutorial page visualization details data generation plotting functions.","code":"# Nemenyi test g = generateCritDifferencesData(bmr, p.value = 0.1, test = \"nemenyi\") plotCritDifferences(g) + coord_cartesian(xlim = c(-1, 5), ylim = c(0, 2)) +   scale_colour_manual(values = c(\"lda\" = \"#F8766D\", \"rpart\" = \"#00BA38\", \"randomForest\" = \"#619CFF\")) # Bonferroni-Dunn test g = generateCritDifferencesData(bmr, p.value = 0.1, test = \"bd\", baseline = \"randomForest\") plotCritDifferences(g) + coord_cartesian(xlim = c(-1, 5), ylim = c(0, 2)) +   scale_colour_manual(values = c(\"lda\" = \"#F8766D\", \"rpart\" = \"#00BA38\", \"randomForest\" = \"#619CFF\"))"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"custom-plots","dir":"Articles > Tutorial","previous_headings":"","what":"Custom plots","title":"Benchmark Experiments","text":"can easily generate visualizations customizing ggplot2::ggplot() objects returned plots , retrieve data ggplot2::ggplot() objects use basis plots, rely base::data.frame()s returned getBMRPerformances() getBMRAggrPerformances(). examples. Instead boxplots (plotBMRBoxplots()) create density plots show performance values resulting individual resampling iterations.  order plot multiple performance measures parallel, perf reshaped long format. generate grouped boxplots showing error rate training time timetrain.  might also useful assess learner performances single resampling iterations, .e., one fold, related. might help gain insight, example closer look train test sets iterations one learner performs exceptionally well another one fairly bad. Moreover, might useful construction ensembles learning algorithms. , function GGally::ggpairs() package GGally::GGally() used generate scatterplot matrix mean misclassification errors mlbench::Sonar() data set.","code":"perf = getBMRPerformances(bmr, as.df = TRUE)  # Density plots for two tasks qplot(mmce,   colour = learner.id, facets = . ~ task.id,   data = perf[perf$task.id %in% c(\"iris-example\", \"Sonar-example\"), ], geom = \"density\") +   theme(strip.text.x = element_text(size = 8)) # Compare mmce and timetrain df = reshape2::melt(perf, id.vars = c(\"task.id\", \"learner.id\", \"iter\")) df = df[df$variable != \"ber\", ] head(df) ##        task.id learner.id iter variable      value ## 1 iris-example        lda    1     mmce 0.06666667 ## 2 iris-example        lda    2     mmce 0.00000000 ## 3 iris-example        lda    3     mmce 0.06666667 ## 4 iris-example        lda    4     mmce 0.00000000 ## 5 iris-example        lda    5     mmce 0.06666667 ## 6 iris-example        lda    6     mmce 0.00000000  qplot(variable, value,   data = df, colour = learner.id, geom = \"boxplot\",   xlab = \"measure\", ylab = \"performance\") +   facet_wrap(~task.id, nrow = 2) perf = getBMRPerformances(bmr, task.id = \"Sonar-example\", as.df = TRUE) df = reshape2::melt(perf, id.vars = c(\"task.id\", \"learner.id\", \"iter\")) df = df[df$variable == \"mmce\", ] df = reshape2::dcast(df, task.id + iter ~ variable + learner.id) head(df) ##         task.id iter  mmce_lda mmce_randomForest mmce_rpart ## 1 Sonar-example    1 0.4285714         0.2380952  0.2380952 ## 2 Sonar-example    2 0.1904762         0.0952381  0.2857143 ## 3 Sonar-example    3 0.3000000         0.1500000  0.3000000 ## 4 Sonar-example    4 0.1904762         0.0000000  0.1904762 ## 5 Sonar-example    5 0.3333333         0.2380952  0.4761905 ## 6 Sonar-example    6 0.2500000         0.1000000  0.2500000  GGally::ggpairs(df, 3:5) ## Registered S3 method overwritten by 'GGally': ##   method from    ##   +.gg   ggplot2"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/benchmark_experiments.html","id":"further-comments","dir":"Articles > Tutorial","previous_headings":"","what":"Further comments","title":"Benchmark Experiments","text":"Note supervised classification mlr offers plots operate BenchmarkResult() objects allow compare performance learning algorithms. See example tutorial page ROC analysis functions generateThreshVsPerfData(), plotROCCurves(), plotViperCharts() well page classifier calibration function generateCalibrationData(). examples shown section applied “raw” learning algorithms, often things complicated. least, many learners hyperparameters need tuned get sensible results. Reliable performance estimates can obtained nested resampling, .e., tuning inner resampling loop estimating performance outer loop. Moreover, might want combine learners pre-processing steps like imputation, scaling, outlier removal, dimensionality reduction feature selection . can easily done using mlr’s wrapper functionality. general principle explained section wrapper Advanced part tutorial. also several sections devoted common pre-processing steps. Benchmark experiments can quickly become computationally demanding. mlr offers possibilities parallelization.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/configureMlr.html","id":"example-reducing-the-output-on-the-console","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Reducing the output on the console","title":"Configuring mlr","text":"bothered output console like example? can suppress output Learner makeLearner() resample() call follows: (Note nnet::multinom() trace switch can alternatively used turn progress messages.) globally suppress output subsequent learners calls resample(), benchmark() etc. following:","code":"rdesc = makeResampleDesc(\"Holdout\") r = resample(\"classif.multinom\", iris.task, rdesc) ## Resampling: holdout ## Measures:             mmce ## # weights:  18 (10 variable) ## initial  value 109.861229  ## iter  10 value 8.635871 ## iter  20 value 0.942436 ## iter  30 value 0.225516 ## iter  40 value 0.144303 ## iter  50 value 0.139259 ## iter  60 value 0.123724 ## iter  70 value 0.089635 ## iter  80 value 0.084994 ## iter  90 value 0.058982 ## iter 100 value 0.056564 ## final  value 0.056564  ## stopped after 100 iterations ## [Resample] iter 1:    0.0400000 ##  ## Aggregated Result: mmce.test.mean=0.0400000 ## lrn = makeLearner(\"classif.multinom\", config = list(show.learner.output = FALSE)) r = resample(lrn, iris.task, rdesc, show.info = FALSE) configureMlr(show.learner.output = FALSE, show.info = FALSE) r = resample(\"classif.multinom\", iris.task, rdesc)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/configureMlr.html","id":"accessing-and-resetting-the-configuration","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing and resetting the configuration","title":"Configuring mlr","text":"Function getMlrOptions() returns base::list() current configuration. restore default configuration call configureMlr() empty argument list.","code":"getMlrOptions() ## $show.info ## [1] FALSE ##  ## $on.learner.error ## [1] \"stop\" ##  ## $on.learner.warning ## [1] \"warn\" ##  ## $on.par.without.desc ## [1] \"stop\" ##  ## $on.par.out.of.bounds ## [1] \"stop\" ##  ## $on.measure.not.applicable ## [1] \"stop\" ##  ## $show.learner.output ## [1] FALSE ##  ## $on.error.dump ## [1] FALSE configureMlr() getMlrOptions() ## $show.info ## [1] TRUE ##  ## $on.learner.error ## [1] \"stop\" ##  ## $on.learner.warning ## [1] \"warn\" ##  ## $on.par.without.desc ## [1] \"stop\" ##  ## $on.par.out.of.bounds ## [1] \"stop\" ##  ## $on.measure.not.applicable ## [1] \"stop\" ##  ## $show.learner.output ## [1] TRUE ##  ## $on.error.dump ## [1] FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/configureMlr.html","id":"example-turning-off-parameter-checking","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Turning off parameter checking","title":"Configuring mlr","text":"might happen want set parameter Learner (makeLearner(), parameter registered learner’s parameter set (ParamHelpers::makeParamSet()) yet. case might want contact us open issue well! problem fixed can turn mlr’s parameter checking. parameter setting passed underlying function without ado.","code":"# Support Vector Machine with linear kernel and new parameter 'newParam' lrn = makeLearner(\"classif.ksvm\", kernel = \"vanilladot\", newParam = 3) ## Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newParam without available description object! ## Did you mean one of these hyperparameters instead: degree scaled kernel ## You can switch off this check by using configureMlr!  # Turn off parameter checking completely configureMlr(on.par.without.desc = \"quiet\") lrn = makeLearner(\"classif.ksvm\", kernel = \"vanilladot\", newParam = 3) train(lrn, iris.task) ##  Setting default kernel parameters ## Model for learner.id=classif.ksvm; learner.class=classif.ksvm ## Trained on: task.id = iris-example; obs = 150; features = 4 ## Hyperparameters: fit=FALSE,kernel=vanilladot,newParam=3  # Option \"quiet\" also masks typos lrn = makeLearner(\"classif.ksvm\", kernl = \"vanilladot\") train(lrn, iris.task) ## Model for learner.id=classif.ksvm; learner.class=classif.ksvm ## Trained on: task.id = iris-example; obs = 150; features = 4 ## Hyperparameters: fit=FALSE,kernl=vanilladot  # Alternatively turn off parameter checking, but still see warnings configureMlr(on.par.without.desc = \"warn\") lrn = makeLearner(\"classif.ksvm\", kernl = \"vanilladot\", newParam = 3) ## Warning in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter kernl without available description object! ## Did you mean one of these hyperparameters instead: kernel nu degree ## You can switch off this check by using configureMlr! ## Warning in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newParam without available description object! ## Did you mean one of these hyperparameters instead: degree scaled kernel ## You can switch off this check by using configureMlr!  train(lrn, iris.task) ## Model for learner.id=classif.ksvm; learner.class=classif.ksvm ## Trained on: task.id = iris-example; obs = 150; features = 4 ## Hyperparameters: fit=FALSE,kernl=vanilladot,newParam=3"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/configureMlr.html","id":"example-handling-errors-in-a-learning-method","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Handling errors in a learning method","title":"Configuring mlr","text":"learning method throws error default behavior mlr generate exception well. However, situations, example conduct larger bechmark experiment multiple data sets learners, usually don’t want whole experiment stopped due one error. can prevent using .learner.error option configureMlr(). .learner.error = \"warn\" warning issued instead exception object class FailureModel() created. can extract error message using function getFailureModelMsg(). steps like prediction performance calculation work return NA's.","code":"# This call gives an error caused by the low number of observations in class \"virginica\" train(\"classif.qda\", task = iris.task, subset = 1:104) ## Error in qda.default(x, grouping, ...): some group is too small for 'qda'  # Get a warning instead of an error configureMlr(on.learner.error = \"warn\") mod = train(\"classif.qda\", task = iris.task, subset = 1:104) ## Warning in train(\"classif.qda\", task = iris.task, subset = 1:104): Could not train learner classif.qda: Error in qda.default(x, grouping, ...) :  ##   some group is too small for 'qda'  mod ## Model for learner.id=classif.qda; learner.class=classif.qda ## Trained on: task.id = iris-example; obs = 104; features = 4 ## Hyperparameters:  ## Training failed: Error in qda.default(x, grouping, ...) :  ##   some group is too small for 'qda' ##  ## Training failed: Error in qda.default(x, grouping, ...) :  ##   some group is too small for 'qda'  # mod is an object of class FailureModel isFailureModel(mod) ## [1] TRUE  # Retrieve the error message getFailureModelMsg(mod) ## [1] \"Error in qda.default(x, grouping, ...) : \\n  some group is too small for 'qda'\\n\"  # predict and performance return NA's pred = predict(mod, iris.task) pred ## Prediction: 150 observations ## predict.type: response ## threshold:  ## time: NA ##   id  truth response ## 1  1 setosa     <NA> ## 2  2 setosa     <NA> ## 3  3 setosa     <NA> ## 4  4 setosa     <NA> ## 5  5 setosa     <NA> ## 6  6 setosa     <NA> ## ... (#rows: 150, #cols: 3)  performance(pred) ## mmce  ##   NA"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"class-dependent-misclassification-costs","dir":"Articles > Tutorial","previous_headings":"","what":"Class-dependent misclassification costs","title":"Cost-Sensitive Classification","text":"classification methods can accomodate misclassification costs directly. One example rpart::rpart(). Alternatively, can use cost-insensitive methods manipulate predictions training data order take misclassification costs account. mlr supports thresholding rebalancing. Thresholding: thresholds used turn posterior probabilities class labels chosen costs minimized. requires Learner (makeLearner()) can predict posterior probabilities. training costs taken account. Rebalancing: idea change proportion classes training data set order account costs training, either weighting sampling. Rebalancing require Learner (makeLearner()) can predict probabilities. weighting need Learner (makeLearner()) supports class weights observation weights. Learner (makeLearner()) deal weights proportion classes can changed - undersampling. start binary classification problems afterwards deal multi-class problems.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"binary-classification-problems","dir":"Articles > Tutorial","previous_headings":"","what":"Binary classification problems","title":"Cost-Sensitive Classification","text":"positive negative classes labeled \\(1\\) \\(-1\\), respectively, consider following cost matrix rows indicate true classes columns predicted classes: Often, diagonal entries zero cost matrix rescaled achieve zeros diagonal (see example O’Brien et al, 2008). well-known cost-sensitive classification problem posed German Credit data set (caret::GermanCredit()) (see also UCI Machine Learning Repository). corresponding cost matrix (though Elkan (2001) argues matrix economically unreasonable) given : table , rows indicate true columns predicted classes. case class-dependent costs sufficient generate ordinary ClassifTask (Task()). CostSensTask (Task()) needed costs example-dependent. R code create ClassifTask (Task()), remove two constant features data set generate cost matrix. Per default, Bad positive class.","code":"data(GermanCredit, package = \"caret\") credit.task = makeClassifTask(data = GermanCredit, target = \"Class\") credit.task = removeConstantFeatures(credit.task) ## Removing 2 columns: Purpose.Vacation,Personal.Female.Single  credit.task ## Supervised task: GermanCredit ## Type: classif ## Target: Class ## Observations: 1000 ## Features: ##    numerics     factors     ordered functionals  ##          59           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ##  Bad Good  ##  300  700  ## Positive class: Bad  costs = matrix(c(0, 1, 5, 0), 2) colnames(costs) = rownames(costs) = getTaskClassLevels(credit.task) costs ##      Bad Good ## Bad    0    5 ## Good   1    0"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"thresholding","dir":"Articles > Tutorial","previous_headings":"Binary classification problems","what":"1. Thresholding","title":"Cost-Sensitive Classification","text":"start fitting logistic regression model (nnet::multinom()) German Credit data set (caret::GermanCredit()) predict posterior probabilities. default thresholds classes 0.5. according cost matrix predict class Good sure Good indeed correct label. Therefore increase threshold class Good decrease threshold class Bad.","code":"# Train and predict posterior probabilities lrn = makeLearner(\"classif.multinom\", predict.type = \"prob\", trace = FALSE) mod = train(lrn, credit.task) pred = predict(mod, task = credit.task) pred ## Prediction: 1000 observations ## predict.type: prob ## threshold: Bad=0.50,Good=0.50 ## time: 0.01 ##   id truth   prob.Bad prob.Good response ## 1  1  Good 0.03525092 0.9647491     Good ## 2  2   Bad 0.63222363 0.3677764      Bad ## 3  3  Good 0.02807414 0.9719259     Good ## 4  4  Good 0.25182703 0.7481730     Good ## 5  5   Bad 0.75193275 0.2480673      Bad ## 6  6  Good 0.26230149 0.7376985     Good ## ... (#rows: 1000, #cols: 5)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"i--theoretical-thresholding","dir":"Articles > Tutorial","previous_headings":"Binary classification problems > 1. Thresholding","what":"i. Theoretical thresholding","title":"Cost-Sensitive Classification","text":"theoretical threshold positive class can calculated cost matrix \\[t^* = \\frac{c(+1,-1) - c(-1,-1)}{c(+1,-1) - c(+1,+1) + c(-1,+1) - c(-1,-1)}.\\] details see Elkan (2001). theoretical threshold German Credit data set (caret::GermanCredit()) calculated used predict class labels. Since diagonal cost matrix zero formula given simplifies accordingly. may recall can change thresholds mlr either training using predict.threshold option makeLearner() prediction calling setThreshold() Prediction() object. already prediction use setThreshold() function. returns altered Prediction() object class predictions theoretical threshold. order calculate average costs entire data set first need create new performance Measure (makeMeasure()). can done function makeCostMeasure(). expected rows cost matrix indicate true columns predicted class labels. average costs can computed function performance(). compare average costs error rate (mmce) learning algorithm default thresholds 0.5 theoretical thresholds. performance values may overly optimistic used data set training prediction, resampling strategies preferred. R code make use predict.threshold argument makeLearner() set threshold 3-fold cross-validation credit.task(). Note create ResampleInstance (makeResampleInstance()) (rin) used throughout next several code chunks get comparable performance values. also interested cross-validated performance default threshold values can call setThreshold() resample prediction (ResamplePrediction()) r$pred. Theoretical thresholding reliable predicted posterior probabilities correct. bias thresholds shifted accordingly. Useful regard function plotThreshVsPerf() can use plot average costs well performance measure versus possible threshold values positive class \\([0,1]\\). underlying data generated generateThreshVsPerfData(). following plots show cross-validated costs error rate (mmce). theoretical threshold th calculated indicated vertical line. can see left-hand plot theoretical threshold seems bit large.","code":"# Calculate the theoretical threshold for the positive class th = costs[2,1]/(costs[2,1] + costs[1,2]) th ## [1] 0.1666667 # Predict class labels according to the theoretical threshold pred.th = setThreshold(pred, th) pred.th ## Prediction: 1000 observations ## predict.type: prob ## threshold: Bad=0.17,Good=0.83 ## time: 0.01 ##   id truth   prob.Bad prob.Good response ## 1  1  Good 0.03525092 0.9647491     Good ## 2  2   Bad 0.63222363 0.3677764      Bad ## 3  3  Good 0.02807414 0.9719259     Good ## 4  4  Good 0.25182703 0.7481730      Bad ## 5  5   Bad 0.75193275 0.2480673      Bad ## 6  6  Good 0.26230149 0.7376985      Bad ## ... (#rows: 1000, #cols: 5) credit.costs = makeCostMeasure(id = \"credit.costs\", name = \"Credit costs\", costs = costs,   best = 0, worst = 5) credit.costs ## Name: Credit costs ## Performance measure: credit.costs ## Properties: classif,classif.multi,req.pred,req.truth,predtype.response,predtype.prob ## Minimize: TRUE ## Best: 0; Worst: 5 ## Aggregated by: test.mean ## Arguments: costs=<matrix>, combine=<function> ## Note: # Performance with default thresholds 0.5 performance(pred, measures = list(credit.costs, mmce)) ## credit.costs         mmce  ##        0.774        0.214  # Performance with theoretical thresholds performance(pred.th, measures = list(credit.costs, mmce)) ## credit.costs         mmce  ##        0.478        0.346 # Cross-validated performance with theoretical thresholds rin = makeResampleInstance(\"CV\", iters = 3, task = credit.task) lrn = makeLearner(\"classif.multinom\", predict.type = \"prob\", predict.threshold = th, trace = FALSE) r = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE) r # Cross-validated performance with default thresholds performance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce)) ## credit.costs         mmce  ##    0.8599378    0.2519885 d = generateThreshVsPerfData(r, measures = list(credit.costs, mmce)) plotThreshVsPerf(d, mark.th = th)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"ii--empirical-thresholding","dir":"Articles > Tutorial","previous_headings":"Binary classification problems > 1. Thresholding","what":"ii. Empirical thresholding","title":"Cost-Sensitive Classification","text":"idea empirical thresholding (see Sheng Ling, 2006) select cost-optimal threshold values given learning method based training data. contrast theoretical thresholding suffices estimated posterior probabilities order-correct. order determine optimal threshold values can use mlr’s function tuneThreshold(). tuning threshold complete training data set can lead overfitting, use resampling strategies. perform 3-fold cross-validation use tuneThreshold() calculate threshold values lowest average costs 3 test data sets. tuneThreshold() returns optimal threshold value positive class corresponding performance. expected tuned threshold smaller theoretical threshold.","code":"lrn = makeLearner(\"classif.multinom\", predict.type = \"prob\", trace = FALSE)  # 3-fold cross-validation r = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE) r  ## Resample Result ## Task: GermanCredit ## Learner: classif.multinom ## Aggr perf: credit.costs.test.mean=0.8461036,mmce.test.mean=0.2539905 ## Runtime: 0.285521  # Tune the threshold based on the predicted probabilities on the 3 test data sets tune.res = tuneThreshold(pred = r$pred, measure = credit.costs) tune.res  ## $th ## [1] 0.1062593 ##  ## $perf ## credit.costs  ##     0.527033"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"rebalancing","dir":"Articles > Tutorial","previous_headings":"Binary classification problems","what":"2. Rebalancing","title":"Cost-Sensitive Classification","text":"order minimize average costs, observations less costly class given higher importance training. can achieved weighting classes, provided learner consideration ‘class weights’ ‘observation weights’ argument. find learning methods support either type weights look list integrated learners Appendix use listLearners(). Alternatively, - undersampling techniques can used.","code":"# Learners that accept observation weights listLearners(\"classif\", properties = \"weights\")[c(\"class\", \"package\")] ##                       class         package ## 1          classif.binomial           stats ## 2               classif.C50             C50 ## 3           classif.cforest           party ## 4             classif.ctree           party ## 5          classif.cvglmnet          glmnet ## 6             classif.earth     earth,stats ## 7            classif.evtree          evtree ## 8        classif.extraTrees      extraTrees ## 9        classif.fdausc.knn         fda.usc ## 10         classif.gamboost          mboost ## 11              classif.gbm             gbm ## 12         classif.glmboost          mboost ## 13           classif.glmnet          glmnet ## 14 classif.h2o.deeplearning             h2o ## 15          classif.h2o.glm             h2o ## 16           classif.logreg           stats ## 17         classif.multinom            nnet ## 18             classif.nnet            nnet ## 19              classif.plr         stepPlr ## 20           classif.probit           stats ## 21  classif.randomForestSRC randomForestSRC ## 22           classif.ranger          ranger ## 23            classif.rpart           rpart ## 24          classif.xgboost         xgboost  # Learners that can deal with class weights listLearners(\"classif\", properties = \"class.weights\")[c(\"class\", \"package\")] ##                            class      package ## 1                   classif.ksvm      kernlab ## 2       classif.LiblineaRL1L2SVC    LiblineaR ## 3      classif.LiblineaRL1LogReg    LiblineaR ## 4       classif.LiblineaRL2L1SVC    LiblineaR ## 5      classif.LiblineaRL2LogReg    LiblineaR ## 6         classif.LiblineaRL2SVC    LiblineaR ## 7 classif.LiblineaRMultiClassSVC    LiblineaR ## 8           classif.randomForest randomForest ## 9                    classif.svm        e1071"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"i--weighting","dir":"Articles > Tutorial","previous_headings":"Binary classification problems > 2. Rebalancing","what":"i. Weighting","title":"Cost-Sensitive Classification","text":"Just theoretical thresholds, theoretical weights can calculated cost matrix. \\(t\\) indicates target threshold \\(t_0\\) original threshold positive class proportion observations positive class multiplied \\[\\frac{1-t}{t} \\frac{t_0}{1-t_0}.\\] Alternatively, proportion observations negative class can multiplied inverse. proof given Elkan (2001). cases, original threshold \\(t_0 = 0.5\\) thus second factor vanishes. additionally target threshold \\(t\\) equals theoretical threshold \\(t^*\\) proportion observations positive class multiplied \\[\\frac{1-t^*}{t^*} = \\frac{c(-1,+1) - c(+1,+1)}{c(+1,-1) - c(-1,-1)}.\\] credit example (caret:GermanCredit()) theoretical threshold corresponds weight 5 positive class. unified convenient way assign class weights Learner (makeLearner()) (tune ) provided function makeWeightedClassesWrapper(). class weights specified using argument wcw.weight. learners support observation weights suitable weight vector generated internally training resampling. learner can deal class weights, weights basically passed appropriate learner parameter. advantage using wrapper case unified way specify class weights. example using learner \"classif.multinom\" (nnet::multinom()) package nnet) accepts observation weights. binary classification problems sufficient specify weight w positive class. negative class automatically receives weight 1. classification methods like \"classif.ksvm\" (support vector machine kernlab::ksvm() package kernlab) support class weights can pass directly. , conveniently, can use makeWeightedClassesWrapper(). Just like theoretical threshold, theoretical weights may always suitable, therefore can tune weight positive class shown following example. Calculating theoretical weight beforehand may help narrow search interval.","code":"# Weight for positive class corresponding to theoretical treshold w = (1 - th)/th w ## [1] 5 # Weighted learner lrn = makeLearner(\"classif.multinom\", trace = FALSE) lrn = makeWeightedClassesWrapper(lrn, wcw.weight = w) lrn  r = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) r  ## Resample Result ## Task: GermanCredit ## Learner: weightedclasses.classif.multinom ## Aggr perf: credit.costs.test.mean=0.5680531,mmce.test.mean=0.3599887 ## Runtime: 0.20412 lrn = makeLearner(\"classif.ksvm\", class.weights = c(Bad = w, Good = 1)) lrn = makeWeightedClassesWrapper(\"classif.ksvm\", wcw.weight = w) r = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) r  ## Resample Result ## Task: GermanCredit ## Learner: weightedclasses.classif.ksvm ## Aggr perf: credit.costs.test.mean=0.6070861,mmce.test.mean=0.3349817 ## Runtime: 1.69427 lrn = makeLearner(\"classif.multinom\", trace = FALSE) lrn = makeWeightedClassesWrapper(lrn) ps = makeParamSet(makeDiscreteParam(\"wcw.weight\", seq(4, 12, 0.5))) ctrl = makeTuneControlGrid() tune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps,   measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE) tune.res ## Tune result: ## Op. pars: wcw.weight=8 ## credit.costs.test.mean=0.5139601,mmce.test.mean=0.4020038  as.data.frame(tune.res$opt.path)[1:3] ##    wcw.weight credit.costs.test.mean mmce.test.mean ## 1           4              0.5439032      0.3279867 ## 2         4.5              0.5598982      0.3439817 ## 3           5              0.5589242      0.3589877 ## 4         5.5              0.5539162      0.3699838 ## 5           6              0.5369202      0.3769878 ## 6         6.5              0.5249201      0.3809918 ## 7           7              0.5289331      0.3889968 ## 8         7.5              0.5159561      0.3960038 ## 9           8              0.5139601      0.4020038 ## 10        8.5              0.5219621      0.4100058 ## 11          9              0.5299551      0.4179988 ## 12        9.5              0.5369561      0.4249999 ## 13         10              0.5439571      0.4320009 ## 14       10.5              0.5439481      0.4359959 ## 15         11              0.5409631      0.4409949 ## 16       11.5              0.5439601      0.4439919 ## 17         12              0.5499661      0.4499979"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"ii--over--and-undersampling","dir":"Articles > Tutorial","previous_headings":"Binary classification problems > 2. Rebalancing","what":"ii. Over- and undersampling","title":"Cost-Sensitive Classification","text":"Learner (makeLearner()) supports neither observation class weights proportions classes training data can changed - undersampling. GermanCredit data set (caret::GermanCredit()) positive class Bad receive theoretical weight w = (1 - th)/th = 5. can achieved oversampling class Bad rate 5 undersampling class Good rate 1/5 (using functions oversample() undersample (undersample()). Note example learner trained oversampled task credit.task.. order get training performance original task predictions calculated credit.task. usually prefer resampled performance values, simply calling resample() oversampled task work since predictions based original task. solution create wrapped Learner (makeLearner()) via function makeUndersampleWrapper(). Internally, oversample() called training, predictions done original data. course, can also tune oversampling rate. purpose create OversampleWrapper (makeUndersampleWrapper()). Optimal values parameter osw.rate can obtained using function tuneParams().","code":"credit.task.over = oversample(credit.task, rate = w, cl = \"Bad\") lrn = makeLearner(\"classif.multinom\", trace = FALSE) mod = train(lrn, credit.task.over) pred = predict(mod, task = credit.task) performance(pred, measures = list(credit.costs, mmce)) ## credit.costs         mmce  ##        0.441        0.337 lrn = makeLearner(\"classif.multinom\", trace = FALSE) lrn = makeOversampleWrapper(lrn, osw.rate = w, osw.cl = \"Bad\") lrn  ## Learner classif.multinom.oversampled from package mlr,nnet ## Type: classif ## Name: ; Short name:  ## Class: OversampleWrapper ## Properties: numerics,factors,weights,prob,twoclass,multiclass ## Predict-Type: response ## Hyperparameters: trace=FALSE,osw.rate=5,osw.cl=Bad  r = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) r  ## Resample Result ## Task: GermanCredit ## Learner: classif.multinom.oversampled ## Aggr perf: credit.costs.test.mean=0.5530710,mmce.test.mean=0.3570067 ## Runtime: 0.418571 lrn = makeLearner(\"classif.multinom\", trace = FALSE) lrn = makeOversampleWrapper(lrn, osw.cl = \"Bad\") ps = makeParamSet(makeDiscreteParam(\"osw.rate\", seq(3, 7, 0.25))) ctrl = makeTuneControlGrid() tune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce),   control = ctrl, show.info = FALSE) tune.res ## Tune result: ## Op. pars: osw.rate=7 ## credit.costs.test.mean=0.5219171,mmce.test.mean=0.3819928"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"multi-class-problems","dir":"Articles > Tutorial","previous_headings":"","what":"Multi-class problems","title":"Cost-Sensitive Classification","text":"consider waveform mlbench::mlbench.waveform() data set package mlbench::mlbench() add artificial cost matrix: start creating Task(), cost matrix corresponding performance measure. multi-class case, , thresholding rebalancing correspond cost matrices certain structure \\(c(k,l) = c(l)\\) \\(k\\), \\(l = 1, \\ldots, K\\), \\(k \\neq l\\). condition means cost misclassifying observation independent predicted class label (see Domingos, 1999). Given cost matrix type, theoretical thresholds weights can derived similar manner binary case. Obviously, cost matrix given special structure.","code":"# Task df = mlbench::mlbench.waveform(500) wf.task = makeClassifTask(id = \"waveform\", data = as.data.frame(df), target = \"classes\")  # Cost matrix costs = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3) colnames(costs) = rownames(costs) = getTaskClassLevels(wf.task)  # Performance measure wf.costs = makeCostMeasure(id = \"wf.costs\", name = \"Waveform costs\", costs = costs,   best = 0, worst = 10)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"thresholding-1","dir":"Articles > Tutorial","previous_headings":"Multi-class problems","what":"1. Thresholding","title":"Cost-Sensitive Classification","text":"Given vector positive threshold values long number classes \\(K\\), predicted probabilities classes adjusted dividing corresponding threshold value. class highest adjusted probability predicted. way, binary case, classes low threshold preferred classes larger threshold. can done function setThreshold() shown following example (alternatively predict.threshold option makeLearner()). Note threshold vector needs names correspond class labels. threshold vector th example chosen according average costs true classes 55, 4.5 9. exactly, th corresponds artificial cost matrix structure mentioned -diagonal elements \\(c(2,1) = c(3,1) = 55\\), \\(c(1,2) = c(3,2) = 4.5\\) \\(c(1,3) = c(2,3) = 9\\). threshold vector may optimal leads smaller total costs data set default.","code":"lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") rin = makeResampleInstance(\"CV\", iters = 3, task = wf.task) r = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE) r  ## Resample Result ## Task: waveform ## Learner: classif.rpart ## Aggr perf: wf.costs.test.mean=5.9764447,mmce.test.mean=0.2460020 ## Runtime: 0.0586474  # Calculate thresholds as 1/(average costs of true classes) th = 2/rowSums(costs) names(th) = getTaskClassLevels(wf.task) th  ##          1          2          3  ## 0.01818182 0.22222222 0.11111111  pred.th = setThreshold(r$pred, threshold = th) performance(pred.th, measures = list(wf.costs, mmce))  ##  wf.costs      mmce  ## 4.6863021 0.2919823"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"ii--empirical-thresholding-1","dir":"Articles > Tutorial","previous_headings":"Multi-class problems > 1. Thresholding","what":"ii. Empirical thresholding","title":"Cost-Sensitive Classification","text":"binary case possible tune threshold vector using function tuneThreshold(). Since scaling threshold vector change predicted class labels tuneThreshold() returns threshold values lie [0,1] sum unity. comparison show standardized version theoretically motivated threshold vector chosen .","code":"tune.res = tuneThreshold(pred = r$pred, measure = wf.costs) tune.res ## $th ##          1          2          3  ## 0.04206368 0.47912405 0.47881227  ##  ## $perf ## [1] 4.594 th/sum(th) ##          1          2          3  ## 0.05172414 0.63218391 0.31609195"},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"i--weighting-1","dir":"Articles > Tutorial","previous_headings":"Multi-class problems > 2. Rebalancing","what":"i. Weighting","title":"Cost-Sensitive Classification","text":"multi-class case pass vector weights long number classes \\(K\\) function makeWeightedClassesWrapper(). weight vector can tuned using function tuneParams().","code":"lrn = makeLearner(\"classif.multinom\", trace = FALSE) lrn = makeWeightedClassesWrapper(lrn)  ps = makeParamSet(makeNumericVectorParam(\"wcw.weight\", len = 3, lower = 0, upper = 1)) ctrl = makeTuneControlRandom()  tune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps,   measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE) tune.res ## Tune result: ## Op. pars: wcw.weight=0.544,0.0852... ## wf.costs.test.mean=2.5099079,mmce.test.mean=0.2340379"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/cost_sensitive_classif.html","id":"example-dependent-misclassification-costs","dir":"Articles > Tutorial","previous_headings":"","what":"Example-dependent misclassification costs","title":"Cost-Sensitive Classification","text":"case example-dependent costs create special Task() via function makeCostSensTask(). purpose feature values \\(x\\) \\(n \\times K\\) cost matrix contains cost vectors \\(n\\) examples data set required. use iris (datasets::iris()) data generate artificial cost matrix (see Beygelzimer et al., 2005). mlr provides several wrappers turn regular classification regression methods Learner (makeLearner())s can deal example-dependent costs. makeCostSensClassifWrapper() (wraps classification Learner (makeLearner())): naive approach costs coerced class labels choosing class label minimum cost example. regular classification method used. makeCostSensRegrWrapper() (wraps regression Learner (makeLearner())): individual regression model fitted costs class. prediction step first costs predicted classes class lowest predicted costs selected. makeCostSensWeightedPairsWrapper() (wraps classification Learner (makeLearner())): also known cost-sensitive one-vs-one (CS-OVO) sophisticated currently supported methods. pair classes, binary classifier fitted. observation class label defined element pair minimal costs. fitting, observations weighted absolute difference costs. Prediction performed simple voting. following example use third method. create wrapped Learner (makeLearner()) train CostSensTask(Task()) defined . models corresponding individual pairs can accessed function getLearnerModel(). mlr provides performance measures example-specific cost-sensitive classification. following example calculate mean costs predicted class labels (meancosts) misclassification penalty (mcp). latter measure average difference costs caused predicted class labels, .e., meancosts, costs resulting choosing class lowest cost observation. order compute measures costs test observations required therefore Task() passed performance().","code":"df = iris cost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10) colnames(cost) = levels(iris$Species) rownames(cost) = rownames(iris) df$Species = NULL  costsens.task = makeCostSensTask(id = \"iris\", data = df, cost = cost) costsens.task ## Supervised task: iris ## Type: costsens ## Observations: 150 ## Features: ##    numerics     factors     ordered functionals  ##           4           0           0           0  ## Missings: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 3 ## setosa, versicolor, virginica lrn = makeLearner(\"classif.multinom\", trace = FALSE) lrn = makeCostSensWeightedPairsWrapper(lrn) lrn ## Learner costsens.classif.multinom from package nnet ## Type: costsens ## Name: ; Short name:  ## Class: CostSensWeightedPairsWrapper ## Properties: twoclass,multiclass,numerics,factors ## Predict-Type: response ## Hyperparameters: trace=FALSE  mod = train(lrn, costsens.task) mod ## Model for learner.id=costsens.classif.multinom; learner.class=CostSensWeightedPairsWrapper ## Trained on: task.id = iris; obs = 150; features = 4 ## Hyperparameters: trace=FALSE getLearnerModel(mod) ## [[1]] ## Model for learner.id=classif.multinom; learner.class=classif.multinom ## Trained on: task.id = feats; obs = 150; features = 4 ## Hyperparameters: trace=FALSE ##  ## [[2]] ## Model for learner.id=classif.multinom; learner.class=classif.multinom ## Trained on: task.id = feats; obs = 150; features = 4 ## Hyperparameters: trace=FALSE ##  ## [[3]] ## Model for learner.id=classif.multinom; learner.class=classif.multinom ## Trained on: task.id = feats; obs = 150; features = 4 ## Hyperparameters: trace=FALSE pred = predict(mod, task = costsens.task) pred  ## Prediction: 150 observations ## predict.type: response ## threshold:  ## time: 0.04 ##   id response ## 1  1   setosa ## 2  2   setosa ## 3  3   setosa ## 4  4   setosa ## 5  5   setosa ## 6  6   setosa ## ... (#rows: 150, #cols: 2)  performance(pred, measures = list(meancosts, mcp), task = costsens.task)  ## meancosts       mcp  ##  129.5971  124.8077"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_filter.html","id":"filter-objects","dir":"Articles > Tutorial","previous_headings":"","what":"Filter objects","title":"Integrating Another Filter Method","text":"mlr filter methods objects class Filter (makeFilter()) registered environment called .FilterRegister (listFilterMethods() looks compile list available methods). get know structure let’s closer look \"rank.correlation\" filter. core element $fun calculates feature importance. \"rank.correlation\" filter just extracts data formula task passes base::cor() function. Additionally, Filter (makeFilter()) object $name, short example used annotate graphics (cp. plotFilterValues()), slightly detailed description slot $desc. filter method implemented another package name given $pkg member. Moreover, supported task types feature types listed.","code":"filters = as.list(mlr:::.FilterRegister) filters$rank.correlation ## Filter: 'rank.correlation' ## Packages: '' ## Supported tasks: regr ## Supported features: numerics  str(filters$rank.correlation) ## List of 6 ##  $ name              : chr \"rank.correlation\" ##  $ desc              : chr \"Spearman's correlation between feature and target\" ##  $ pkg               : chr(0) ##  $ supported.tasks   : chr \"regr\" ##  $ supported.features: chr \"numerics\" ##  $ fun               :function (task, nselect, ...) ##   ..- attr(*, \"srcref\")= 'srcref' int [1:8] 325 9 328 3 9 3 2308 2311 ##   .. ..- attr(*, \"srcfile\")=Classes 'srcfilealias', 'srcfile' <environment: 0x55db8ddf6dc0> ##  - attr(*, \"class\")= chr \"Filter\"  filters$rank.correlation$fun ## function(task, nselect, ...) { ##     data = getTaskData(task, target.extra = TRUE) ##     abs(cor(as.matrix(data$data), data$target, use = \"pairwise.complete.obs\", method = \"spearman\")[, 1L]) ##   } ## <bytecode: 0x55db8dc9fe00> ## <environment: namespace:mlr>"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_filter.html","id":"writing-a-new-filter-method","dir":"Articles > Tutorial","previous_headings":"","what":"Writing a new filter method","title":"Integrating Another Filter Method","text":"can integrate filter method using makeFilter(). function generates Filter (makeFilter()) object also registers .FilterRegister environment. arguments makeFilter() correspond slot names Filter (makeFilter()) object . Currently, feature filtering supported supervised learning tasks possible values supported.tasks \"regr\", \"classif\" \"surv\". supported.features can \"numerics\", \"factors\" \"ordered\". fun must function least following formal arguments: task mlr learning Task(). nselect corresponds argument generateFilterValuesData() name specifies number features calculate importance scores. filter methods option stop certain number top-ranked features found order save time ressources number features high. majority filter methods integrated mlr doesn’t support thus nselect ignored cases. exception minimum redundancy maximum relevance filter package mRMRe. ... additional arguments. fun must return named vector feature importance values. convention important features receive highest scores. making use nselect option fun can either return vector nselect scores vector long total numbers features task filled NAs features whose scores weren’t calculated. writing fun many getter functions Task()s come handy, particularly getTaskData(), getTaskFormula() getTaskFeatureNames(). ’s worth closer look getTaskData() provides many options formatting data recoding target variable. short demonstration write totally meaningless filter determines importance features according alphabetical order, .e., giving highest scores features names come first (decreasing = TRUE) last (decreasing = FALSE) alphabet. nonsense.filter now registered mlr shown listFilterMethods(). can use like filter method already integrated mlr (.e., via method argument generateFilterValuesData() fw.method argument makeFilterWrapper(); see also page feature selection.  might also want look source code filter methods already integrated mlr complex meaningful examples.","code":"makeFilter(   name = \"nonsense.filter\",   desc = \"Calculates scores according to alphabetical order of features\",   pkg = character(0),   supported.tasks = c(\"classif\", \"regr\", \"surv\"),   supported.features = c(\"numerics\", \"factors\", \"ordered\"),   fun = function(task, nselect, decreasing = TRUE, ...) {     feats = getTaskFeatureNames(task)     imp = order(feats, decreasing = decreasing)     names(imp) = feats     imp   } ) ## Filter: 'nonsense.filter' ## Packages: '' ## Supported tasks: classif,regr,surv ## Supported features: numerics,factors,ordered listFilterMethods()$id ##  [1] \"anova.test\"                            ##  [2] \"auc\"                                   ##  [3] \"carscore\"                              ##  [4] \"FSelector_chi.squared\"                 ##  [5] \"FSelector_gain.ratio\"                  ##  [6] \"FSelector_information.gain\"            ##  [7] \"FSelector_oneR\"                        ##  [8] \"FSelector_relief\"                      ##  [9] \"FSelector_symmetrical.uncertainty\"     ## [10] \"FSelectorRcpp_gain.ratio\"              ## [11] \"FSelectorRcpp_information.gain\"        ## [12] \"FSelectorRcpp_relief\"                  ## [13] \"FSelectorRcpp_symmetrical.uncertainty\" ## [14] \"kruskal.test\"                          ## [15] \"linear.correlation\"                    ## [16] \"mrmr\"                                  ## [17] \"nonsense.filter\"                       ## [18] \"party_cforest.importance\"              ## [19] \"permutation.importance\"                ## [20] \"praznik_CMIM\"                          ## [21] \"praznik_DISR\"                          ## [22] \"praznik_JMI\"                           ## [23] \"praznik_JMIM\"                          ## [24] \"praznik_MIM\"                           ## [25] \"praznik_MRMR\"                          ## [26] \"praznik_NJMIM\"                         ## [27] \"randomForest_importance\"               ## [28] \"randomForestSRC_importance\"            ## [29] \"randomForestSRC_var.select\"            ## [30] \"ranger_impurity\"                       ## [31] \"ranger_permutation\"                    ## [32] \"rank.correlation\"                      ## [33] \"univariate.model.score\"                ## [34] \"variance\" d = generateFilterValuesData(iris.task, method = c(\"nonsense.filter\", \"anova.test\")) d ## FilterValues: ## Task: iris-example ##            name    type          filter      value ## 1: Petal.Length numeric nonsense.filter    4.00000 ## 2:  Petal.Width numeric nonsense.filter    3.00000 ## 3: Sepal.Length numeric nonsense.filter    2.00000 ## 4:  Sepal.Width numeric nonsense.filter    1.00000 ## 5: Petal.Length numeric      anova.test 1180.16118 ## 6:  Petal.Width numeric      anova.test  960.00715 ## 7: Sepal.Length numeric      anova.test  119.26450 ## 8:  Sepal.Width numeric      anova.test   49.16004  plotFilterValues(d, filter = \"anova.test\") iris.task.filtered = filterFeatures(iris.task, method = \"nonsense.filter\", abs = 2) iris.task.filtered ## Supervised task: iris-example ## Type: classif ## Target: Species ## Observations: 150 ## Features: ##    numerics     factors     ordered functionals  ##           2           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 3 ##     setosa versicolor  virginica  ##         50         50         50  ## Positive class: NA  getTaskFeatureNames(iris.task.filtered) ## [1] \"Petal.Length\" \"Petal.Width\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_imputation.html","id":"example-imputation-using-the-mean","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Imputation using the mean","title":"Creating an Imputation Method","text":"Let’s look function imputeMean (imputations()). imputeMean (imputations()) calls unexported mlr function simpleImpute defined follows. learn function calculates mean non-missing observations column col. mean passed via argument const impute function replaces missing values feature col.","code":"imputeMean = function ()  {     makeImputeMethod(learn = function(data, target, col) mean(data[[col]],          na.rm = TRUE), impute = simpleImpute) } simpleImpute = function (data, target, col, const)  {     if (is.na(const)) {         stopf(\"Error imputing column '%s'. Maybe all input data was missing?\",              col)     }     x = data[[col]]     if (is.logical(x) && !is.logical(const)) {         x = as.factor(x)     }     if (is.factor(x) && const %nin% levels(x)) {         levels(x) = c(levels(x), as.character(const))     }     replace(x, is.na(x), const) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_imputation.html","id":"writing-your-own-imputation-method","dir":"Articles > Tutorial","previous_headings":"","what":"Writing your own imputation method","title":"Creating an Imputation Method","text":"Now let’s write new imputation method: frequently used simple technique longitudinal data last observation carried forward (LOCF). Missing values replaced recent observed value. R code learn function determines last observed value previous NA (values) well corresponding number consecutive NA's (times). impute function generates vector replicating entries values according times replaces NA's feature col. Note function just demonstration lacking checks real-world usage (example ‘happen first value x already missing?’). used impute missing values features Ozone Solar.R airquality (datasets::airquality()) data set.","code":"imputeLOCF = function() {   makeImputeMethod(     learn = function(data, target, col) {       x = data[[col]]       ind = is.na(x)       dind = diff(ind)       lastValue = which(dind == 1)  # position of the last observed value previous to NA       lastNA = which(dind == -1)    # position of the last of potentially several consecutive NA's       values = x[lastValue]         # last observed value previous to NA       times = lastNA - lastValue    # number of consecutive NA's       return(list(values = values, times = times))     },     impute = function(data, target, col, values, times) {       x = data[[col]]       replace(x, is.na(x), rep(values, times))     }   ) } data(airquality) imp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()),   dummy.cols = c(\"Ozone\", \"Solar.R\")) head(imp$data, 10) ##    Ozone Solar.R Wind Temp Month Day Ozone.dummy Solar.R.dummy ## 1     41     190  7.4   67     5   1       FALSE         FALSE ## 2     36     118  8.0   72     5   2       FALSE         FALSE ## 3     12     149 12.6   74     5   3       FALSE         FALSE ## 4     18     313 11.5   62     5   4       FALSE         FALSE ## 5     18     313 14.3   56     5   5        TRUE          TRUE ## 6     28     313 14.9   66     5   6       FALSE          TRUE ## 7     23     299  8.6   65     5   7       FALSE         FALSE ## 8     19      99 13.8   59     5   8       FALSE         FALSE ## 9      8      19 20.1   61     5   9       FALSE         FALSE ## 10     8     194  8.6   69     5  10        TRUE         FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"classes-constructors-and-naming-schemes","dir":"Articles > Tutorial","previous_headings":"","what":"Classes, constructors, and naming schemes","title":"Integrating Another Learner","text":"already know makeLearner() generates object class Learner (makeLearner()). first element class attribute vector name learner class passed cl argument makeLearner(). Obviously, adheres naming conventions \"classif.<R_method_name>\" classification, \"multilabel.<R_method_name>\" multilabel classification, \"regr.<R_method_name>\" regression, \"surv.<R_method_name>\" survival analysis, \"cluster.<R_method_name>\" clustering. Additionally, exist intermediate classes reflect type learning problem, .e., classification learners inherit RLearnerClassif (RLearner()), regression learners RLearnerRegr (RLearner()) . superclasses RLearner() finally Learner (makeLearner()). (sub)classes exist constructor functions makeRLearner (RLearner()), makeRLearnerClassif (RLearner()), makeRLearneRegr (RLearner()) etc. called internally makeLearner(). short side remark: might noticed exist special learner class cost-sensitive classification (costsens) example-specific costs. type learning task currently exclusively handled wrappers like makeCostSensWeightedPairsWrapper(). following show integrate learners five types learning tasks mentioned . Defining completely new type learner special properties fit one existing schemes course possible, much advanced covered . use classification example explain general principles (even interested integrating learner another type learning task might want read following section). Examples types learning tasks shown later .","code":"class(makeLearner(cl = \"classif.lda\")) ## [1] \"classif.lda\"     \"RLearnerClassif\" \"RLearner\"        \"Learner\"  class(makeLearner(cl = \"regr.lm\")) ## [1] \"regr.lm\"      \"RLearnerRegr\" \"RLearner\"     \"Learner\"  class(makeLearner(cl = \"surv.coxph\")) ## [1] \"surv.coxph\"   \"RLearnerSurv\" \"RLearner\"     \"Learner\"  class(makeLearner(cl = \"cluster.kmeans\")) ## [1] \"cluster.kmeans\"  \"RLearnerCluster\" \"RLearner\"        \"Learner\"  class(makeLearner(cl = \"multilabel.rFerns\")) ## [1] \"multilabel.rFerns\"  \"RLearnerMultilabel\" \"RLearner\"           ## [4] \"Learner\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"classification","dir":"Articles > Tutorial","previous_headings":"","what":"Classification","title":"Integrating Another Learner","text":"show Linear Discriminant Analysis (MASS::lda()) package MASS integrated classification learner classif.lda mlr example.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"definition-of-the-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Definition of the learner","title":"Integrating Another Learner","text":"minimal information required define learner mlr name learner, package, parameter set, set properties learner. addition, may provide human-readable name, short name note information relevant users learner. First, name learner. According naming conventions name starts classif. choose classif.lda. Second, need define parameters learner. options can set running change learns, input interpreted, output generated, . mlr provides number functions define parameters, complete list can found documentation LearnerParam (ParamHelpers::LearnerParam()) ParamHelpers package. example, discrete numeric parameters, use makeDiscreteLearnerParam (ParamHelpers::LearnerParam()) makeNumericLearnerParam (ParamHelpers::LearnerParam()) incorporate complete description parameters. include possible values discrete parameters lower upper bounds numeric parameters. Strictly speaking necessary provide bounds parameters information available can estimated, providing accurate specific information makes possible tune learner much better (see section tuning). Next, add information properties learner (see also section learners). types features supported (numerics, factors)? case weights supported? class weights supported? Can method deal missing values features deal NA’s meaningful way (na.omit)? one-class, two-class, multi-class problems supported? Can learner predict posterior probabilities? learner supports class weights name relevant learner parameter can specified via argument class.weights.param. Sometimes parameters allow certain values don’t “fit” formal parameter types. Often, also default values. example: numeric parameter accepts values NA, NULL, \"auto\", . option special.vals, allows list extra values feasible. example missing argument xgboost. accepts numeric values (’s ’s set numeric parameter) NULL default NA. parameter depends another one, requires attribute added argument. Dependent parameters requires field must use quote() expression() define . complete code definition LDA learner. one discrete parameter, method, two continuous ones, nu tol. supports classification problems two classes can deal numeric factor explanatory variables. can predict posterior probabilities.","code":"makeNumericLearnerParam(id = \"missing\", default = NA, tunable = FALSE, when = \"both\",   special.vals = list(NA, NA_real_, NULL)) makeRLearner.classif.lda = function() {   makeRLearnerClassif(     cl = \"classif.lda\",     package = \"MASS\",     par.set = makeParamSet(       makeDiscreteLearnerParam(id = \"method\", default = \"moment\",         values = c(\"moment\", \"mle\", \"mve\", \"t\")),       makeNumericLearnerParam(id = \"nu\", lower = 2,         requires = quote(method == \"t\")),       makeNumericLearnerParam(id = \"tol\", default = 1e-4, lower = 0),       makeDiscreteLearnerParam(id = \"predict.method\",         values = c(\"plug-in\", \"predictive\", \"debiased\"),         default = \"plug-in\", when = \"predict\"),       makeLogicalLearnerParam(id = \"CV\", default = FALSE, tunable = FALSE)     ),     properties = c(\"twoclass\", \"multiclass\", \"numerics\", \"factors\", \"prob\"),     name = \"Linear Discriminant Analysis\",     short.name = \"lda\",     note = \"Learner param 'predict.method' maps to 'method' in predict.lda.\"   ) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"creating-the-training-function-of-the-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Creating the training function of the learner","title":"Integrating Another Learner","text":"learner defined, need tell mlr call train model. name function start trainLearner., followed mlr name learner defined (classif.lda ). prototype function looks follows. function must fit model data task .task regard subset defined integer vector .subset parameters passed ... arguments. Usually, data extracted task using getTaskData(). take care subsetting well. must return fitted model. mlr assumes special data type return value – passed predict function going define , special code learner may need can encapsulated . example, definition function looks like . addition data task, also need formula describes predict. use function getTaskFormula() extract task.","code":"function(.learner, .task, .subset, .weights = NULL, ...) { } trainLearner.classif.lda = function (.learner, .task, .subset, .weights = NULL, ...)  {     f = getTaskFormula(.task)     MASS::lda(f, data = getTaskData(.task, .subset), ...) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"creating-the-prediction-method","dir":"Articles > Tutorial","previous_headings":"","what":"Creating the prediction method","title":"Integrating Another Learner","text":"Finally, prediction function needs defined. name function starts predictLearner., followed mlr name learner. prototype function follows. must predict new observations data.frame .newdata wrapped model .model, returned training function. actual model learner built stored $learner.model member can accessed simply .model$learner.model. classification, return factor predicted classes .learner$predict.type \"response\", matrix predicted probabilities .learner$predict.type \"prob\" type prediction supported learner. latter case matrix must number columns classes task columns named class names. definition LDA looks like . pretty much just straight pass-arguments base::predict() function extraction prediction data depending type prediction requested.","code":"function(.learner, .model, .newdata, ...) { } predictLearner.classif.lda = function (.learner, .model, .newdata, predict.method = \"plug-in\",      ...)  {     p = predict(.model$learner.model, newdata = .newdata, method = predict.method,          ...)     if (.learner$predict.type == \"response\") {         return(p$class)     }     else {         return(p$posterior)     } }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"regression","dir":"Articles > Tutorial","previous_headings":"","what":"Regression","title":"Integrating Another Learner","text":"main difference regression type predictions different (numeric instead labels probabilities) properties relevant. particular, whether one-, two-, multi-class problems posterior probabilities supported applicable. Apart , everything explained applies. definition earth::earth() learner. data passed straight /train/predict functions learner.","code":"makeRLearner.regr.earth = function() {   makeRLearnerRegr(     cl = \"regr.earth\",     package = \"earth\",     par.set = makeParamSet(       makeLogicalLearnerParam(id = \"keepxy\", default = FALSE, tunable = FALSE),       makeNumericLearnerParam(id = \"trace\", default = 0, upper = 10, tunable = FALSE),       makeIntegerLearnerParam(id = \"degree\", default = 1L, lower = 1L),       makeNumericLearnerParam(id = \"penalty\"),       makeIntegerLearnerParam(id = \"nk\", lower = 0L),       makeNumericLearnerParam(id = \"thres\", default = 0.001),       makeIntegerLearnerParam(id = \"minspan\", default = 0L),       makeIntegerLearnerParam(id = \"endspan\", default = 0L),       makeNumericLearnerParam(id = \"newvar.penalty\", default = 0),       makeIntegerLearnerParam(id = \"fast.k\", default = 20L, lower = 0L),       makeNumericLearnerParam(id = \"fast.beta\", default = 1),       makeDiscreteLearnerParam(id = \"pmethod\", default = \"backward\",         values = c(\"backward\", \"none\", \"exhaustive\", \"forward\", \"seqrep\", \"cv\")),       makeIntegerLearnerParam(id = \"nprune\")     ),     properties = c(\"numerics\", \"factors\"),     name = \"Multivariate Adaptive Regression Splines\",     short.name = \"earth\",     note = \"\"   ) } trainLearner.regr.earth = function (.learner, .task, .subset, .weights = NULL, ...)  {     f = getTaskFormula(.task)     earth::earth(f, data = getTaskData(.task, .subset), ...) } predictLearner.regr.earth = function (.learner, .model, .newdata, ...)  {     predict(.model$learner.model, newdata = .newdata)[, 1L] }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"survival-analysis","dir":"Articles > Tutorial","previous_headings":"","what":"Survival analysis","title":"Integrating Another Learner","text":"survival analysis, return -called linear predictors order compute default measure task type, cindex (.learner$predict.type == \"response\"). .learner$predict.type == \"prob\", substantially meaningful measure (yet). may either ignore case return something like predicted survival curves (cf. example ). three properties specific survival learners: “rcens”, “lcens” “icens”, defining type(s) censoring learner can handle – right, left /interval censored. Let’s look Cox Proportional Hazard Model (survival::coxph()) package survival integrated survival learner surv.coxph mlr example:","code":"makeRLearner.surv.coxph = function() {   makeRLearnerSurv(     cl = \"surv.coxph\",     package = \"survival\",     par.set = makeParamSet(       makeDiscreteLearnerParam(id = \"ties\", default = \"efron\",         values = c(\"efron\", \"breslow\", \"exact\")),       makeLogicalLearnerParam(id = \"singular.ok\", default = TRUE),       makeNumericLearnerParam(id = \"eps\", default = 1e-09, lower = 0),       makeNumericLearnerParam(id = \"toler.chol\",         default = .Machine$double.eps^0.75, lower = 0),       makeIntegerLearnerParam(id = \"iter.max\", default = 20L, lower = 1L),       makeNumericLearnerParam(id = \"toler.inf\",         default = sqrt(.Machine$double.eps^0.75), lower = 0),       makeIntegerLearnerParam(id = \"outer.max\", default = 10L, lower = 1L),       makeLogicalLearnerParam(id = \"model\", default = FALSE, tunable = FALSE),       makeLogicalLearnerParam(id = \"x\", default = FALSE, tunable = FALSE),       makeLogicalLearnerParam(id = \"y\", default = TRUE, tunable = FALSE)     ),     properties = c(\"missings\", \"numerics\", \"factors\", \"weights\", \"prob\", \"rcens\"),     name = \"Cox Proportional Hazard Model\",     short.name = \"coxph\",     note = \"\"   ) } trainLearner.surv.coxph = function (.learner, .task, .subset, .weights = NULL, ...)  {     f = getTaskFormula(.task)     data = getTaskData(.task, subset = .subset)     if (is.null(.weights)) {         survival::coxph(formula = f, data = data, ...)     }     else {         survival::coxph(formula = f, data = data, weights = .weights,              ...)     } } predictLearner.surv.coxph = function (.learner, .model, .newdata, ...)  {     predict(.model$learner.model, newdata = .newdata, type = \"lp\",          ...) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"clustering","dir":"Articles > Tutorial","previous_headings":"","what":"Clustering","title":"Integrating Another Learner","text":"clustering, return numeric vector IDs clusters respective datum assigned . numbering start 1. definition FarthestFirst (RWeka::FarthestFirst()) learner RWeka package. Weka starts IDs clusters 0, add 1 predicted clusters. RWeka different way setting learner parameters; use special Weka_control function .","code":"makeRLearner.cluster.FarthestFirst = function() {   makeRLearnerCluster(     cl = \"cluster.FarthestFirst\",     package = \"RWeka\",     par.set = makeParamSet(       makeIntegerLearnerParam(id = \"N\", default = 2L, lower = 1L),       makeIntegerLearnerParam(id = \"S\", default = 1L, lower = 1L),       makeLogicalLearnerParam(id = \"output-debug-info\", default = FALSE,         tunable = FALSE)     ),     properties = c(\"numerics\"),     name = \"FarthestFirst Clustering Algorithm\",     short.name = \"farthestfirst\"   ) } trainLearner.cluster.FarthestFirst = function (.learner, .task, .subset, .weights = NULL, ...)  {     ctrl = RWeka::Weka_control(...)     RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl) } predictLearner.cluster.FarthestFirst = function (.learner, .model, .newdata, ...)  {     as.integer(predict(.model$learner.model, .newdata, ...)) +          1L }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"multilabel-classification","dir":"Articles > Tutorial","previous_headings":"","what":"Multilabel classification","title":"Integrating Another Learner","text":"stated multilabel section, multilabel classification methods can divided problem transformation methods algorithm adaptation methods. moment problem transformation method implemented mlr binary relevance method (makeMultilabelBinaryRelevanceWrapper()). Integrating methods requires good knowledge architecture mlr package. integration algorithm adaptation multilabel classification learner easier works similar normal multiclass-classification. contrast multiclass case, learner properties relevant. particular, whether one-, two-, multi-class problems supported applicable. Furthermore prediction function output must matrix prediction label one column names labels column names. .learner$predict.type \"response\" predictions must logical. .learner$predict.type \"prob\" type prediction supported learner, matrix must consist predicted probabilities. definition rFerns::rFerns() learner rFerns package, support probability predictions.","code":"makeRLearner.multilabel.rFerns = function() {   makeRLearnerMultilabel(     cl = \"multilabel.rFerns\",     package = \"rFerns\",     par.set = makeParamSet(       makeIntegerLearnerParam(id = \"depth\", default = 5L),       makeIntegerLearnerParam(id = \"ferns\", default = 1000L)     ),     properties = c(\"numerics\", \"factors\", \"ordered\"),     name = \"Random ferns\",     short.name = \"rFerns\",     note = \"\"   ) } trainLearner.multilabel.rFerns = function (.learner, .task, .subset, .weights = NULL, ...)  {     d = getTaskData(.task, .subset, target.extra = TRUE)     rFerns::rFerns(x = d$data, y = as.matrix(d$target), ...) } predictLearner.multilabel.rFerns = function (.learner, .model, .newdata, ...)  {     as.matrix(predict(.model$learner.model, .newdata, ...)) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"creating-a-new-method-for-extracting-feature-importance-values","dir":"Articles > Tutorial","previous_headings":"","what":"Creating a new method for extracting feature importance values","title":"Integrating Another Learner","text":"learners, example decision trees random forests, can calculate feature importance values, can extracted fitted model (makeWrappedModel()) using function getFeatureImportance(). newly integrated learner supports need add \"featimp\" learner properties implement new S3 method function getFeatureImportanceLearner() (later called internally getFeatureImportance()) order make work. method takes Learner() .learner, WrappedModel (makeWrappedModel()) .model potential arguments calculates extracts feature importance. must return named vector importance values. two simple examples. case \"classif.rpart\" feature importance values can easily extracted fitted model. randomForestSRC::rfsrc() package randomForestSRC function randomForestSRC::vimp() called.","code":"getFeatureImportanceLearner.classif.rpart = function (.learner, .model, ...)  {     mod = getLearnerModel(.model, more.unwrap = TRUE)     mod$variable.importance } getFeatureImportanceLearner.classif.randomForestSRC = function (.learner, .model, ...)  {     mod = getLearnerModel(.model, more.unwrap = TRUE)     randomForestSRC::vimp(mod, ...)$importance[, \"all\"] }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"creating-a-new-method-for-extracting-out-of-bag-predictions","dir":"Articles > Tutorial","previous_headings":"","what":"Creating a new method for extracting out-of-bag predictions","title":"Integrating Another Learner","text":"Many ensemble learners generate --bag predictions (OOB predictions) automatically. mlr provides function getOOBPreds() access predictions mlr framework. newly integrated learner able calculate OOB predictions want able access mlr via getOOBPreds() need add \"oobpreds\" learner properties implement new S3 method function getOOBPredsLearner() (later called internally getOOBPreds()). method takes Learner (makeLearner()) .learner WrappedModel (makeWrappedModel()) .model extracts OOB predictions. must return predictions format predictLearner() function.","code":"getOOBPredsLearner.classif.randomForest = function (.learner, .model)  {     if (.learner$predict.type == \"response\") {         m = getLearnerModel(.model, more.unwrap = TRUE)         unname(m$predicted)     }     else {         getLearnerModel(.model, more.unwrap = TRUE)$votes     } }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"registering-your-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Registering your learner","title":"Integrating Another Learner","text":"interface code new learning algorithm exists locally, .e., (yet) merged mlr live extra package proper namespace might want register new S3 methods make sure found , e.g., listLearners(). can follows: written methods, example order extract feature importance values --bag predictions also need registered manner, example: new learner work parallelization, may export new methods explicitly:","code":"registerS3method(\"makeRLearner\", \"<awesome_new_learner_class>\",   makeRLearner.<awesome_new_learner_class>) registerS3method(\"trainLearner\", \"<awesome_new_learner_class>\",   trainLearner.<awesome_new_learner_class>) registerS3method(\"predictLearner\", \"<awesome_new_learner_class>\",   predictLearner.<awesome_new_learner_class>) registerS3method(\"getFeatureImportanceLearner\", \"<awesome_new_learner_class>\",   getFeatureImportanceLearner.<awesome_new_learner_class>) parallelExport(\"trainLearner.<awesome_new_learner_class>\",   \"predictLearner.<awesome_new_learner_class>\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"further-information-for-developers","dir":"Articles > Tutorial","previous_headings":"","what":"Further information for developers","title":"Integrating Another Learner","text":"haven’t written learner interface private use , intend send pull request included mlr package things take care , importantly unit testing! general information contributing package, unit testing, version control setup like please also read coding guidelines mlr Wiki. R file containing interface code adhere naming convention RLearner_<type>_<learner_name>.R, e.g., RLearner_classif_lda.R, see example https://github.com/mlr-org/mlr/blob/main/R/RLearner_classif_lda.R contain necessary roxygen @export tags register S3 methods NAMESPACE. learner interfaces work box without requiring parameters set, e.g., train(\"classif.lda\", iris.task) run. Sometimes, makes necessary change set additional defaults explained – important – informing user note. parameter set learner complete possible. Every learner interface must unit tested.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_learner.html","id":"unit-testing","dir":"Articles > Tutorial","previous_headings":"","what":"Unit testing","title":"Integrating Another Learner","text":"tests make sure get results learner invoked mlr interface using original functions. familiar want learn unit testing package testthat look Testing chapter Hadley Wickham’s R packages. mlr unit tests following directory: https://github.com/mlr-org/mlr/tree/main/tests/testthat. learner interface individual file whose name follows scheme test_<type>_<learner_name>.R, example https://github.com/mlr-org/mlr/blob/main/tests/testthat/test_classif_lda.R. snippet tests lda interface https://github.com/mlr-org/mlr/blob/main/tests/testthat/test_classif_lda.R. tests make use numerous helper objects helper functions. defined helper_ files https://github.com/mlr-org/mlr/blob/main/tests/testthat/. code first line just loads package MASS skips test package available. objects multiclass.formula, multiclass.train, multiclass.test etc. defined https://github.com/mlr-org/mlr/blob/main/tests/testthat/helper_objects.R. tried choose fairly self-explanatory names: example multiclass indicates multi-class classification problem, multiclass.train contains data training, multiclass.formula formula object etc. test fits lda model training set makes predictions test set using original functions MASS::lda() MASS:predict.lda(). helper functions testSimple testProb perform training prediction data using mlr interface – testSimple predict.type = \"response testProbs predict.type = \"prob\" – check predicted class labels probabilities coincide outcomes p$class p$posterior. order get reproducible results seeding required many learners. \"mlr.debug.seed\" works follows: invoking tests option \"mlr.debug.seed\" set (see https://github.com/mlr-org/mlr/blob/main/tests/testthat/helper_zzz.R), set.seed(getOption(\"mlr.debug.seed\")) used specify seed. Internally, mlr’s train predict.WrappedModel functions check \"mlr.debug.seed\" option set yes, also specify seed. Note option \"mlr.debug.seed\" set testing, seeding happens normal usage mlr. Let’s look second example. Many learners parameters commonly changed tuned important make sure passed correctly. snippet https://github.com/mlr-org/mlr/blob/main/tests/testthat/test_regr_randomForest.R. tested parameter configurations collected parset.list. order make sure default parameter configuration tested first element parset.list empty list (base::list()). simply loop parameter settings store resulting predictions old.predicts.list. helper function testSimpleParsets using mlr interface compares outcomes. Additional tests individual learners also general tests loop integrated learners make example sure learners correct properties (e.g. learner property \"factors\" can cope factor (base::factor()) features, learner property \"weights\" takes observation weights account properly etc.). example https://github.com/mlr-org/mlr/blob/main/tests/testthat/test_learners_all_classif.R runs classification learners. Similar tests exist types learning methods like regression, cluster survival analysis well multilabel classification. order run tests , e.g., classification learners machine can invoke tests within R command line using Michel’s rt tool","code":"test_that(\"classif_lda\", {   requirePackagesOrSkip(\"MASS\", default.method = \"load\")    set.seed(getOption(\"mlr.debug.seed\"))   m = MASS::lda(formula = multiclass.formula, data = multiclass.train)   set.seed(getOption(\"mlr.debug.seed\"))   p = predict(m, newdata = multiclass.test)    testSimple(\"classif.lda\", multiclass.df, multiclass.target, multiclass.train.inds, p$class)   testProb(\"classif.lda\", multiclass.df, multiclass.target, multiclass.train.inds, p$posterior) }) test_that(\"regr_randomForest\", {   requirePackagesOrSkip(\"randomForest\", default.method = \"load\")    parset.list = list(     list(),     list(ntree = 5, mtry = 2),     list(ntree = 5, mtry = 4),     list(proximity = TRUE, oob.prox = TRUE),     list(nPerm = 3)   )    old.predicts.list = list()    for (i in 1:length(parset.list)) {     parset = parset.list[[i]]     pars = list(formula = regr.formula, data = regr.train)     pars = c(pars, parset)     set.seed(getOption(\"mlr.debug.seed\"))     m = do.call(randomForest::randomForest, pars)     set.seed(getOption(\"mlr.debug.seed\"))     p = predict(m, newdata = regr.test, type = \"response\")     old.predicts.list[[i]] = p   }    testSimpleParsets(\"regr.randomForest\", regr.df, regr.target,     regr.train.inds, old.predicts.list, parset.list) }) devtools::test(\"mlr\", filter = \"classif\") rtest --filter=classif"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_measure.html","id":"performance-measures-and-aggregation-schemes","dir":"Articles > Tutorial","previous_headings":"","what":"Performance measures and aggregation schemes","title":"Integrating Another Measure","text":"Performance measures mlr objects class Measure (makeMeasure()). example mse (mean squared error) looks follows. See Measure (makeMeasure()) documentation page detailed description object slots. core slot $fun contains function calculates performance value. actual work done function measureMSE (measures()). Similar functions, generally adhering naming scheme measure followed capitalized measure ID, exist performance measures. See [measures()] help page complete list. Just Task() Learner (makeLearner()) objects Measure (makeMeasure()) identifier $id example used annotate results plots. plots also option use longer measure $name instead. See tutorial page visualization information. Moreover, Measure (makeMeasure()) includes number $properties indicate types learning problems suitable information required calculate . Obviously, measures need Prediction() object (\"req.pred\") , supervised problems, true values target variable(s) (\"req.truth\"). can use functions getMeasureProperties (MeasureProperties()) hasMeasureProperties (MeasureProperties()) determine properties Measure (makeMeasure()). Moreover, listMeasureProperties() shows measure properties currently available mlr. Additional properties, Measure (makeMeasure()) knows extreme values $best $worst wants minimized maximized ($minimize) tuning feature selection. resampling slot $aggr specifies overall performance across resampling iterations calculated. Typically, just matter aggregating performance values obtained test sets perf.test training sets perf.train simple function. far common scheme test.mean (aggregations()), .e., unweighted mean performances test sets. aggregation schemes objects class Aggregation() function slot $fun actual work. $properties member indicates predictions (performance values) training test data sets required calculate aggregation. can change aggregation scheme Measure (makeMeasure()) via function setAggregation(). See tutorial page resampling examples ?aggregations() help page available aggregation schemes. can construct Measure (makeMeasure()) Aggregation() objects via functions makeMeasure(), makeCostMeasure(), makeCustomResampledMeasure() makeAggregation(). examples shown following.","code":"str(mse) ## List of 10 ##  $ id        : chr \"mse\" ##  $ minimize  : logi TRUE ##  $ properties: chr [1:3] \"regr\" \"req.pred\" \"req.truth\" ##  $ fun       :function (task, model, pred, feats, extra.args) ##   ..- attr(*, \"srcref\")= 'srcref' int [1:8] 118 9 120 3 9 3 26476 26478 ##   .. ..- attr(*, \"srcfile\")=Classes 'srcfilealias', 'srcfile' <environment: 0x5596902b5300> ##  $ extra.args: list() ##  $ best      : num 0 ##  $ worst     : num Inf ##  $ name      : chr \"Mean of squared errors\" ##  $ note      : chr \"Defined as: mean((response - truth)^2)\" ##  $ aggr      :List of 4 ##   ..$ id        : chr \"test.mean\" ##   ..$ name      : chr \"Test mean\" ##   ..$ fun       :function (task, perf.test, perf.train, measure, group, pred) ##   .. ..- attr(*, \"srcref\")= 'srcref' int [1:8] 43 9 43 83 9 83 19157 19157 ##   .. .. ..- attr(*, \"srcfile\")=Classes 'srcfilealias', 'srcfile' <environment: 0x55969028dc98> ##   ..$ properties: chr \"req.test\" ##   ..- attr(*, \"class\")= chr \"Aggregation\" ##  - attr(*, \"class\")= chr \"Measure\"  mse$fun ## function(task, model, pred, feats, extra.args) { ##     measureMSE(pred$data$truth, pred$data$response) ##   } ## <bytecode: 0x55968f621870> ## <environment: namespace:mlr>  measureMSE ## function(truth, response) { ##   mean((response - truth)^2) ## } ## <bytecode: 0x559691533288> ## <environment: namespace:mlr> listMeasureProperties() ##  [1] \"classif\"       \"classif.multi\" \"multilabel\"    \"regr\"          ##  [5] \"surv\"          \"cluster\"       \"costsens\"      \"req.pred\"      ##  [9] \"req.truth\"     \"req.task\"      \"req.feats\"     \"req.model\"     ## [13] \"req.prob\" str(test.mean) ## List of 4 ##  $ id        : chr \"test.mean\" ##  $ name      : chr \"Test mean\" ##  $ fun       :function (task, perf.test, perf.train, measure, group, pred) ##  $ properties: chr \"req.test\" ##  - attr(*, \"class\")= chr \"Aggregation\"  test.mean$fun ## function (task, perf.test, perf.train, measure, group, pred) ## mean(perf.test) ## <bytecode: 0xc0a82e8> ## <environment: namespace:mlr>"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_measure.html","id":"constructing-a-performance-measure","dir":"Articles > Tutorial","previous_headings":"","what":"Constructing a performance measure","title":"Integrating Another Measure","text":"Function makeMeasure() provides simple way construct performance measure. exemplified re-implementing mean misclassification error mmce. first write function computes measure basis true predicted class labels. Note function must certain formal arguments listed documentation makeMeasure(). Measure (makeMeasure()) object created work usual performance() function. See R documentation makeMeasure() details various parameters.","code":"# Define a function that calculates the misclassification rate my.mmce.fun = function(task, model, pred, feats, extra.args) {   tb = table(getPredictionResponse(pred), getPredictionTruth(pred))   1 - sum(diag(tb)) / sum(tb) }  # Generate the Measure object my.mmce = makeMeasure(   id = \"my.mmce\", name = \"My Mean Misclassification Error\",   properties = c(\"classif\", \"classif.multi\", \"req.pred\", \"req.truth\"),   minimize = TRUE, best = 0, worst = 1,   fun = my.mmce.fun )  # Train a learner and make predictions mod = train(\"classif.lda\", iris.task) pred = predict(mod, task = iris.task)  # Calculate the performance using the new measure performance(pred, measures = my.mmce) ## my.mmce  ##    0.02  # Apparently the result coincides with the mlr implementation performance(pred, measures = mmce) ## mmce  ## 0.02"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_measure.html","id":"constructing-a-measure-for-ordinary-misclassification-costs","dir":"Articles > Tutorial","previous_headings":"","what":"Constructing a measure for ordinary misclassification costs","title":"Integrating Another Measure","text":"depth explanations details see tutorial page cost-sensitive classification. create measure involves ordinary, .e., class-dependent misclassification costs can use function makeCostMeasure(). first need define cost matrix. rows indicate true columns predicted classes rows columns named class labels. cost matrix can wrapped Measure (makeMeasure()) object predictions can evaluated usual performance() function. See R documentation function makeCostMeasure() details various parameters.","code":"# Create the cost matrix costs = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3) rownames(costs) = colnames(costs) = getTaskClassLevels(iris.task)  # Encapsulate the cost matrix in a Measure object my.costs = makeCostMeasure(   id = \"my.costs\", name = \"My Costs\",   costs = costs,   minimize = TRUE, best = 0, worst = 3 )  # Train a learner and make a prediction mod = train(\"classif.lda\", iris.task) pred = predict(mod, newdata = iris)  # Calculate the average costs performance(pred, measures = my.costs) ##   my.costs  ## 0.02666667"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_measure.html","id":"creating-an-aggregation-scheme","dir":"Articles > Tutorial","previous_headings":"","what":"Creating an aggregation scheme","title":"Integrating Another Measure","text":"possible create aggregation scheme using function makeAggregation(). need specify identifier id, properties, write function actual aggregation. Optionally, can name aggregation scheme. Possible settings properties \"req.test\" \"req.train\" predictions either training test sets required, vector c(\"req.train\", \"req.test\") needed. aggregation function must certain signature detailed documentation makeAggregation(). Usually, need performance values test sets perf.test training sets perf.train. rare cases, e.g., Prediction() object pred information stored Task() object might required obtain aggregated performance. example look definition function test.join (aggregations()).","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/create_measure.html","id":"example-evaluating-the-range-of-measures","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Evaluating the range of measures","title":"Integrating Another Measure","text":"Let’s say interested range performance values obtained individual test sets. perf.train perf.test numerical vectors containing performances train test data sets. cases (unless using bootstrap resampling strategy set predict = \"\" makeResampleDesc()) perf.train vector empty. Now can run feature selection based first measure provided list see measures turn .  plot shows range versus mean misclassification error. value y-axis thus corresponds length error bars. (Note points error bars jittered y-direction.)","code":"my.range.aggr = makeAggregation(id = \"test.range\", name = \"Test Range\",   properties = \"req.test\",   fun = function (task, perf.test, perf.train, measure, group, pred)     diff(range(perf.test)) ) # mmce with default aggregation scheme test.mean ms1 = mmce  # mmce with new aggregation scheme my.range.aggr ms2 = setAggregation(ms1, my.range.aggr)  # Minimum and maximum of the mmce over test sets ms1min = setAggregation(ms1, test.min) ms1max = setAggregation(ms1, test.max)  # Feature selection rdesc = makeResampleDesc(\"CV\", iters = 3) res = selectFeatures(\"classif.rpart\", iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max),   control = makeFeatSelControlExhaustive(), show.info = FALSE)  # Optimization path, i.e., performances for the 16 possible feature subsets perf.data = as.data.frame(res$opt.path) head(perf.data[1:8]) ##   Sepal.Length Sepal.Width Petal.Length Petal.Width mmce.test.mean ## 1            0           0            0           0     0.68000000 ## 2            1           0            0           0     0.34000000 ## 3            0           1            0           0     0.48666667 ## 4            0           0            1           0     0.06666667 ## 5            0           0            0           1     0.04666667 ## 6            1           1            0           0     0.36000000 ##   mmce.test.range mmce.test.min mmce.test.max ## 1            0.20          0.56          0.76 ## 2            0.32          0.18          0.50 ## 3            0.18          0.42          0.60 ## 4            0.06          0.04          0.10 ## 5            0.06          0.02          0.08 ## 6            0.22          0.28          0.50  pd = position_jitter(width = 0.005, height = 0) p = ggplot(aes(x = mmce.test.range, y = mmce.test.mean, ymax = mmce.test.max, ymin = mmce.test.min,   color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) +   geom_pointrange(position = pd) +   coord_flip() print(p)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"filter-methods","dir":"Articles > Tutorial","previous_headings":"","what":"Filter methods","title":"Feature Selection","text":"Filter methods assign importance value feature. Based values features can ranked feature subset can selected. can see algorithms implemented.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"calculating-the-feature-importance","dir":"Articles > Tutorial","previous_headings":"","what":"Calculating the feature importance","title":"Feature Selection","text":"Different methods calculating feature importance built mlr’s function generateFilterValuesData(). Currently, classification, regression survival analysis tasks supported. table showing available methods can found article filter methods. basic approach use generateFilterValuesData() directly Task() character string specifying filter method. fv FilterValues() object fv$data contains data.frame gives importance values features. Optionally, vector filter methods can passed. bar plot importance values individual features can obtained using function plotFilterValues().  default plotFilterValues() create facetted subplots multiple filter methods passed input generateFilterValuesData(). According \"information.gain\" measure, Petal.Width Petal.Length contain information target variable Species.","code":"fv = generateFilterValuesData(iris.task, method = \"FSelectorRcpp_information.gain\") fv ## FilterValues: ## Task: iris-example ##            name    type                         filter     value ## 1:  Petal.Width numeric FSelectorRcpp_information.gain 0.9554360 ## 2: Petal.Length numeric FSelectorRcpp_information.gain 0.9402853 ## 3: Sepal.Length numeric FSelectorRcpp_information.gain 0.4521286 ## 4:  Sepal.Width numeric FSelectorRcpp_information.gain 0.2672750 fv2 = generateFilterValuesData(iris.task,   method = c(\"FSelectorRcpp_information.gain\", \"FSelectorRcpp_relief\")) fv2$data ##            name    type                         filter     value ## 1: Sepal.Length numeric FSelectorRcpp_information.gain 0.4521286 ## 2:  Sepal.Width numeric FSelectorRcpp_information.gain 0.2672750 ## 3: Petal.Length numeric FSelectorRcpp_information.gain 0.9402853 ## 4:  Petal.Width numeric FSelectorRcpp_information.gain 0.9554360 ## 5: Sepal.Length numeric           FSelectorRcpp_relief 0.2197222 ## 6:  Sepal.Width numeric           FSelectorRcpp_relief 0.1120833 ## 7: Petal.Length numeric           FSelectorRcpp_relief 0.3854237 ## 8:  Petal.Width numeric           FSelectorRcpp_relief 0.3620833 plotFilterValues(fv2, filter = \"FSelectorRcpp_information.gain\") +   ggpubr::theme_pubr()"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"selecting-a-feature-subset","dir":"Articles > Tutorial","previous_headings":"","what":"Selecting a feature subset","title":"Feature Selection","text":"mlrs function filterFeatures() can create new Task() leaving features lower importance. several ways select feature subset based feature importance values: Keep certain absolute number (abs) features highest importance. Keep certain percentage (perc) features highest importance. Keep features whose importance exceeds certain threshold value (threshold). Function filterFeatures() supports three methods shown following example. Moreover, can either specify method calculating feature importance can use previously computed importance values via argument fval.","code":"# Keep the 2 most important features filtered.task = filterFeatures(iris.task, method = \"FSelectorRcpp_information.gain\", abs = 2)  # Keep the 25% most important features filtered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)  # Keep all features with importance greater than 0.5 filtered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5) filtered.task ## Supervised task: iris-example ## Type: classif ## Target: Species ## Observations: 150 ## Features: ##    numerics     factors     ordered functionals  ##           2           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 3 ##     setosa versicolor  virginica  ##         50         50         50  ## Positive class: NA"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"fuse-a-learner-with-a-filter-method","dir":"Articles > Tutorial","previous_headings":"","what":"Fuse a learner with a filter method","title":"Feature Selection","text":"Often feature selection based filter method part data preprocessing subsequent step learning method applied filtered data. proper experimental setup might want automate selection features can part validation method choice. Learner (makeLearner()) can fused filter method function makeFilterWrapper(). resulting Learner (makeLearner()) additional class attribute FilterWrapper(). advantage filter parameters (fw.method, fw.perc. fw.abs) can now treated hyperparameters. can tuned nested CV setting level algorithm hyperparameters. can think “tuning dataset”.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"using-fixed-parameters","dir":"Articles > Tutorial","previous_headings":"Fuse a learner with a filter method","what":"Using fixed parameters","title":"Feature Selection","text":"following example calculate 10-fold cross-validated error rate mmce k-nearest neighbor classifier (FNN::fnn()) preceding feature selection iris (datasets::iris()) data set. use information.gain importance measure aim subset dataset two features highest importance. resampling iteration feature selection carried corresponding training data set fitting learner. may want know features used. Luckily, called resample() argument models = TRUE, means r$models contains list models (makeWrappedModel()) fitted individual resampling iterations. order access selected feature subsets can call getFilteredFeatures() model. result shows ten folds always Petal.Length Petal.Width chosen (remember wanted best two, .e. \\(10 \\times 2\\)). selection features seems stable dataset. features Sepal.Length Sepal.Width make single fold.","code":"lrn = makeFilterWrapper(   learner = \"classif.fnn\",   fw.method = \"FSelectorRcpp_information.gain\", fw.abs = 2) rdesc = makeResampleDesc(\"CV\", iters = 10) r = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE) r$aggr ## mmce.test.mean  ##     0.05333333 sfeats = sapply(r$models, getFilteredFeatures) table(sfeats) ## sfeats ## Petal.Length  Petal.Width  ##           10           10"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"tuning-the-size-of-the-feature-subset","dir":"Articles > Tutorial","previous_headings":"Fuse a learner with a filter method","what":"Tuning the size of the feature subset","title":"Feature Selection","text":"examples number/percentage features select threshold value arbitrarily chosen. However, usually unclear subset features results best performance. answer question, can tune number features taken (ranking chosen algorithms applied) subset fold. Three tunable parameters exist mlr, documented makeFilterWrapper(): percentage features selected (fw.perc) absolute number features selected (fw.abs) threshold filter method (fw.threshold) following regression example consider BostonHousing (mlbench::BostonHousing()) data set. use Support Vector Machine determine optimal percentage value feature selection 3-fold cross-validated mean squared error (mse()) learner minimal. Additionally, tune hyperparameters algorithm time. search strategy tuning random search five iterations used. performance percentage values visited tuning : optimal percentage corresponding performance can accessed follows: tuning can generate new wrapped learner optimal percentage value use (e.g. predict new data).","code":"lrn = makeFilterWrapper(   learner = \"regr.ksvm\",   fw.method = \"FSelectorRcpp_information.gain\",   more.args = list(\"FSelectorRcpp_information.gain\" = list(     equal = TRUE))) ps = makeParamSet(   makeNumericParam(\"fw.perc\", lower = 0, upper = 1),   makeNumericParam(\"C\",     lower = -10, upper = 10,     trafo = function(x) 2^x),   makeNumericParam(\"sigma\",     lower = -10, upper = 10,     trafo = function(x) 2^x) ) rdesc = makeResampleDesc(\"CV\", iters = 3) res = tuneParams(lrn,   task = bh.task, resampling = rdesc, par.set = ps,   control = makeTuneControlRandom(maxit = 5)) ## [Tune] Started tuning learner regr.ksvm.filtered for parameter set: ##            Type len Def    Constr Req Tunable Trafo ## fw.perc numeric   -   -    0 to 1   -    TRUE     - ## C       numeric   -   - -10 to 10   -    TRUE     Y ## sigma   numeric   -   - -10 to 10   -    TRUE     Y ## With control class: TuneControlRandom ## Imputation value: Inf ## [Tune-x] 1: fw.perc=0.66; C=0.0766; sigma=474 ## [Tune-y] 1: mse.test.mean=86.5680746; time: 0.0 min ## [Tune-x] 2: fw.perc=0.779; C=0.108; sigma=0.775 ## [Tune-y] 2: mse.test.mean=56.8669659; time: 0.0 min ## [Tune-x] 3: fw.perc=0.416; C=46.1; sigma=0.817 ## [Tune-y] 3: mse.test.mean=24.9021386; time: 0.0 min ## [Tune-x] 4: fw.perc=0.0153; C=7.21; sigma=0.0253 ## [Tune-y] 4: mse.test.mean=85.1375318; time: 0.0 min ## [Tune-x] 5: fw.perc=0.747; C=0.0588; sigma=279 ## [Tune-y] 5: mse.test.mean=86.6044639; time: 0.0 min ## [Tune] Result: fw.perc=0.416; C=46.1; sigma=0.817 : mse.test.mean=24.9021386 res ## Tune result: ## Op. pars: fw.perc=0.416; C=46.1; sigma=0.817 ## mse.test.mean=24.9021386 df = as.data.frame(res$opt.path) df[, -ncol(df)] ##     fw.perc         C      sigma mse.test.mean dob eol error.message ## 1 0.6601122 -3.706174  8.8892278      86.56807   1  NA          <NA> ## 2 0.7789706 -3.211375 -0.3670747      56.86697   2  NA          <NA> ## 3 0.4159948  5.526678 -0.2914599      24.90214   3  NA          <NA> ## 4 0.0152991  2.849026 -5.3034003      85.13753   4  NA          <NA> ## 5 0.7470259 -4.087853  8.1243638      86.60446   5  NA          <NA> res$x ## $fw.perc ## [1] 0.4159948 ##  ## $C ## [1] 46.09946 ##  ## $sigma ## [1] 0.8170748 res$y ## mse.test.mean  ##      24.90214 lrn = makeFilterWrapper(   learner = makeLearner(\"regr.ksvm\", C = res$x$C, sigma = res$x$sigma),   fw.method = \"FSelectorRcpp_information.gain\",   more.args = list(\"FSelectorRcpp_information.gain\" = list(     equal = TRUE)),   fw.perc = res$x$fw.perc) mod = train(lrn, bh.task) mod ## Model for learner.id=regr.ksvm.filtered; learner.class=FilterWrapper ## Trained on: task.id = BostonHousing-example; obs = 506; features = 13 ## Hyperparameters: fit=FALSE,C=46.1,sigma=0.817,fw.perc=0.416  getFilteredFeatures(mod) ## [1] \"indus\"   \"nox\"     \"rm\"      \"ptratio\" \"lstat\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"wrapper-methods","dir":"Articles > Tutorial","previous_headings":"","what":"Wrapper methods","title":"Feature Selection","text":"Wrapper methods use performance learning algorithm assess usefulness feature set. order select feature subset learner trained repeatedly different feature subsets subset leads best learner performance chosen. order use wrapper approach decide: assess performance: involves choosing performance measure serves feature selection criterion resampling strategy. learning method use. search space possible feature subsets. search strategy defined functions following naming convention makeFeatSelControl<search_strategy. following search strategies available: Exhaustive search makeFeatSelControlExhaustive (?FeatSelControl()), Genetic algorithm makeFeatSelControlGA (?FeatSelControl()), Random search makeFeatSelControlRandom (?FeatSelControl()), Deterministic forward backward search makeFeatSelControlSequential (?FeatSelControl()).","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"select-a-feature-subset","dir":"Articles > Tutorial","previous_headings":"","what":"Select a feature subset","title":"Feature Selection","text":"Feature selection can conducted function selectFeatures(). following example perform exhaustive search Wisconsin Prognostic Breast Cancer (TH.data::wpbc()) data set. learning method use Cox proportional hazards model (survival::coxph()). performance assessed holdout estimate concordance index cindex). ctrl aFeatSelControl() object contains information search strategy potential parameter values. sfeatsis FeatSelResult (selectFeatures()) object. selected features corresponding performance can accessed follows: second example fit simple linear regression model BostonHousing (mlbench::BostonHousing()) data set use sequential search find feature set minimizes mean squared error mse). method = \"sfs\" indicates want conduct sequential forward search features added model performance improved anymore. See documentation page makeFeatSelControlSequential (?FeatSelControl()) available sequential search methods. search stopped improvement smaller alpha = 0.02. information sequential feature selection process can obtained function analyzeFeatSelResult().","code":"# Specify the search strategy ctrl = makeFeatSelControlRandom(maxit = 20L) ctrl ## FeatSel control: FeatSelControlRandom ## Same resampling instance: TRUE ## Imputation value: <worst> ## Max. features: <not used> ## Max. iterations: 20 ## Tune threshold: FALSE ## Further arguments: prob=0.5 # Resample description rdesc = makeResampleDesc(\"Holdout\")  # Select features sfeats = selectFeatures(   learner = \"surv.coxph\", task = wpbc.task, resampling = rdesc,   control = ctrl, show.info = FALSE) sfeats ## FeatSel result: ## Features (16): mean_radius, mean_texture, mean_area, mean_frac... ## cindex.test.mean=0.5953039 sfeats$x ##  [1] \"mean_radius\"      \"mean_texture\"     \"mean_area\"        \"mean_fractaldim\"  ##  [5] \"SE_perimeter\"     \"SE_area\"          \"SE_smoothness\"    \"SE_concavepoints\" ##  [9] \"SE_symmetry\"      \"SE_fractaldim\"    \"worst_radius\"     \"worst_perimeter\"  ## [13] \"worst_area\"       \"worst_concavity\"  \"tsize\"            \"pnodes\" sfeats$y ## cindex.test.mean  ##        0.5953039 # Specify the search strategy ctrl = makeFeatSelControlSequential(method = \"sfs\", alpha = 0.02)  # Select features rdesc = makeResampleDesc(\"CV\", iters = 10) sfeats = selectFeatures(   learner = \"regr.lm\", task = bh.task, resampling = rdesc, control = ctrl,   show.info = FALSE) sfeats ## FeatSel result: ## Features (11): crim, zn, chas, nox, rm, dis, rad, tax, ptratio... ## mse.test.mean=23.2447398 analyzeFeatSelResult(sfeats) ## Features         : 11 ## Performance      : mse.test.mean=23.2447398 ## crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat ##  ## Path to optimum: ## - Features:    0  Init   :                       Perf = 84.954  Diff: NA  * ## - Features:    1  Add    : lstat                 Perf = 39.114  Diff: 45.84  * ## - Features:    2  Add    : rm                    Perf = 31.487  Diff: 7.627  * ## - Features:    3  Add    : ptratio               Perf = 28.052  Diff: 3.4353  * ## - Features:    4  Add    : dis                   Perf = 27.056  Diff: 0.99529  * ## - Features:    5  Add    : nox                   Perf = 25.609  Diff: 1.4469  * ## - Features:    6  Add    : b                     Perf = 25.076  Diff: 0.53344  * ## - Features:    7  Add    : chas                  Perf = 24.671  Diff: 0.40483  * ## - Features:    8  Add    : zn                    Perf = 24.342  Diff: 0.32961  * ## - Features:    9  Add    : crim                  Perf = 24.194  Diff: 0.14755  * ## - Features:   10  Add    : rad                   Perf = 23.774  Diff: 0.42011  * ## - Features:   11  Add    : tax                   Perf = 23.245  Diff: 0.52917  * ##  ## Stopped, because no improving feature was found."},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"fuse-a-learner-with-feature-selection","dir":"Articles > Tutorial","previous_headings":"","what":"Fuse a learner with feature selection","title":"Feature Selection","text":"Learner (makeLearner()) can fused feature selection strategy (.e., search strategy, performance measure resampling strategy) function makeFeatSelWrapper(). training features selected according specified selection scheme. , learner trained selected feature subset. result feature selection can extracted function getFeatSelResult(). selected features : 5-fold cross-validated performance learner specified can computed follows: selected feature sets individual resampling iterations can extracted follows:","code":"rdesc = makeResampleDesc(\"CV\", iters = 3) lrn = makeFeatSelWrapper(\"surv.coxph\",   resampling = rdesc,   control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE) mod = train(lrn, task = wpbc.task) mod ## Model for learner.id=surv.coxph.featsel; learner.class=FeatSelWrapper ## Trained on: task.id = wpbc-example; obs = 194; features = 32 ## Hyperparameters: sfeats = getFeatSelResult(mod) sfeats ## FeatSel result: ## Features (16): mean_texture, mean_perimeter, mean_smoothness, ... ## cindex.test.mean=0.5951147 sfeats$x ##  [1] \"mean_texture\"       \"mean_perimeter\"     \"mean_smoothness\"    ##  [4] \"mean_compactness\"   \"mean_concavepoints\" \"mean_fractaldim\"    ##  [7] \"SE_radius\"          \"SE_area\"            \"SE_smoothness\"      ## [10] \"SE_fractaldim\"      \"worst_texture\"      \"worst_perimeter\"    ## [13] \"worst_smoothness\"   \"worst_concavity\"    \"worst_fractaldim\"   ## [16] \"pnodes\" out.rdesc = makeResampleDesc(\"CV\", iters = 5)  r = resample(   learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE,   show.info = FALSE) r$aggr ## cindex.test.mean  ##         0.606664 lapply(r$models, getFeatSelResult) ## [[1]] ## FeatSel result: ## Features (16): mean_concavity, mean_symmetry, mean_fractaldim,... ## cindex.test.mean=0.6282322 ##  ## [[2]] ## FeatSel result: ## Features (16): mean_radius, mean_texture, mean_perimeter, mean... ## cindex.test.mean=0.5250988 ##  ## [[3]] ## FeatSel result: ## Features (15): mean_texture, mean_perimeter, mean_smoothness, ... ## cindex.test.mean=0.7002573 ##  ## [[4]] ## FeatSel result: ## Features (15): mean_radius, mean_texture, mean_area, mean_smoo... ## cindex.test.mean=0.6983633 ##  ## [[5]] ## FeatSel result: ## Features (18): mean_texture, mean_smoothness, mean_concavity, ... ## cindex.test.mean=0.6896807"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/feature_selection.html","id":"feature-importance-from-trained-models","dir":"Articles > Tutorial","previous_headings":"","what":"Feature importance from trained models","title":"Feature Selection","text":"algorithms internally compute feature importance training. using getFeatureImportance() possible extract part trained model.","code":"task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.ranger\", importance = c(\"permutation\")) mod = train(lrn, task)  getFeatureImportance(mod) ## FeatureImportance: ## Task: iris ##  ## Learner: classif.ranger ## Measure: NA ## Contrast: NA ## Aggregation: function (x)  x ## Replace: NA ## Number of Monte-Carlo iterations: NA ## Local: FALSE ## # A tibble: 4 × 2 ##   variable     importance ##   <chr>             <dbl> ## 1 Sepal.Length    0.0336  ## 2 Sepal.Width     0.00627 ## 3 Petal.Length    0.309   ## 4 Petal.Width     0.294"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"how-to-model-functional-data","dir":"Articles > Tutorial","previous_headings":"","what":"How to model functional data?","title":"Functional Data","text":"two commonly used approaches analyzing functional data. Directly analyze functional data using learner suitable functional data task. learners prefixes classif.fda regr.fda. info learners see fda learners. purpose, functional data saved matrix column data frame used constructing task. info functional tasks, consider following section. Transform task format suitable standard classification regression learners. done extracting non-temporal/non-functional features curves. Non-temporal features interdependence , similarly features traditional machine learning. explained detail .","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"creating-a-task-that-contains-functional-features","dir":"Articles > Tutorial","previous_headings":"","what":"Creating a task that contains functional features","title":"Functional Data","text":"first step get data right format. mlr expects base::data.frame consists functional features target variable input. Functional data contrast numeric data stored matrix column data frame. , task contains data well-defined format created. tasks come different flavors, makeClassifTask() makeRegrTask(), can used according class target variable. following example, data first stored matrix columns using helper function makeFunctionalData() fuelSubset data package FDboost. data provided following structure: heatan target variable, case, numeric value. h2o additional scalar variable. NIR UVVIS matrices containing curve data. column corresponds single wavelength data sampled . row indicates single curve. NIR measured 231 wavelength points, UVVIS measured 129 wavelength points. nir.lambda uvvis.lambda numeric vectors length 231 129 indicate wavelength points data measured . entry corresponds one column NIR UVVIS respectively. now, ignore additional information mlr. data already contains functional features matrices list. order demonstrate matrix can created arbitrary numeric columns, transform list data frame set numeric columns matrix. columns refer matrix columns list, .e., UVVIS.1 first column UVVIS matrix. constructing task, data reformatted therefore contains column matrices. done providing list fd.features, identifies functional covariates. columns mentioned list kept -. case, column indices 3:136 correspond columns UVVIS matrix. Alternatively, also specify respective column names. makeFunctionalData() returns base::data.frame, functional features contained matrices. Now data frame containing functionals matrices, task can created:","code":"str(fuelSubset) ## List of 7 ##  $ heatan      : num [1:129] 26.8 27.5 23.8 18.2 17.5 ... ##  $ h2o         : num [1:129] 2.3 3 2 1.85 2.39 ... ##  $ nir.lambda  : num [1:231] 800 803 805 808 810 ... ##  $ NIR         : num [1:129, 1:231] 0.2818 0.2916 -0.0042 -0.034 -0.1804 ... ##  $ uvvis.lambda: num [1:134] 250 256 261 267 273 ... ##  $ UVVIS       : num [1:129, 1:134] 0.145 -1.584 -0.814 -1.311 -1.373 ... ##  $ h2o.fit     : num [1:129] 2.58 3.43 1.83 2.03 3.07 ... # Put all values into a data frame df = data.frame(fuelSubset[c(\"heatan\", \"h2o\", \"UVVIS\", \"NIR\")]) str(df[, 1:5]) ## 'data.frame':    129 obs. of  5 variables: ##  $ heatan : num  26.8 27.5 23.8 18.2 17.5 ... ##  $ h2o    : num  2.3 3 2 1.85 2.39 ... ##  $ UVVIS.1: num  0.145 -1.584 -0.814 -1.311 -1.373 ... ##  $ UVVIS.2: num  -0.0111 -2.0467 -1.053 -1.2445 -1.8826 ... ##  $ UVVIS.3: num  0.0372 -1.5695 -0.9381 -1.0649 -1.4016 ... library(mlr) # fd.features is a named list, where each name corresponds to the name of the  # functional feature and the values to the respective column indices or column names. fd.features = list(\"UVVIS\" = 3:136, \"NIR\" = 137:367) fdf = makeFunctionalData(df, fd.features = fd.features) str(fdf) ## 'data.frame':    129 obs. of  4 variables: ##  $ heatan: num  26.8 27.5 23.8 18.2 17.5 ... ##  $ h2o   : num  2.3 3 2 1.85 2.39 ... ##  $ UVVIS : num [1:129, 1:134] 0.145 -1.584 -0.814 -1.311 -1.373 ... ##   ..- attr(*, \"dimnames\")=List of 2 ##   .. ..$ : NULL ##   .. ..$ : chr [1:134] \"UVVIS.1\" \"UVVIS.2\" \"UVVIS.3\" \"UVVIS.4\" ... ##  $ NIR   : num [1:129, 1:231] 0.2818 0.2916 -0.0042 -0.034 -0.1804 ... ##   ..- attr(*, \"dimnames\")=List of 2 ##   .. ..$ : NULL ##   .. ..$ : chr [1:231] \"NIR.1\" \"NIR.2\" \"NIR.3\" \"NIR.4\" ... # Create a regression task, classification tasks behave analogously # In this case we use column indices tsk1 = makeRegrTask(\"fuelsubset\", data = fdf, target = \"heatan\") tsk1 ## Supervised task: fuelsubset ## Type: regr ## Target: heatan ## Observations: 129 ## Features: ##    numerics     factors     ordered functionals  ##           1           0           0           2  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"constructing-a-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Constructing a learner","title":"Functional Data","text":"functional data, learners constructed using makeLearner(\"classif.<R_method_name>\") makeLearner(\"regr.<R_method_name>\") depending target variable. Applying learners task works two ways: Either use learner suitable functional data: Learners can different properties, depending whether support either single.functional multiple functionals, .e. multiple different sensors observation. learner property functionals can handle one multiple functional covariates. learner property single.functional can handle single functional covariate. can check properties selecting leaner: use standard learner: case, temporal structure disregarded, functional data treated simple numeric features. Alternatively, transform functional data non-temporal/non-functional space extracting features training. case, standard regression- classification-learner can applied. explained detail feature extraction section .","code":"# The following learners can be used for tsk1 (a regression task). listLearners(tsk1, properties = \"functionals\", warn.missing.packages = FALSE) ##              class                                        name  short.name ## 1     regr.FDboost Functional linear array regression boosting     FDboost ## 2 regr.featureless                      Featureless regression featureless ##          package ## 1 FDboost,mboost ## 2            mlr ##                                                                                                                                                                                                                          note ## 1 Only allow one base learner for functional covariate and one base learner for scalar covariate, the parameters for these base learners are the same. Also we currently do not support interaction between scalar covariates ## 2                                                                                                                                                                                                                             ##   type installed numerics factors ordered missings weights  prob oneclass ## 1 regr      TRUE     TRUE   FALSE   FALSE    FALSE   FALSE FALSE    FALSE ## 2 regr      TRUE     TRUE    TRUE    TRUE     TRUE   FALSE FALSE    FALSE ##   twoclass multiclass class.weights featimp oobpreds functionals ## 1    FALSE      FALSE         FALSE   FALSE    FALSE        TRUE ## 2    FALSE      FALSE         FALSE   FALSE    FALSE        TRUE ##   single.functional    se lcens rcens icens ## 1             FALSE FALSE FALSE FALSE FALSE ## 2             FALSE FALSE FALSE FALSE FALSE # Create an FDboost learner for regression fdalrn = makeLearner(\"regr.FDboost\") # Or alternatively, use knn for classification: knn.lrn = makeLearner(\"classif.fdausc.knn\") getLearnerProperties(fdalrn) ## [1] \"numerics\"    \"functionals\" # Decision Tree learner rpartlrn = makeLearner(\"classif.rpart\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"training-the-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Training the learner","title":"Functional Data","text":"resulting learner can now trained task created section Creating task . Alternatively, learners explicitly treat functional covariates can applied. case, temporal structure completely disregarded, columns treated independent.","code":"# Train the fdalrn on the constructed task m = train(learner = fdalrn, task = tsk1) p = predict(m, tsk1) performance(p, rmse) ##     rmse  ## 2.181438 # Train a normal learner on the constructed task. # Note that we get a message, that functionals have been converted to numerics. rpart.lrn = makeLearner(\"regr.rpart\") m = train(learner = rpart.lrn, task = tsk1) ## Functional features have been converted to numerics  m ## Model for learner.id=regr.rpart; learner.class=regr.rpart ## Trained on: task.id = fuelsubset; obs = 129; features = 3 ## Hyperparameters: xval=0"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"feature-extraction","dir":"Articles > Tutorial","previous_headings":"","what":"Feature extraction","title":"Functional Data","text":"contrast applying learner works task containing functional features, task can converted standard task. works transforming functional features non-functional domain, e.g., extracting wavelets. currently supported preprocessing functions : discrete wavelet transform; fast Fourier transform; functional principal component analysis; multi-resolution feature extraction. order , specify methods functional feature task list. case, simply want extract Fourier transform UVVIS functional Functional PCA Scores NIR functional. Variable names can specified multiple times different extractors. Additional arguments supplied extract functions passed .","code":"# feat.methods specifies what to extract from which functional # In this example, we extract the Fourier transformation from the first functional. # From the second functional, fpca scores are extracted. feat.methods = list(\"UVVIS\" = extractFDAFourier(), \"NIR\" = extractFDAFPCA())  # Either create a new task from an existing task extracted = extractFDAFeatures(tsk1, feat.methods = feat.methods) extracted ## $task ## Supervised task: fuelsubset ## Type: regr ## Target: heatan ## Observations: 129 ## Features: ##    numerics     factors     ordered functionals  ##         264           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ##  ## $desc ## Extraction of features from functional data: ## Target: heatan ## Remaining functional Features: 2 after extraction on 2 functional features"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"wavelets","dir":"Articles > Tutorial","previous_headings":"","what":"Wavelets","title":"Functional Data","text":"example, discrete wavelet feature transformation applied data using function extractFDAWavelets. Discrete wavelet transform decomposes functional several wavelets. essentially transforms time signal time-scale representation, every wavelet captures data different resolution. can specify additional parameters (.e., filter (type wavelet) boundary) pars argument. function returns regression task type regr since raw data contained temporal structure transformed data inherit temporal structure anymore. information wavelets consider documentation wavelets::dwt. comprehensive guide , example, given .","code":"# Specify the feature extraction method and generate new task. # Here, we use the Haar filter: feat.methods = list(\"UVVIS\" = extractFDAWavelets(filter = \"haar\")) task.w = extractFDAFeatures(tsk1, feat.methods = feat.methods)  # Use the Daubechie wavelet with filter length 4. feat.methods = list(\"NIR\" = extractFDAWavelets(filter = \"d4\")) task.wd4 = extractFDAFeatures(tsk1, feat.methods = feat.methods)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"fourier-transformation","dir":"Articles > Tutorial","previous_headings":"","what":"Fourier transformation","title":"Functional Data","text":"Now, use Fourier feature transformation. Fourier transform takes functional transforms frequency domain splitting signal different frequency components. detailed tutorial Fourier transform can found . Either amplitude phase complex Fourier coefficients can used analysis. can specified additional trafo.coeff argument:","code":"# Specify the feature extraction method and generate new task. # We use the Fourier features and the amplitude for NIR, as well as the phase for UVVIS feat.methods = list(\"NIR\" = extractFDAFourier(trafo.coeff = \"amplitude\"),                     \"UVVIS\" = extractFDAFourier(trafo.coeff = \"phase\")) task.fourier = extractFDAFeatures(tsk1, feat.methods = feat.methods) task.fourier"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/functional_data.html","id":"wrappers","dir":"Articles > Tutorial","previous_headings":"","what":"Wrappers","title":"Functional Data","text":"Additionally can wrap preprocessing around standard learner classif.rpart. additional information, please consider Wrappers section.","code":"# Use a FDAFeatExtractWrapper # In this case we extract the Fourier features from the NIR variable feat.methods = list(\"NIR\" = extractFDAFourier()) wrapped.lrn = makeExtractFDAFeatsWrapper(\"regr.rpart\",  feat.methods = feat.methods)  # And run the learner train(wrapped.lrn, fuelsubset.task) ## Functional features have been converted to numerics ## Model for learner.id=regr.rpart.extracted; learner.class=extractFDAFeatsWrapper ## Trained on: task.id = fs.fdf; obs = 129; features = 3 ## Hyperparameters: xval=0"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"introduction","dir":"Articles > Tutorial","previous_headings":"","what":"Introduction","title":"Handling of Spatial Data","text":"Spatial data different non-spatial data spatial reference information attached observation. information usually stored coordinates, often named x y. Coordinates either stored UTM (Universal Transverse Mercator) latitude/longitude format. Treating spatial data sets like non-spatial ones leads overoptimistic results predictive accuracy models (Brenning 2005). due underlying spatial autocorrelation data. Spatial autocorrelation occur spatial data sets. Magnitude varies depending characteristics data set. closer observations located , similar . common validation procedures like cross-validation applied data sets, assume independence observation upfront provide unbiased estimates. However, assumption violated spatial case due spatial autocorrelation. Subsequently, non-spatial cross-validation fail provide accurate performance estimates. Nested Spatial Non-Spatial Cross-Validation random sampling data set (.e., non-spatial sampling), training test set data often located directly next (geographical space). Hence, test set contain observations somewhat similar (due spatial autocorrelation) observations training set. leads effect model, trained training set, performs quite well test data already knows degree. reduce bias resulting predictive accuracy estimate, Brenning 2005 suggested using spatial partitioning favor random partitioning (see Figure 1). , spatial clusters equal number folds chosen. spatially disjoint subsets data introduce spatial distance training test set. reduces influence spatial autocorrelation subsequently also overoptimistic predictive accuracy estimates. example Figure 1 shows five-fold nested cross-validation setting exhibits difference spatial non-spatial partitioning. nested approach used hyperparameter tuning performed.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"how-to-use-spatial-partitioning-in-mlr","dir":"Articles > Tutorial","previous_headings":"","what":"How to use spatial partitioning in mlr","title":"Handling of Spatial Data","text":"Spatial partitioning can used performing cross-validation. resample() call can choose SpCV SpRepCV use . SpCV perform spatial cross-validation one repetition, SpRepCV gives option choose number repetitions. rule thumb, usually 100 repetitions used aim reduce variance introduced partitioning. prerequisites : specifying task, need provide spatial coordinates argument coordinates. supplied data.frame needs number rows data least two dimensions need supplied. means 3-D partitioning might also work tested explicitly. Coordinates need numeric suggested use UTM projection. applies, coordinates used spatial partitioning SpCV SpRepCV selected resampling strategy.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"examples","dir":"Articles > Tutorial","previous_headings":"","what":"Examples","title":"Handling of Spatial Data","text":"spatial.task data set serves example data set spatial modeling tasks mlr. task argument coordinates data.frame two coordinates later used spatial partitioning data set. example, “Random Forest” algorithm (package ranger) used model binomial response variable. performance assessment, repeated spatial cross-validation 5 folds 5 repetitions chosen.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"spatial-cross-validation","dir":"Articles > Tutorial","previous_headings":"","what":"Spatial Cross-Validation","title":"Handling of Spatial Data","text":"can check introduced spatial autocorrelation bias performing modeling task using non-spatial partitioning setting. , simply choose RepCV instead SpRepCV. need remove coordinates task used SpCV SpRepCV selected resampling strategy.","code":"data(\"spatial.task\") spatial.task ## Supervised task: ecuador ## Type: classif ## Target: slides ## Observations: 751 ## Features: ##    numerics     factors     ordered functionals  ##          10           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: TRUE ## Classes: 2 ## FALSE  TRUE  ##   251   500  ## Positive class: TRUE  learner.rf = makeLearner(\"classif.ranger\", predict.type = \"prob\")  resampling = makeResampleDesc(\"SpRepCV\", fold = 5, reps = 5)  set.seed(123) out = resample(learner = learner.rf, task = spatial.task,   resampling = resampling, measures = list(auc)) ## Resampling: repeated spatial cross-validation ## Measures:             auc ## [Resample] iter 1:    0.5398810 ## [Resample] iter 2:    0.7046371 ## [Resample] iter 3:    0.6703070 ## [Resample] iter 4:    0.7350827 ## [Resample] iter 5:    0.4451993 ## [Resample] iter 6:    0.7221774 ## [Resample] iter 7:    0.7232207 ## [Resample] iter 8:    0.6615998 ## [Resample] iter 9:    0.4508605 ## [Resample] iter 10:   0.5366071 ## [Resample] iter 11:   0.6672078 ## [Resample] iter 12:   0.7108196 ## [Resample] iter 13:   0.7040323 ## [Resample] iter 14:   0.4429348 ## [Resample] iter 15:   0.5380952 ## [Resample] iter 16:   0.7224771 ## [Resample] iter 17:   0.4468864 ## [Resample] iter 18:   0.7132179 ## [Resample] iter 19:   0.5254868 ## [Resample] iter 20:   0.6689787 ## [Resample] iter 21:   0.7235356 ## [Resample] iter 22:   0.7069237 ## [Resample] iter 23:   0.4537241 ## [Resample] iter 24:   0.5137457 ## [Resample] iter 25:   0.6664699 ##  ## Aggregated Result: auc.test.mean=0.6157643 ##   mean(out$measures.test$auc) ## [1] 0.6157643"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"non-spatial-cross-validation","dir":"Articles > Tutorial","previous_headings":"","what":"Non-Spatial Cross-Validation","title":"Handling of Spatial Data","text":"introduced bias (caused spatial autocorrelation) performance example around 0.12 AUROC.","code":"learner.rf = makeLearner(\"classif.ranger\", predict.type = \"prob\")  resampling = makeResampleDesc(\"RepCV\", fold = 5, reps = 5)  set.seed(123) out = resample(learner = learner.rf, task = spatial.task,   resampling = resampling, measures = list(auc)) ## Resampling: repeated cross-validation ## Measures:             auc ## [Resample] iter 1:    0.7525173 ## [Resample] iter 2:    0.7893519 ## [Resample] iter 3:    0.7368117 ## [Resample] iter 4:    0.7832925 ## [Resample] iter 5:    0.7351196 ## [Resample] iter 6:    0.7983069 ## [Resample] iter 7:    0.7962963 ## [Resample] iter 8:    0.7596154 ## [Resample] iter 9:    0.6641679 ## [Resample] iter 10:   0.8370274 ## [Resample] iter 11:   0.7772298 ## [Resample] iter 12:   0.7561364 ## [Resample] iter 13:   0.7850379 ## [Resample] iter 14:   0.7368225 ## [Resample] iter 15:   0.7632756 ## [Resample] iter 16:   0.7820844 ## [Resample] iter 17:   0.7316300 ## [Resample] iter 18:   0.7466029 ## [Resample] iter 19:   0.8049018 ## [Resample] iter 20:   0.7146898 ## [Resample] iter 21:   0.7245908 ## [Resample] iter 22:   0.7309942 ## [Resample] iter 23:   0.7865146 ## [Resample] iter 24:   0.7500000 ## [Resample] iter 25:   0.8117284 ##  ## Aggregated Result: auc.test.mean=0.7621898 ##   mean(out$measures.test$auc) ## [1] 0.7621898"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"visualization-of-spatial-partitions","dir":"Articles > Tutorial","previous_headings":"","what":"Visualization of spatial partitions","title":"Handling of Spatial Data","text":"can visualize spatial partitioning using createSpatialResamplingPlots(). function creates multiple ggplot2 objects can visualized gridded way using favorite “plot arrangement” function recommend use cowplot::plot_grid() cowplot::save_plot() . can pass multiple resample result objects inputs. useful visualize differences spatial non-spatial partitioning. pass resample objects named list, names populated plot titles. default everything reprojected EPSG: 4326 (WGS 84) can changed using argument datum. like plot appearance, can customize plots stored resulting list just applying function list, example using purrr::map(), base::lapply() similar functions.","code":"library(mlr) library(cowplot)  rdesc1 = makeResampleDesc(\"SpRepCV\", folds = 5, reps = 4) r1 = resample(makeLearner(\"classif.qda\"), spatial.task, rdesc1, show.info = FALSE) rdesc2 = makeResampleDesc(\"RepCV\", folds = 5, reps = 4) r2 = resample(makeLearner(\"classif.qda\"), spatial.task, rdesc2, show.info = FALSE)  plots = createSpatialResamplingPlots(spatial.task,   list(\"SpRepCV\" = r1, \"RepCV\" = r2), crs = 32717, repetitions = 1,   x.axis.breaks = c(-79.075), y.axis.breaks = c(-3.975)) plot_grid(plotlist = plots[[\"Plots\"]], ncol = 5, nrow = 2,   labels = plots[[\"Labels\"]], label_size = 8)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/handling_of_spatial_data.html","id":"notes","dir":"Articles > Tutorial","previous_headings":"","what":"Notes","title":"Handling of Spatial Data","text":"models affected spatial autocorrelation others. general, can said flexible model , profit underlying spatial autocorrelation. Simpler models (e.g., GLM) show less overoptimistic performance estimates. concept spatial cross-validation originally implemented package sperrorest. package comes even partitioning options ability visualize spatial grouping folds. plan integrate functions sperrorest mlr stay tuned! detailed information, see Brenning 2005 Brenning2012.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/hyperpar_tuning_effects.html","id":"generating-hyperparameter-tuning-data","dir":"Articles > Tutorial","previous_headings":"","what":"Generating hyperparameter tuning data","title":"Evaluating Hyperparameter Tuning","text":"mlr separates generation data plotting data case user wishes use data custom way downstream. generateHyperParsEffectData() method takes tuning result along 2 additional arguments: trafo include.diagnostics. trafo argument convert hyperparameter data transformed scale case transformation used creating parameter (case ). include.diagnostics argument tell mlr whether include eol error messages learner. perform random search C parameter SVM famous Pima Indians (mlbench::PimaIndiansDiabetes()) dataset. generate hyperparameter effect data C parameter transformed scale include diagnostic data: reminder resampling tutorial, wanted generate data training set well validation set, need make minor changes: example , perform grid search C parameter SVM Pima Indians dataset using nested cross validation. generate hyperparameter effect data C parameter untransformed scale include diagnostic data. can see , nested cross validation supported without extra work user, allowing user obtain unbiased estimator performance. generating hyperparameter effect data, next step visualize . mlr several methods built-visualize data, meant support needs researcher engineer industry. next sections walk visualization support several use-cases.","code":"ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x) ) ctrl = makeTuneControlRandom(maxit = 100L) rdesc = makeResampleDesc(\"CV\", iters = 2L) res = tuneParams(\"classif.ksvm\", task = pid.task, control = ctrl,   measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) generateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)  ## HyperParsEffectData: ## Hyperparameters: C ## Measures: acc.test.mean,mmce.test.mean ## Optimizer: TuneControlRandom ## Nested CV Used: FALSE ## Snapshot of data: ##            C acc.test.mean mmce.test.mean iteration exec.time ## 1 0.18477464     0.7317708      0.2682292         1     0.833 ## 2 0.19252244     0.7304688      0.2695312         2     0.041 ## 3 4.75219120     0.7200521      0.2799479         3     0.040 ## 4 0.05567812     0.6536458      0.3463542         4     0.579 ## 5 0.12179856     0.7161458      0.2838542         5     0.064 ## 6 6.25359181     0.7239583      0.2760417         6     0.056 ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x) ) ctrl = makeTuneControlRandom(maxit = 100L) rdesc = makeResampleDesc(\"CV\", iters = 2L, predict = \"both\") res = tuneParams(\"classif.ksvm\", task = pid.task, control = ctrl,   measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce,     train.mean)), resampling = rdesc, par.set = ps, show.info = FALSE) generateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)  ## HyperParsEffectData: ## Hyperparameters: C ## Measures: acc.test.mean,acc.train.mean,mmce.test.mean,mmce.train.mean ## Optimizer: TuneControlRandom ## Nested CV Used: FALSE ## Snapshot of data: ##            C acc.test.mean acc.train.mean mmce.test.mean mmce.train.mean ## 1 0.07253028     0.6575521      0.6588542      0.3424479       0.3411458 ## 2 4.09306975     0.7617188      0.8906250      0.2382812       0.1093750 ## 3 2.23762232     0.7552083      0.8710938      0.2447917       0.1289062 ## 4 0.11080224     0.6940104      0.7187500      0.3059896       0.2812500 ## 5 3.37586150     0.7565104      0.8867188      0.2434896       0.1132812 ## 6 1.77872565     0.7669271      0.8567708      0.2330729       0.1432292 ##   iteration exec.time ## 1         1     0.095 ## 2         2     0.079 ## 3         3     0.102 ## 4         4     0.092 ## 5         5     0.071 ## 6         6     0.088 ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x) ) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"CV\", iters = 2L) lrn = makeTuneWrapper(\"classif.ksvm\", control = ctrl,   measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) res = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) generateHyperParsEffectData(res)  ## HyperParsEffectData: ## Hyperparameters: C ## Measures: acc.test.mean,mmce.test.mean ## Optimizer: TuneControlGrid ## Nested CV Used: TRUE ## Snapshot of data: ##            C acc.test.mean mmce.test.mean iteration exec.time ## 1 -5.0000000     0.6536458      0.3463542         1     0.064 ## 2 -3.8888889     0.6536458      0.3463542         2     0.037 ## 3 -2.7777778     0.6718750      0.3281250         3     0.036 ## 4 -1.6666667     0.7291667      0.2708333         4     0.038 ## 5 -0.5555556     0.7369792      0.2630208         5     0.037 ## 6  0.5555556     0.7369792      0.2630208         6     0.051 ##   nested_cv_run ## 1             1 ## 2             1 ## 3             1 ## 4             1 ## 5             1 ## 6             1 ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x) ) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"CV\", iters = 2L) lrn = makeTuneWrapper(\"classif.ksvm\", control = ctrl,   measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) res = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) generateHyperParsEffectData(res)  ## HyperParsEffectData: ## Hyperparameters: C ## Measures: acc.test.mean,mmce.test.mean ## Optimizer: TuneControlGrid ## Nested CV Used: TRUE ## Snapshot of data: ##            C acc.test.mean mmce.test.mean iteration exec.time ## 1 -5.0000000     0.6718750      0.3281250         1     0.066 ## 2 -3.8888889     0.6718750      0.3281250         2     0.077 ## 3 -2.7777778     0.6744792      0.3255208         3     0.070 ## 4 -1.6666667     0.7187500      0.2812500         4     0.049 ## 5 -0.5555556     0.7526042      0.2473958         5     0.049 ## 6  0.5555556     0.7630208      0.2369792         6     0.054 ##   nested_cv_run ## 1             1 ## 2             1 ## 3             1 ## 4             1 ## 5             1 ## 6             1"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/hyperpar_tuning_effects.html","id":"visualizing-the-effect-of-a-single-hyperparameter","dir":"Articles > Tutorial","previous_headings":"","what":"Visualizing the effect of a single hyperparameter","title":"Evaluating Hyperparameter Tuning","text":"situation user tuning single hyperparameter learner, user may wish plot performance learner values hyperparameter. example , tune number clusters silhouette score mtcars dataset. specify x-axis x argument y-axis y argument. plot.type argument specified, mlr attempt plot scatterplot default. Since plotHyperParsEffect() returns ggplot2::ggplot() object, can easily customize liking! example , tune SVM C hyperparameter Pima dataset. use simulated annealing optimizer, interested seeing optimization algorithm actually improves iterations. default, mlr plots improvements global optimum.  case learner crash, mlr impute crash worst value graphically indicate point. example , give C parameter negative values, result learner crash SVM.  example uses nested cross validation outer loop 2 runs. mlr indicates run within visualization.","code":"library(\"clusterSim\")  ps = makeParamSet(   makeDiscreteParam(\"centers\", values = 3:10) ) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"Holdout\") res = tuneParams(\"cluster.kmeans\", task = mtcars.task, control = ctrl,   measures = silhouette, resampling = rdesc, par.set = ps, show.info = FALSE) data = generateHyperParsEffectData(res) plt = plotHyperParsEffect(data, x = \"centers\", y = \"silhouette.test.mean\")  # add our own touches to the plot plt + geom_point(colour = \"red\") +   ggtitle(\"Evaluating Number of Cluster Centers on mtcars\") +   scale_x_continuous(breaks = 3:10) +   theme_bw() ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x) ) ctrl = makeTuneControlGenSA(budget = 100L) rdesc = makeResampleDesc(\"Holdout\") res = tuneParams(\"classif.ksvm\", task = pid.task, control = ctrl,   resampling = rdesc, par.set = ps, show.info = FALSE) data = generateHyperParsEffectData(res) plt = plotHyperParsEffect(data, x = \"iteration\", y = \"mmce.test.mean\",   plot.type = \"line\") plt + ggtitle(\"Analyzing convergence of simulated annealing\") +   theme_minimal() ps = makeParamSet(   makeDiscreteParam(\"C\", values = c(-1, -0.5, 0.5, 1, 1.5)) ) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"CV\", iters = 2L) res = tuneParams(\"classif.ksvm\", task = pid.task, control = ctrl,   measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) data = generateHyperParsEffectData(res) plt = plotHyperParsEffect(data, x = \"C\", y = \"acc.test.mean\") plt + ggtitle(\"SVM learner crashes with negative C\") +   theme_bw() ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x) ) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"Holdout\") lrn = makeTuneWrapper(\"classif.ksvm\", control = ctrl,   measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) res = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) data = generateHyperParsEffectData(res) plotHyperParsEffect(data, x = \"C\", y = \"acc.test.mean\", plot.type = \"line\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/hyperpar_tuning_effects.html","id":"visualizing-the-effect-of-2-hyperparameters","dir":"Articles > Tutorial","previous_headings":"","what":"Visualizing the effect of 2 hyperparameters","title":"Evaluating Hyperparameter Tuning","text":"case tuning 2 hyperparameters simultaneously, mlr provides ability plot heatmap contour plot addition scatterplot line. example , tune C sigma parameters SVM Pima dataset. use interpolation produce regular grid plotting heatmap. interpolation argument accepts regression learner mlr perform interpolation. z argument used fill heatmap color lines, depending plot.type used.  can use show.experiments argument order visualize points specifically passed learner original experiment points interpolated mlr:  can also visualize long optimizer takes reach optima example:  case tuning 2 hyperparameters learner crash, mlr indicate respective points impute worst value. example , tune C sigma, forcing C negative instances crash SVM. perform interpolation get regular grid order plot heatmap. can see interpolation creates axis parallel lines resulting learner crashes.  slightly complicated example using nested cross validation simultaneously tuning 2 hyperparameters. order plot heatmap case, mlr aggregate nested runs user-specified function. default function mean. expected, can still take advantage interpolation.","code":"ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -5, upper = 5, trafo = function(x) 2^x)) ctrl = makeTuneControlRandom(maxit = 100L) rdesc = makeResampleDesc(\"Holdout\") learn = makeLearner(\"classif.ksvm\", par.vals = list(kernel = \"rbfdot\")) res = tuneParams(learn, task = pid.task, control = ctrl, measures = acc,   resampling = rdesc, par.set = ps, show.info = FALSE) data = generateHyperParsEffectData(res) plt = plotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"acc.test.mean\",   plot.type = \"heatmap\", interpolate = \"regr.earth\") min_plt = min(data$data$acc.test.mean, na.rm = TRUE) max_plt = max(data$data$acc.test.mean, na.rm = TRUE) med_plt = mean(c(min_plt, max_plt)) plt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),   low = \"blue\", mid = \"white\", high = \"red\", midpoint = med_plt) ## Scale for 'fill' is already present. Adding another scale for 'fill', which ## will replace the existing scale. plt = plotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"acc.test.mean\",   plot.type = \"heatmap\", interpolate = \"regr.earth\", show.experiments = TRUE) plt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),   low = \"blue\", mid = \"white\", high = \"red\", midpoint = med_plt) ## Scale for 'fill' is already present. Adding another scale for 'fill', which ## will replace the existing scale. plotHyperParsEffect(data, x = \"iteration\", y = \"acc.test.mean\",   plot.type = \"line\") ps = makeParamSet(   makeDiscreteParam(\"C\", values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)),   makeDiscreteParam(\"sigma\", values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5))) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"Holdout\") learn = makeLearner(\"classif.ksvm\", par.vals = list(kernel = \"rbfdot\")) res = tuneParams(learn, task = pid.task, control = ctrl, measures = acc,   resampling = rdesc, par.set = ps, show.info = FALSE) data = generateHyperParsEffectData(res) plotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"acc.test.mean\",   plot.type = \"heatmap\", interpolate = \"regr.earth\") ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -5, upper = 5, trafo = function(x) 2^x)) ctrl = makeTuneControlRandom(maxit = 100) rdesc = makeResampleDesc(\"Holdout\") learn = makeLearner(\"classif.ksvm\", par.vals = list(kernel = \"rbfdot\")) lrn = makeTuneWrapper(learn, control = ctrl, measures = list(acc, mmce),   resampling = rdesc, par.set = ps, show.info = FALSE) res = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) data = generateHyperParsEffectData(res) plt = plotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"acc.test.mean\",   plot.type = \"heatmap\", interpolate = \"regr.earth\", show.experiments = TRUE,   nested.agg = mean) min_plt = min(plt$data$acc.test.mean, na.rm = TRUE) max_plt = max(plt$data$acc.test.mean, na.rm = TRUE) med_plt = mean(c(min_plt, max_plt)) plt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),   low = \"red\", mid = \"white\", high = \"blue\", midpoint = med_plt) ## Scale for 'fill' is already present. Adding another scale for 'fill', which ## will replace the existing scale."},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/hyperpar_tuning_effects.html","id":"visualizing-the-effects-of-more-than-2-hyperparameters","dir":"Articles > Tutorial","previous_headings":"","what":"Visualizing the effects of more than 2 hyperparameters","title":"Evaluating Hyperparameter Tuning","text":"order visualize result tuning 3 hyperparameters simultaneously can take advantage partial dependence plots show performance depends one- two-dimensional subset hyperparameters. tune three hyperparameters C, sigma, degree SVM Bessel kernel set partial.dep flag TRUE indicate intend calculate partial dependences. can generate plot single hyperparameter like C shown . partial.dep.learn can regression Learner (makeLearner()) mlr used regress attained performance values values 3 hyperparameters visited tuning. fitted model serves basis calculating partial dependences.  can also look two hyperparameters simultaneously, example C sigma.","code":"ps = makeParamSet(   makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -5, upper = 5, trafo = function(x) 2^x),   makeDiscreteParam(\"degree\", values = 2:5)) ctrl = makeTuneControlRandom(maxit = 100L) rdesc = makeResampleDesc(\"Holdout\", predict = \"both\") learn = makeLearner(\"classif.ksvm\", par.vals = list(kernel = \"besseldot\")) res = tuneParams(learn, task = pid.task, control = ctrl,   measures = list(acc, setAggregation(acc, train.mean)), resampling = rdesc,   par.set = ps, show.info = FALSE) data = generateHyperParsEffectData(res, partial.dep = TRUE) plotHyperParsEffect(data, x = \"C\", y = \"acc.test.mean\", plot.type = \"line\",   partial.dep.learn = \"regr.randomForest\") ## Loading required package: mmpf plotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"acc.test.mean\",   plot.type = \"heatmap\", partial.dep.learn = \"regr.randomForest\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/impute.html","id":"imputation-and-reimputation","dir":"Articles > Tutorial","previous_headings":"","what":"Imputation and reimputation","title":"Imputation of Missing Values","text":"Imputation can done function impute(). can specify imputation method feature individually classes features like numerics factors. Moreover, can generate dummy variables indicate values missing, also either classes features individual features. allow identify patterns reasons missing data permit treat imputed observed values differently subsequent analysis. Let’s look airquality (datasets::airquality()) data set. 37 NA's variable Ozone (ozone pollution) 7 NA's variable Solar.R (solar radiation). demonstration purposes insert artificial NA's column Wind (wind speed) coerce factor. want impute NA's integer features (include Ozone Solar.R) mean, factor features (Wind) mode additionally generate dummy variables integer features, can follows: impute() returns list slot $data contains imputed data set. Per default, dummy variables factors levels \"TRUE\" \"FALSE\". also possible create numeric zero-one indicator variables. Slot $desc ImputationDesc (impute()) object stores relevant information imputation. current example includes means mode computed non-missing data. imputation description shows name target variable (present), number features number imputed features. Note latter number refers features imputation method specified (five integers plus one factor) features actually containing NA's. dummy.type indicates dummy variables factors. details impute.new.levels recode.factor.levels see help page function impute(). Let’s look another example involving target variable. possible learning task associated airquality (datasets::airquality()) data predict ozone pollution based meteorological features. Since want use columns Day Month remove . first 100 observations used training data set. case supervised learning problem need pass name target variable impute(). prevents imputation creation dummy variable target variable makes sure target variable used impute features. contrast example specify imputation methods individual features instead classes features. Missing values Solar.R imputed random numbers drawn empirical distribution non-missing observations. Function imputeLearner (imputations()) allows use supervised learning algorithms integrated mlr imputation. type Learner (makeLearner()) (regr, classif) must correspond class feature imputed. missing values Wind replaced predictions classification tree (rpart::rpart()). Per default, available columns airq.train except target variable (Ozone) variable imputed (Wind) used features classification tree, Solar.R Temp. can also select manually columns use. Note rpart::rpart() can deal missing feature values, therefore NA's column Solar.R pose problem. ImputationDesc (impute()) object can used function reimpute() impute test data set way training data. Especially evaluating machine learning method resampling technique might want impute()/reimpute() called automatically time training/prediction. can achieved creating imputation wrapper.","code":"data(airquality) summary(airquality) ##      Ozone           Solar.R           Wind             Temp       ##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   ##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   ##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   ##  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   ##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   ##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   ##  NA's   :37       NA's   :7                                        ##      Month            Day       ##  Min.   :5.000   Min.   : 1.0   ##  1st Qu.:6.000   1st Qu.: 8.0   ##  Median :7.000   Median :16.0   ##  Mean   :6.993   Mean   :15.8   ##  3rd Qu.:8.000   3rd Qu.:23.0   ##  Max.   :9.000   Max.   :31.0   ## airq = airquality ind = sample(nrow(airq), 10) airq$Wind[ind] = NA airq$Wind = cut(airq$Wind, c(0,8,16,24)) summary(airq) ##      Ozone           Solar.R           Wind         Temp           Month       ##  Min.   :  1.00   Min.   :  7.0   (0,8]  :51   Min.   :56.00   Min.   :5.000   ##  1st Qu.: 18.00   1st Qu.:115.8   (8,16] :85   1st Qu.:72.00   1st Qu.:6.000   ##  Median : 31.50   Median :205.0   (16,24]: 7   Median :79.00   Median :7.000   ##  Mean   : 42.13   Mean   :185.9   NA's   :10   Mean   :77.88   Mean   :6.993   ##  3rd Qu.: 63.25   3rd Qu.:258.8                3rd Qu.:85.00   3rd Qu.:8.000   ##  Max.   :168.00   Max.   :334.0                Max.   :97.00   Max.   :9.000   ##  NA's   :37       NA's   :7                                                    ##       Day       ##  Min.   : 1.0   ##  1st Qu.: 8.0   ##  Median :16.0   ##  Mean   :15.8   ##  3rd Qu.:23.0   ##  Max.   :31.0   ## imp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()),   dummy.classes = \"integer\") head(imp$data, 10) ##       Ozone  Solar.R    Wind Temp Month Day Ozone.dummy Solar.R.dummy ## 1  41.00000 190.0000   (0,8]   67     5   1       FALSE         FALSE ## 2  36.00000 118.0000   (0,8]   72     5   2       FALSE         FALSE ## 3  12.00000 149.0000  (8,16]   74     5   3       FALSE         FALSE ## 4  18.00000 313.0000  (8,16]   62     5   4       FALSE         FALSE ## 5  42.12931 185.9315  (8,16]   56     5   5        TRUE          TRUE ## 6  28.00000 185.9315  (8,16]   66     5   6       FALSE          TRUE ## 7  23.00000 299.0000  (8,16]   65     5   7       FALSE         FALSE ## 8  19.00000  99.0000  (8,16]   59     5   8       FALSE         FALSE ## 9   8.00000  19.0000 (16,24]   61     5   9       FALSE         FALSE ## 10 42.12931 194.0000  (8,16]   69     5  10        TRUE         FALSE imp$desc ## Imputation description ## Target:  ## Features: 6; Imputed: 6 ## impute.new.levels: TRUE ## recode.factor.levels: TRUE ## dummy.type: factor airq = subset(airq, select = 1:4) airq.train = airq[1:100,] airq.test = airq[-c(1:100),] imp = impute(airq.train, target = \"Ozone\", cols = list(Solar.R = imputeHist(),   Wind = imputeLearner(\"classif.rpart\")), dummy.cols = c(\"Solar.R\", \"Wind\")) summary(imp$data) ##      Ozone           Solar.R           Wind         Temp       Solar.R.dummy ##  Min.   :  1.00   Min.   :  7.0   (0,8]  :35   Min.   :56.00   FALSE:93      ##  1st Qu.: 16.00   1st Qu.:100.5   (8,16] :59   1st Qu.:69.00   TRUE : 7      ##  Median : 34.00   Median :223.0   (16,24]: 6   Median :79.50                 ##  Mean   : 41.59   Mean   :192.0                Mean   :76.87                 ##  3rd Qu.: 63.00   3rd Qu.:273.2                3rd Qu.:84.00                 ##  Max.   :135.00   Max.   :334.0                Max.   :93.00                 ##  NA's   :31                                                                  ##  Wind.dummy ##  FALSE:94   ##  TRUE : 6   ##             ##             ##             ##             ##   imp$desc ## Imputation description ## Target: Ozone ## Features: 3; Imputed: 2 ## impute.new.levels: TRUE ## recode.factor.levels: TRUE ## dummy.type: factor airq.test.imp = reimpute(airq.test, imp$desc) head(airq.test.imp) ##   Ozone Solar.R   Wind Temp Solar.R.dummy Wind.dummy ## 1   110     207  (0,8]   90         FALSE      FALSE ## 2    NA     222 (8,16]   92         FALSE      FALSE ## 3    NA     137 (8,16]   86         FALSE      FALSE ## 4    44     192 (8,16]   86         FALSE      FALSE ## 5    28     273 (8,16]   82         FALSE      FALSE ## 6    65     157 (8,16]   80         FALSE      FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/impute.html","id":"fusing-a-learner-with-imputation","dir":"Articles > Tutorial","previous_headings":"","what":"Fusing a learner with imputation","title":"Imputation of Missing Values","text":"can couple Learner (makeLearner()) imputation function makeImputeWrapper() basically formal arguments impute(). Like example impute Solar.R random numbers empirical distribution, Wind predictions classification tree generate dummy variables features. training resulting Learner (makeLearner()), impute() applied training set. prediction reimpute() called test set ImputationDesc (impute()) object training stage. aim predict ozone pollution meteorological variables. order create Task() need delete observations missing values target variable. following 3-fold cross-validated mean squared error calculated. second possibility fuse learner imputation provided makePreprocWrapperCaret(), interface carets caret::preProcess() function. caret::preProcess() works numeric features offers imputation k-nearest neighbors, bagged trees, median.","code":"lrn = makeImputeWrapper(\"regr.lm\", cols = list(Solar.R = imputeHist(),   Wind = imputeLearner(\"classif.rpart\")), dummy.cols = c(\"Solar.R\", \"Wind\")) lrn ## Learner regr.lm.imputed from package stats ## Type: regr ## Name: ; Short name:  ## Class: ImputeWrapper ## Properties: numerics,factors,se,weights,missings ## Predict-Type: response ## Hyperparameters: airq = subset(airq, subset = !is.na(airq$Ozone)) task = makeRegrTask(data = airq, target = \"Ozone\") rdesc = makeResampleDesc(\"CV\", iters = 3) r = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE) r$aggr ## mse.test.mean  ##      483.2621 lapply(r$models, getLearnerModel, more.unwrap = TRUE) ## [[1]] ##  ## Call: ## stats::lm(formula = f, data = d) ##  ## Coefficients: ##       (Intercept)            Solar.R         Wind(8,16]        Wind(16,24]   ##         -72.62255            0.05914          -27.65865          -27.50254   ##              Temp  Solar.R.dummyTRUE     Wind.dummyTRUE   ##           1.58889           -9.24041            1.45173   ##  ##  ## [[2]] ##  ## Call: ## stats::lm(formula = f, data = d) ##  ## Coefficients: ##       (Intercept)            Solar.R         Wind(8,16]        Wind(16,24]   ##        -118.37366            0.07683          -18.92091           -1.82817   ##              Temp  Solar.R.dummyTRUE     Wind.dummyTRUE   ##           2.01424           -7.18644          -14.57578   ##  ##  ## [[3]] ##  ## Call: ## stats::lm(formula = f, data = d) ##  ## Coefficients: ##       (Intercept)            Solar.R         Wind(8,16]        Wind(16,24]   ##         -93.94976            0.06823          -20.38373          -17.85132   ##              Temp  Solar.R.dummyTRUE     Wind.dummyTRUE   ##           1.75427          -17.09540            1.37333"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/integrated_learners.html","id":"classification-83","dir":"Articles > Tutorial","previous_headings":"","what":"Classification (83)","title":"Integrated Learners","text":"classification following additional learner properties relevant shown column Props: prob: method can predict probabilities, oneclass, twoclass, multiclass: One-class, two-class (binary) multi-class classification problems handled, class.weights: Class weights can handled.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/integrated_learners.html","id":"regression-59","dir":"Articles > Tutorial","previous_headings":"","what":"Regression (59)","title":"Integrated Learners","text":"Additional learner properties: se: Standard errors can predicted.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/integrated_learners.html","id":"survival-analysis-10","dir":"Articles > Tutorial","previous_headings":"","what":"Survival analysis (10)","title":"Integrated Learners","text":"Additional learner properties: prob: Probabilities can predicted, rcens, lcens, icens: learner can handle right, left /interval censored data.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/integrated_learners.html","id":"cluster-analysis-10","dir":"Articles > Tutorial","previous_headings":"","what":"Cluster analysis (10)","title":"Integrated Learners","text":"Additional learner properties: prob: Probabilities can predicted.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/integrated_learners.html","id":"cost-sensitive-classification","dir":"Articles > Tutorial","previous_headings":"","what":"Cost-sensitive classification","title":"Integrated Learners","text":"ordinary misclassification costs can use standard classification methods listed . example-dependent costs several ways generate cost-sensitive learners ordinary regression classification learners. See section cost-sensitive classification documentation makeCostSensClassifWrapper(), makeCostSensRegrWrapper() makeCostSensWeightedPairsWrapper() details.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/integrated_learners.html","id":"multilabel-classification-3","dir":"Articles > Tutorial","previous_headings":"","what":"Multilabel classification (3)","title":"Integrated Learners","text":"Moreover, can use binary relevance method apply ordinary classification learners multilabel problem. See documentation function makeMultilabelBinaryRelevanceWrapper() tutorial section multilabel classification details.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/learner.html","id":"constructing-a-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Constructing a learner","title":"Learners","text":"learner mlr generated calling makeLearner(). constructor need specify learning method want use. Moreover, can: Set hyperparameters. Control output later prediction, e.g., classification whether want factor predicted class labels probabilities. Set ID name object (methods later use ID name results annotate plots). first argument specifies algorithm use. naming convention classif.<R_method_name> classification methods, regr.<R_method_name> regression methods, surv.<R_method_name> survival analysis, cluster.<R_method_name> clustering methods, multilabel.<R_method_name> multilabel classification. Hyperparameter values can specified either via ... argument list via par.vals. first option preferred par.vals mainly used declare hyperparameters set differently mlr compared defaults underlying model. want change hyperparameter mlr default differs actual default, make sure also add entry \"note\" slot learner. entry describe reason change. Common ones turning automatic parallelization changing logical arguments learner enable conservative memory management. Occasionally, factor features may cause problems fewer levels present test data set training data. setting fix.factors.prediction = TRUE avoided adding factor level missing data test data set. Let’s look two learners created . generated learners objects class Learner (makeLearner()). class contains properties method, e.g., types features can handle, kind output possible prediction, whether multi-class problems, observations weights missing values supported. might noticed, currently special learner class cost-sensitive classification. ordinary misclassification costs can use standard classification methods. example-dependent costs several ways generate cost-sensitive learners ordinary regression classification learners. explained greater detail section cost-sensitive classification.","code":"# Classification tree, set it up for predicting probabilities classif.lrn = makeLearner(\"classif.randomForest\", predict.type = \"prob\", fix.factors.prediction = TRUE)  # Regression gradient boosting machine, specify hyperparameters via a list regr.lrn = makeLearner(\"regr.gbm\", par.vals = list(n.trees = 500, interaction.depth = 3))  # Cox proportional hazards model with custom name surv.lrn = makeLearner(\"surv.coxph\", id = \"cph\")  # K-means with 5 clusters cluster.lrn = makeLearner(\"cluster.kmeans\", centers = 5)  # Multilabel Random Ferns classification algorithm multilabel.lrn = makeLearner(\"multilabel.rFerns\") classif.lrn ## Learner classif.randomForest from package randomForest ## Type: classif ## Name: Random Forest; Short name: rf ## Class: classif.randomForest ## Properties: twoclass,multiclass,numerics,factors,ordered,prob,class.weights,oobpreds,featimp ## Predict-Type: prob ## Hyperparameters:  surv.lrn ## Learner cph from package survival ## Type: surv ## Name: Cox Proportional Hazard Model; Short name: coxph ## Class: surv.coxph ## Properties: numerics,factors,weights ## Predict-Type: response ## Hyperparameters:"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/learner.html","id":"accessing-a-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing a learner","title":"Learners","text":"Learner (makeLearner()) object list following elements contain information regarding hyperparameters type prediction. Slot $par.set object class ParamSet (ParamHelpers::makeParamSet()). contains, among others, type hyperparameters (e.g., numeric, logical), potential default values range allowed values. Moreover, mlr provides function getHyperPars() alternative getLearnerParVals() access current hyperparameter setting Learner, (makeLearner()) getParamSet() get description possible settings. particularly useful case wrapped Learner (makeLearner())s, example learner fused feature selection strategy, , learner well feature selection method, hyperparameters. details see section wrapped learners. can also use getParamSet() alias getLearnerParamSet() get quick overview available hyperparameters defaults learning method without explicitly constructing (calling makeLearner()). Functions accessing Learner’s meta information available mlr. can use getLearnerId(), getLearnerShortName() getLearnerType() get Learner’s ID, short name type, respectively. Moreover, order show required packages Learner, one can call getLearnerPackages().","code":"# Get the configured hyperparameter settings that deviate from the defaults cluster.lrn$par.vals ## $centers ## [1] 5  # Get the set of hyperparameters classif.lrn$par.set ##                      Type  len   Def   Constr Req Tunable Trafo ## ntree             integer    -   500 1 to Inf   -    TRUE     - ## mtry              integer    -     - 1 to Inf   -    TRUE     - ## replace           logical    -  TRUE        -   -    TRUE     - ## classwt     numericvector <NA>     - 0 to Inf   -    TRUE     - ## cutoff      numericvector <NA>     -   0 to 1   -    TRUE     - ## strata            untyped    -     -        -   -   FALSE     - ## sampsize    integervector <NA>     - 1 to Inf   -    TRUE     - ## nodesize          integer    -     1 1 to Inf   -    TRUE     - ## maxnodes          integer    -     - 1 to Inf   -    TRUE     - ## importance        logical    - FALSE        -   -    TRUE     - ## localImp          logical    - FALSE        -   -    TRUE     - ## proximity         logical    - FALSE        -   -   FALSE     - ## oob.prox          logical    -     -        -   Y   FALSE     - ## norm.votes        logical    -  TRUE        -   -   FALSE     - ## do.trace          logical    - FALSE        -   -   FALSE     - ## keep.forest       logical    -  TRUE        -   -   FALSE     - ## keep.inbag        logical    - FALSE        -   -   FALSE     -  # Get the type of prediction regr.lrn$predict.type ## [1] \"response\" # Get current hyperparameter settings getHyperPars(cluster.lrn) ## $centers ## [1] 5  # Get a description of all possible hyperparameter settings getParamSet(classif.lrn) ##                      Type  len   Def   Constr Req Tunable Trafo ## ntree             integer    -   500 1 to Inf   -    TRUE     - ## mtry              integer    -     - 1 to Inf   -    TRUE     - ## replace           logical    -  TRUE        -   -    TRUE     - ## classwt     numericvector <NA>     - 0 to Inf   -    TRUE     - ## cutoff      numericvector <NA>     -   0 to 1   -    TRUE     - ## strata            untyped    -     -        -   -   FALSE     - ## sampsize    integervector <NA>     - 1 to Inf   -    TRUE     - ## nodesize          integer    -     1 1 to Inf   -    TRUE     - ## maxnodes          integer    -     - 1 to Inf   -    TRUE     - ## importance        logical    - FALSE        -   -    TRUE     - ## localImp          logical    - FALSE        -   -    TRUE     - ## proximity         logical    - FALSE        -   -   FALSE     - ## oob.prox          logical    -     -        -   Y   FALSE     - ## norm.votes        logical    -  TRUE        -   -   FALSE     - ## do.trace          logical    - FALSE        -   -   FALSE     - ## keep.forest       logical    -  TRUE        -   -   FALSE     - ## keep.inbag        logical    - FALSE        -   -   FALSE     - getParamSet(\"classif.randomForest\") ##                      Type  len   Def   Constr Req Tunable Trafo ## ntree             integer    -   500 1 to Inf   -    TRUE     - ## mtry              integer    -     - 1 to Inf   -    TRUE     - ## replace           logical    -  TRUE        -   -    TRUE     - ## classwt     numericvector <NA>     - 0 to Inf   -    TRUE     - ## cutoff      numericvector <NA>     -   0 to 1   -    TRUE     - ## strata            untyped    -     -        -   -   FALSE     - ## sampsize    integervector <NA>     - 1 to Inf   -    TRUE     - ## nodesize          integer    -     1 1 to Inf   -    TRUE     - ## maxnodes          integer    -     - 1 to Inf   -    TRUE     - ## importance        logical    - FALSE        -   -    TRUE     - ## localImp          logical    - FALSE        -   -    TRUE     - ## proximity         logical    - FALSE        -   -   FALSE     - ## oob.prox          logical    -     -        -   Y   FALSE     - ## norm.votes        logical    -  TRUE        -   -   FALSE     - ## do.trace          logical    - FALSE        -   -   FALSE     - ## keep.forest       logical    -  TRUE        -   -   FALSE     - ## keep.inbag        logical    - FALSE        -   -   FALSE     - # Get object's id getLearnerId(surv.lrn) ## [1] \"cph\"  # Get the short name getLearnerShortName(classif.lrn) ## [1] \"rf\"  # Get the type of the learner getLearnerType(multilabel.lrn) ## [1] \"multilabel\"  # Get required packages getLearnerPackages(cluster.lrn) ## [1] \"stats\" \"clue\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/learner.html","id":"modifying-a-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Modifying a learner","title":"Learners","text":"also functions enable change certain aspects Learner (makeLearner()) without needing create new Learner (makeLearner()) scratch. examples.","code":"# Change the ID surv.lrn = setLearnerId(surv.lrn, \"CoxModel\") surv.lrn ## Learner CoxModel from package survival ## Type: surv ## Name: Cox Proportional Hazard Model; Short name: coxph ## Class: surv.coxph ## Properties: numerics,factors,weights ## Predict-Type: response ## Hyperparameters:  # Change the prediction type, predict a factor with class labels instead of probabilities classif.lrn = setPredictType(classif.lrn, \"response\")  # Change hyperparameter values cluster.lrn = setHyperPars(cluster.lrn, centers = 4)  # Go back to default hyperparameter values regr.lrn = removeHyperPars(regr.lrn, c(\"n.trees\", \"interaction.depth\"))"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/learner.html","id":"listing-learners","dir":"Articles > Tutorial","previous_headings":"","what":"Listing learners","title":"Learners","text":"list learners integrated mlr respective properties shown Appendix. like list available learners, maybe certain properties suitable certain learning Task() use function listLearners().","code":"# List everything in mlr lrns = listLearners() head(lrns[c(\"class\", \"package\")]) ##                 class      package ## 1         classif.ada    ada,rpart ## 2  classif.adaboostm1        RWeka ## 3 classif.bartMachine  bartMachine ## 4    classif.binomial        stats ## 5    classif.boosting adabag,rpart ## 6         classif.bst    bst,rpart  # List classifiers that can output probabilities lrns = listLearners(\"classif\", properties = \"prob\") head(lrns[c(\"class\", \"package\")]) ##                 class      package ## 1         classif.ada    ada,rpart ## 2  classif.adaboostm1        RWeka ## 3 classif.bartMachine  bartMachine ## 4    classif.binomial        stats ## 5    classif.boosting adabag,rpart ## 6         classif.C50          C50  # List classifiers that can be applied to iris (i.e., multiclass) and output probabilities lrns = listLearners(iris.task, properties = \"prob\") head(lrns[c(\"class\", \"package\")]) ##                class      package ## 1 classif.adaboostm1        RWeka ## 2   classif.boosting adabag,rpart ## 3        classif.C50          C50 ## 4    classif.cforest        party ## 5      classif.ctree        party ## 6   classif.cvglmnet       glmnet  # The calls above return character vectors, but you can also create learner objects head(listLearners(\"cluster\", create = TRUE), 2) ## [[1]] ## Learner cluster.cmeans from package e1071,clue ## Type: cluster ## Name: Fuzzy C-Means Clustering; Short name: cmeans ## Class: cluster.cmeans ## Properties: numerics,prob ## Predict-Type: response ## Hyperparameters: centers=2 ##  ##  ## [[2]] ## Learner cluster.Cobweb from package RWeka ## Type: cluster ## Name: Cobweb Clustering Algorithm; Short name: cobweb ## Class: cluster.Cobweb ## Properties: numerics ## Predict-Type: response ## Hyperparameters:"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/learning_curve.html","id":"plotting-the-learning-curve","dir":"Articles > Tutorial","previous_headings":"","what":"Plotting the learning curve","title":"Learning Curve Analysis","text":"mlr function generateLearningCurveData() can generate data learning curves multiple learners multiple performance measures . plotLearningCurve() result generateLearningCurveData() can plotted using ggplot2. plotLearningCurve() argument facet can either \"measure\" \"learner\". default facet = \"measure\" facetted subplots created measure input generateLearningCurveData(). facet = \"measure\" learners mapped color, vice versa.  happens generateLearningCurveData() following: learner internally wrapped DownsampleWrapper (makeDownsampleWrapper()). measure performance first step percs, say 0.1, first data split training test set according given resampling strategy. random sample containing 10% observations training set drawn used train learner. performance measured complete test set. steps repeated defined given resampling method value percs. first example simply passed vector learner names generateLearningCurveData(). usual, can also create learners beforehand provide list Learner (makeLearner()) objects, even pass mixed list Learner (makeLearner()) objects strings. Make sure learners unique ids.  can display performance train set well test set:","code":"r = generateLearningCurveData(   learners = c(\"classif.rpart\", \"classif.knn\"),   task = sonar.task,   percs = seq(0.1, 1, by = 0.2),   measures = list(tp, fp, tn, fn),   resampling = makeResampleDesc(method = \"CV\", iters = 5),   show.info = FALSE) plotLearningCurve(r) lrns = list(   makeLearner(cl = \"classif.ksvm\", id = \"ksvm1\", sigma = 0.2, C = 2),   makeLearner(cl = \"classif.ksvm\", id = \"ksvm2\", sigma = 0.1, C = 1),   \"classif.randomForest\" ) rin = makeResampleDesc(method = \"CV\", iters = 5) lc = generateLearningCurveData(learners = lrns, task = sonar.task,   percs = seq(0.1, 1, by = 0.1), measures = acc,   resampling = rin, show.info = FALSE) plotLearningCurve(lc) rin2 = makeResampleDesc(method = \"CV\", iters = 5, predict = \"both\") lc2 = generateLearningCurveData(learners = lrns, task = sonar.task,   percs = seq(0.1, 1, by = 0.1),   measures = list(acc, setAggregation(acc, train.mean)), resampling = rin2,   show.info = FALSE) plotLearningCurve(lc2, facet = \"learner\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/measures.html","id":"cost-sensitive-classification","dir":"Articles > Tutorial","previous_headings":"","what":"Cost-sensitive classification","title":"Implemented Performance Measures","text":"Note case ordinary misclassification costs can also generate performance measures cost matrices function makeCostMeasure(). details see tutorial page cost-sensitive classification also page custom performance measures.","code":""},{"path":[]},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"creating-a-task","dir":"Articles > Tutorial","previous_headings":"","what":"Creating a task","title":"Multilabel Classification","text":"first thing multilabel classification mlr get data right format. need data.frame consists features logical vector label indicates label present observation . can create MultilabelTask (Task()) like normal ClassifTask (Task()). Instead one target name specify vector targets correspond names logical variables data.frame. following example get yeast data frame already existing yeast.task(), extract 14 label names create task .","code":"yeast = getTaskData(yeast.task) labels = colnames(yeast)[1:14] yeast.task = makeMultilabelTask(id = \"multi\", data = yeast, target = labels) yeast.task ## Supervised task: multi ## Type: multilabel ## Target: label1,label2,label3,label4,label5,label6,label7,label8,label9,label10,label11,label12,label13,label14 ## Observations: 2417 ## Features: ##    numerics     factors     ordered functionals  ##         103           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 14 ##  label1  label2  label3  label4  label5  label6  label7  label8  label9 label10  ##     762    1038     983     862     722     597     428     480     178     253  ## label11 label12 label13 label14  ##     289    1816    1799      34"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"constructing-a-learner","dir":"Articles > Tutorial","previous_headings":"","what":"Constructing a learner","title":"Multilabel Classification","text":"Multilabel classification mlr can currently done two ways: Algorithm adaptation methods: Treat whole problem specific algorithm. Problem transformation methods: Transform problem, simple binary classification algorithms can applied.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"algorithm-adaptation-methods","dir":"Articles > Tutorial","previous_headings":"","what":"Algorithm adaptation methods","title":"Multilabel Classification","text":"Currently available algorithm adaptation methods R multivariate random forest [%randomForestSRC] package random ferns multilabel algorithm [%rFerns] package. can create learner algorithms like multiclass classification problems.","code":"lrn.rfsrc = makeLearner(\"multilabel.randomForestSRC\") lrn.rFerns = makeLearner(\"multilabel.rFerns\") lrn.rFerns ## Learner multilabel.rFerns from package rFerns ## Type: multilabel ## Name: Random ferns; Short name: rFerns ## Class: multilabel.rFerns ## Properties: numerics,factors,ordered ## Predict-Type: response ## Hyperparameters:"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"problem-transformation-methods","dir":"Articles > Tutorial","previous_headings":"","what":"Problem transformation methods","title":"Multilabel Classification","text":"generating wrapped multilabel learner first create binary (multiclass) classification learner makeLearner(). Afterwards apply function like makeMultilabelBinaryRelevanceWrapper(), makeMultilabelClassifierChainsWrapper(), makeMultilabelNestedStackingWrapper(), makeMultilabelDBRWrapper() makeMultilabelStackingWrapper() learner convert learner uses respective problem transformation method. can also generate binary relevance learner directly, can see example. different methods shortly described following.","code":"lrn.br = makeLearner(\"classif.rpart\", predict.type = \"prob\") lrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br) lrn.br ## Learner multilabel.binaryRelevance.classif.rpart from package rpart ## Type: multilabel ## Name: ; Short name:  ## Class: MultilabelBinaryRelevanceWrapper ## Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass ## Predict-Type: prob ## Hyperparameters: xval=0  lrn.br2 = makeMultilabelBinaryRelevanceWrapper(\"classif.rpart\") lrn.br2 ## Learner multilabel.binaryRelevance.classif.rpart from package rpart ## Type: multilabel ## Name: ; Short name:  ## Class: MultilabelBinaryRelevanceWrapper ## Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass ## Predict-Type: response ## Hyperparameters: xval=0"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"binary-relevance","dir":"Articles > Tutorial","previous_headings":"Problem transformation methods","what":"Binary relevance","title":"Multilabel Classification","text":"problem transformation method converts multilabel problem binary classification problems label applies simple binary classificator . mlr can done converting binary learner wrapped binary relevance multilabel learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"classifier-chains","dir":"Articles > Tutorial","previous_headings":"Problem transformation methods","what":"Classifier chains","title":"Multilabel Classification","text":"Trains consecutively labels input data. input data step augmented already trained labels (real observed values). Therefore order labels specified. prediction time labels predicted order training. required labels input data given previous done prediction respective label.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"nested-stacking","dir":"Articles > Tutorial","previous_headings":"Problem transformation methods","what":"Nested stacking","title":"Multilabel Classification","text":"classifier chains, labels input data real ones, estimations labels obtained already trained learners.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"dependent-binary-relevance","dir":"Articles > Tutorial","previous_headings":"Problem transformation methods","what":"Dependent binary relevance","title":"Multilabel Classification","text":"label trained real observed values labels. prediction phase label necessary labels obtained previous step base learner like binary relevance method.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"stacking","dir":"Articles > Tutorial","previous_headings":"Problem transformation methods","what":"Stacking","title":"Multilabel Classification","text":"dependent binary relevance method, training phase labels used input label obtained binary relevance method.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"train","dir":"Articles > Tutorial","previous_headings":"","what":"Train","title":"Multilabel Classification","text":"can train() model usual multilabel learner multilabel task input. can also pass subset weights arguments learner supports .","code":"mod = train(lrn.br, yeast.task) mod = train(lrn.br, yeast.task, subset = 1:1500, weights = rep(1/1500, 1500)) mod ## Model for learner.id=multilabel.binaryRelevance.classif.rpart; learner.class=MultilabelBinaryRelevanceWrapper ## Trained on: task.id = multi; obs = 1500; features = 103 ## Hyperparameters: xval=0  mod2 = train(lrn.rfsrc, yeast.task, subset = 1:100) mod2 ## Model for learner.id=multilabel.randomForestSRC; learner.class=multilabel.randomForestSRC ## Trained on: task.id = multi; obs = 100; features = 103 ## Hyperparameters: na.action=na.impute"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"predict","dir":"Articles > Tutorial","previous_headings":"","what":"Predict","title":"Multilabel Classification","text":"Prediction can done usual mlr predict (predict.WrappedModel()) passing trained model either task task argument new data newdata argument. always can specify subset data predicted. Depending chosen predict.type learner get true predicted values possibly probabilities class label. can extracted usual accessor functions getPredictionTruth(), getPredictionResponse() getPredictionProbabilities().","code":"pred = predict(mod, task = yeast.task, subset = 1:10) pred = predict(mod, newdata = yeast[1501:1600,]) names(as.data.frame(pred)) ##  [1] \"truth.label1\"     \"truth.label2\"     \"truth.label3\"     \"truth.label4\"     ##  [5] \"truth.label5\"     \"truth.label6\"     \"truth.label7\"     \"truth.label8\"     ##  [9] \"truth.label9\"     \"truth.label10\"    \"truth.label11\"    \"truth.label12\"    ## [13] \"truth.label13\"    \"truth.label14\"    \"prob.label1\"      \"prob.label2\"      ## [17] \"prob.label3\"      \"prob.label4\"      \"prob.label5\"      \"prob.label6\"      ## [21] \"prob.label7\"      \"prob.label8\"      \"prob.label9\"      \"prob.label10\"     ## [25] \"prob.label11\"     \"prob.label12\"     \"prob.label13\"     \"prob.label14\"     ## [29] \"response.label1\"  \"response.label2\"  \"response.label3\"  \"response.label4\"  ## [33] \"response.label5\"  \"response.label6\"  \"response.label7\"  \"response.label8\"  ## [37] \"response.label9\"  \"response.label10\" \"response.label11\" \"response.label12\" ## [41] \"response.label13\" \"response.label14\"  pred2 = predict(mod2, task = yeast.task) names(as.data.frame(pred2)) ##  [1] \"id\"               \"truth.label1\"     \"truth.label2\"     \"truth.label3\"     ##  [5] \"truth.label4\"     \"truth.label5\"     \"truth.label6\"     \"truth.label7\"     ##  [9] \"truth.label8\"     \"truth.label9\"     \"truth.label10\"    \"truth.label11\"    ## [13] \"truth.label12\"    \"truth.label13\"    \"truth.label14\"    \"response.label1\"  ## [17] \"response.label2\"  \"response.label3\"  \"response.label4\"  \"response.label5\"  ## [21] \"response.label6\"  \"response.label7\"  \"response.label8\"  \"response.label9\"  ## [25] \"response.label10\" \"response.label11\" \"response.label12\" \"response.label13\" ## [29] \"response.label14\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"performance","dir":"Articles > Tutorial","previous_headings":"","what":"Performance","title":"Multilabel Classification","text":"performance prediction can assessed via function performance(). can specify via measures argument measure(s) calculate. default measure multilabel classification Hamming loss multilabel.hamloss. available measures multilabel classification can shown listMeasures() found table performance measures ?measures() documentation page.","code":"performance(pred)  performance(pred2, measures = list(multilabel.subset01, multilabel.hamloss, multilabel.acc,   multilabel.f1, timepredict))  ## multilabel.subset01  multilabel.hamloss      multilabel.acc  ##           0.8721556           0.2047402           0.4611732  ##       multilabel.f1         timepredict  ##           0.5715320           0.9210000  listMeasures(\"multilabel\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"resampling","dir":"Articles > Tutorial","previous_headings":"","what":"Resampling","title":"Multilabel Classification","text":"evaluating overall performance learning algorithm can resampling. usual define resampling strategy, either via makeResampleDesc() makeResampleInstance(). can run resample() function. default measure Hamming loss calculated.","code":"rdesc = makeResampleDesc(method = \"CV\", stratify = FALSE, iters = 3) r = resample(learner = lrn.br, task = yeast.task, resampling = rdesc, show.info = FALSE) r  ## Resample Result ## Task: multi ## Learner: multilabel.binaryRelevance.classif.rpart ## Aggr perf: multilabel.hamloss.test.mean=0.2200186 ## Runtime: 6.67501  r = resample(learner = lrn.rFerns, task = yeast.task, resampling = rdesc, show.info = FALSE) r  ## Resample Result ## Task: multi ## Learner: multilabel.rFerns ## Aggr perf: multilabel.hamloss.test.mean=0.4762392 ## Runtime: 0.320362"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/multilabel.html","id":"binary-performance","dir":"Articles > Tutorial","previous_headings":"","what":"Binary performance","title":"Multilabel Classification","text":"want calculate binary performance measure like, e.g., accuracy, mmce auc label, can use function getMultilabelBinaryPerformances(). can apply function multilabel prediction, e.g., also resample multilabel prediction. calculating auc need predicted probabilities.","code":"getMultilabelBinaryPerformances(pred, measures = list(acc, mmce, auc)) ##         acc.test.mean mmce.test.mean auc.test.mean ## label1           0.75           0.25     0.6321925 ## label2           0.64           0.36     0.6547917 ## label3           0.68           0.32     0.7118227 ## label4           0.69           0.31     0.6764835 ## label5           0.73           0.27     0.6676923 ## label6           0.70           0.30     0.6417739 ## label7           0.81           0.19     0.5968750 ## label8           0.73           0.27     0.5164474 ## label9           0.89           0.11     0.4688458 ## label10          0.86           0.14     0.3996463 ## label11          0.85           0.15     0.5000000 ## label12          0.76           0.24     0.5330667 ## label13          0.75           0.25     0.5938610 ## label14          1.00           0.00            NA  getMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce)) ##         acc.test.mean mmce.test.mean ## label1     0.70459247      0.2954075 ## label2     0.58874638      0.4112536 ## label3     0.70045511      0.2995449 ## label4     0.70955730      0.2904427 ## label5     0.70707489      0.2929251 ## label6     0.58295408      0.4170459 ## label7     0.54075300      0.4592470 ## label8     0.52420356      0.4757964 ## label9     0.30326851      0.6967315 ## label10    0.44890360      0.5510964 ## label11    0.45966074      0.5403393 ## label12    0.53537443      0.4646256 ## label13    0.53620190      0.4637981 ## label14    0.02275548      0.9772445"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"tuning","dir":"Articles > Tutorial","previous_headings":"","what":"Tuning","title":"Nested Resampling","text":"might recall tutorial page tuning, need define search space function ParamHelpers::makeParamSet(), search strategy makeTuneControl*(TuneControl()), method evaluate hyperparameter settings (.e., inner resampling strategy performance measure). classification example. evaluate performance support vector machine (kernlab::ksvm()) tuned cost parameter C RBF kernel parameter sigma. use 3-fold cross-validation outer subsampling 2 iterations inner loop. tuing grid search used find hyperparameters lowest error rate (mmce default measure classification). wrapped Learner (makeLearner()) generated calling makeTuneWrapper(). Note practice parameter set larger. common recommendation 2^(-12:12) C sigma. can obtain error rates 3 outer test sets :","code":"# Tuning in inner resampling loop ps = makeParamSet(   makeDiscreteParam(\"C\", values = 2^(-2:2)),   makeDiscreteParam(\"sigma\", values = 2^(-2:2)) ) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"Subsample\", iters = 2) lrn = makeTuneWrapper(\"classif.ksvm\", resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)  # Outer resampling loop outer = makeResampleDesc(\"CV\", iters = 3) r = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)  r  ## Resample Result ## Task: iris-example ## Learner: classif.ksvm.tuned ## Aggr perf: mmce.test.mean=0.0400000 ## Runtime: 5.50584 r$measures.test ##   iter mmce ## 1    1 0.06 ## 2    2 0.04 ## 3    3 0.02"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"accessing-the-tuning-result","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing the tuning result","title":"Nested Resampling","text":"kept results tuning evaluations. example one might want find , best obtained configurations vary different outer splits. storing entire models may expensive (possible setting models = TRUE) used extract option resample(). Function getTuneResult() returns, among things, optimal hyperparameter values optimization path (ParamHelpers::OptPath()) iteration outer resampling loop. Note performance values shown printing r$extract aggregated performances resulting inner resampling outer training set best hyperparameter configurations (confused r$measures.test shown ). can compare optimal parameter settings obtained 3 resampling iterations. can see, optimal configuration usually depends data. may able identify range parameter settings achieve good performance though, e.g., values C least 1 values sigma 0 1. function getNestedTuneResultsOptPathDf() can extract optimization paths 3 outer cross-validation iterations inspection analysis. stacked one data.frame column iter indicating resampling iteration. visualize opt.paths 3 outer resampling iterations.  Another useful function getNestedTuneResultsX(), extracts best found hyperparameter settings outer resampling iteration. can furthermore access resampling indices inner level using getResamplingIndices() used either extract = getTuneResult extract = getFeatSelResult resample() call:","code":"r$extract ## [[1]] ## Tune result: ## Op. pars: C=1; sigma=0.5 ## mmce.test.mean=0.0294118 ##  ## [[2]] ## Tune result: ## Op. pars: C=2; sigma=0.25 ## mmce.test.mean=0.0294118 ##  ## [[3]] ## Tune result: ## Op. pars: C=2; sigma=0.25 ## mmce.test.mean=0.0147059  names(r$extract[[1]]) ## [1] \"learner\"    \"control\"    \"x\"          \"y\"          \"resampling\" ## [6] \"threshold\"  \"opt.path\" opt.paths = getNestedTuneResultsOptPathDf(r) head(opt.paths, 10)  ##       C sigma mmce.test.mean dob eol error.message exec.time iter ## 1  0.25  0.25     0.10294118   1  NA          <NA>     1.463    1 ## 2   0.5  0.25     0.11764706   2  NA          <NA>     0.036    1 ## 3     1  0.25     0.07352941   3  NA          <NA>     0.043    1 ## 4     2  0.25     0.07352941   4  NA          <NA>     0.040    1 ## 5     4  0.25     0.08823529   5  NA          <NA>     0.042    1 ## 6  0.25   0.5     0.13235294   6  NA          <NA>     0.041    1 ## 7   0.5   0.5     0.07352941   7  NA          <NA>     0.043    1 ## 8     1   0.5     0.07352941   8  NA          <NA>     0.042    1 ## 9     2   0.5     0.07352941   9  NA          <NA>     0.037    1 ## 10    4   0.5     0.10294118  10  NA          <NA>     0.042    1 g = ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean)) g + geom_tile() + facet_wrap(~iter) getNestedTuneResultsX(r) ##   C sigma ## 1 1  0.50 ## 2 2  0.25 ## 3 2  0.25 getResamplingIndices(r, inner = TRUE) ## [[1]] ## [[1]]$train.inds ## [[1]]$train.inds[[1]] ##  [1] 122 139 148  93  60  28 146 141 126   1  38 117  10 124 129  34 101  36  91 ## [20]   2  99  33  42  61  51  87  96  54   7 132 115 113  32 150  41  40  64  69 ## [39]  12 110 112 106  31  78 109 102   9  44  84 104  56 147  86 130 123 105 125 ## [58]  70 133 135 142  58 128 111  16  55 ##  ## [[1]]$train.inds[[2]] ##  [1]  36 136 118  93 123   2 117  75  33 109  98  62 116 144  56 150  28   1  82 ## [20] 115  20  86 125  29  14  44  11  17  54 133  91  60  18  99  73 104  37 132 ## [39]   9   7  61  68  87  65 106  50 130 147  16  38 114 139  12 145  70 124 134 ## [58] 146  58  42  55  31  74  77  26 148 ##  ##  ## [[1]]$test.inds ## [[1]]$test.inds[[1]] ##  [1]  73  20  90  13  80  68  77  14  26 145  11  37 118 103 116  62 134  29  27 ## [20]  98  65  30  18 100  82 144  22  50  74  17   5  75 114 136 ##  ## [[1]]$test.inds[[2]] ##  [1] 141  69  90  41  34 112  13  80 103  84  32 110 142  27 122  30 135 113 102 ## [20] 100 128  96  10 101 126 111  51  22  64 105   5  40  78 129 ##  ##  ##  ## [[2]] ## [[2]]$train.inds ## [[2]]$train.inds[[1]] ##  [1] 143  56 119 108  48  49  85  43 120  24 135 146 149 114  12 133  40  36   6 ## [20]  33 107  13 128  31  73 127  46 140  59  41  51  35 100  72   7  53  44  77 ## [39] 150  20 105  74  90 118  52  92  79 137   3  57  16 121  87  45  76  80  30 ## [58]  94  26  95  15  63 126  89  67 124 ##  ## [[2]]$train.inds[[2]] ##  [1]  59 111   3  48  76  16 140 149   8   5 128  94  20  79  87  42  85  83  35 ## [20] 138  62  56  21  44 121  25 135  18 144 114  45  89 127  36  88  81 101  92 ## [39]  73  39 145  61  24 150  90  51  13   4 108  70  26 104 119 146 129  46  77 ## [58]  17 133  97  23 107 120  63 143 118 ##  ##  ## [[2]]$test.inds ## [[2]]$test.inds[[1]] ##  [1]   5  17  19  18  55  61  25  47   4 131 129 101 144  88  81  99  62  70 145 ## [20]  50 111  23  21  39  97  58  42  71 104  66 138  78   8  83 ##  ## [[2]]$test.inds[[2]] ##  [1] 126 100  19  15  55  40  43  47  74 105   7 131 124  49  12  53  99  33   6 ## [20]  50  41  52  58  67  31  71  66  57  78  95  72  80 137  30 ##  ##  ##  ## [[3]] ## [[3]]$train.inds ## [[3]]$train.inds[[1]] ##  [1]  63  45  10  75  69 113 106  92   1  29  89  47 142  81 122   6  65  25  71 ## [20]  53  60 120  93 136   9 143 110  48  79  86  19   3  57  24  67 123  66 131 ## [39] 127 140  23   2  85  72  76  39  46 119  82 138 130 149   8  35 116 125  14 ## [58]  34 117  15  64   4  98  27 148 103 ##  ## [[3]]$train.inds[[2]] ##  [1] 125 140 119 147  84  79  96  47  69  93   3  91 131  95  43  35 134 110  45 ## [20]  85  60  27  53 103   6 137  49  67 115  83 116  76  25  64 149  97   8  52 ## [39]  34  89  28   1  66  21  19  29  11  32 112   9 120   4 108 102  10 127 117 ## [58] 138 113  75 141 143  39  59  22 132 ##  ##  ## [[3]]$test.inds ## [[3]]$test.inds[[1]] ##  [1]  52  37  88  22 109  95 102  84 132  83 134 141 115 121 108  54  94 112  32 ## [20]  38  43 139 147  49  28  59  21  97  68 107  91 137  96  11 ##  ## [[3]]$test.inds[[2]] ##  [1] 130 122  14  37  63  92  82  88 109  57  24  71  72 121  81 142  54  46  94 ## [20]  98  48  38  23 139  86   2 148  15  68 107 136 123 106  65"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"feature-selection","dir":"Articles > Tutorial","previous_headings":"","what":"Feature selection","title":"Nested Resampling","text":"might recall section feature selection, mlr supports filter wrapper approach.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"wrapper-methods","dir":"Articles > Tutorial","previous_headings":"","what":"Wrapper methods","title":"Nested Resampling","text":"Wrapper methods use performance learning algorithm assess usefulness feature set. order select feature subset learner trained repeatedly different feature subsets subset leads best learner performance chosen. feature selection inner resampling loop, need choose search strategy (function makeFeatSelControl* (FeatSelControl())), performance measure inner resampling strategy. use function makeFeatSelWrapper() bind everything together. use sequential forward selection linear regression BostonHousing (mlbench::BostonHousing() data set (bh.task()).","code":"# Feature selection in inner resampling loop inner = makeResampleDesc(\"CV\", iters = 3) lrn = makeFeatSelWrapper(\"regr.lm\",   resampling = inner,   control = makeFeatSelControlSequential(method = \"sfs\"), show.info = FALSE)  # Outer resampling loop outer = makeResampleDesc(\"Subsample\", iters = 2) r = resample(   learner = lrn, task = bh.task, resampling = outer, extract = getFeatSelResult,   show.info = FALSE)  r  ## Resample Result ## Task: BostonHousing-example ## Learner: regr.lm.featsel ## Aggr perf: mse.test.mean=24.8753005 ## Runtime: 7.01506  r$measures.test  ##   iter      mse ## 1    1 22.28967 ## 2    2 27.46093"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"accessing-the-selected-features","dir":"Articles > Tutorial","previous_headings":"Wrapper methods","what":"Accessing the selected features","title":"Nested Resampling","text":"result feature selection can extracted function getFeatSelResult(). also possible keep whole models (makeWrappedModel()) setting models = TRUE calling resample(). tuning, can extract optimization paths. resulting data.frames contain, among others, binary columns features, indicating included linear regression model, corresponding performances. easy--read version optimization path sequential feature selection can obtained function analyzeFeatSelResult().","code":"r$extract ## [[1]] ## FeatSel result: ## Features (8): zn, chas, nox, rm, dis, ptratio, b, lstat ## mse.test.mean=26.8943900 ##  ## [[2]] ## FeatSel result: ## Features (10): crim, zn, nox, rm, dis, rad, tax, ptratio, b, l... ## mse.test.mean=21.5240684  # Selected features in the first outer resampling iteration r$extract[[1]]$x ## [1] \"zn\"      \"chas\"    \"nox\"     \"rm\"      \"dis\"     \"ptratio\" \"b\"       ## [8] \"lstat\"  # Resampled performance of the selected feature subset on the first inner training set r$extract[[1]]$y ## mse.test.mean  ##      26.89439 opt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path)) head(opt.paths[[1]])  ##   crim zn indus chas nox rm age dis rad tax ptratio b lstat mse.test.mean ## 1    0  0     0    0   0  0   0   0   0   0       0 0     0      84.52018 ## 2    1  0     0    0   0  0   0   0   0   0       0 0     0      95.46348 ## 3    0  1     0    0   0  0   0   0   0   0       0 0     0      74.97858 ## 4    0  0     1    0   0  0   0   0   0   0       0 0     0      66.35546 ## 5    0  0     0    1   0  0   0   0   0   0       0 0     0      81.49228 ## 6    0  0     0    0   1  0   0   0   0   0       0 0     0      67.72664 ##   dob eol error.message exec.time ## 1   1   2          <NA>     0.031 ## 2   2   2          <NA>     0.037 ## 3   2   2          <NA>     0.027 ## 4   2   2          <NA>     0.030 ## 5   2   2          <NA>     0.032 ## 6   2   2          <NA>     0.031 analyzeFeatSelResult(r$extract[[1]]) ## Features         : 8 ## Performance      : mse.test.mean=26.8943900 ## zn, chas, nox, rm, dis, ptratio, b, lstat ##  ## Path to optimum: ## - Features:    0  Init   :                       Perf = 91.377  Diff: NA  * ## - Features:    1  Add    : lstat                 Perf = 40.871  Diff: 50.506  * ## - Features:    2  Add    : rm                    Perf = 34.606  Diff: 6.265  * ## - Features:    3  Add    : ptratio               Perf = 31.673  Diff: 2.9328  * ## - Features:    4  Add    : dis                   Perf = 30.535  Diff: 1.1381  * ## - Features:    5  Add    : nox                   Perf = 28.968  Diff: 1.5667  * ## - Features:    6  Add    : zn                    Perf = 27.993  Diff: 0.97562  * ## - Features:    7  Add    : b                     Perf = 27.253  Diff: 0.73917  * ## - Features:    8  Add    : chas                  Perf = 26.894  Diff: 0.35905  * ##  ## Stopped, because no improving feature was found."},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"filter-methods-with-tuning","dir":"Articles > Tutorial","previous_headings":"","what":"Filter methods with tuning","title":"Nested Resampling","text":"Filter methods assign importance value feature. Based values can select feature subset either keeping features importance higher certain threshold keeping fixed number percentage highest ranking features. Often, neither theshold number percentage features known advance thus tuning necessary. example threshold value (fw.threshold) tuned inner resampling loop. purpose base Learner (makeLearner()) \"regr.lm\" wrapped two times. First, makeFilterWrapper() used fuse linear regression feature filtering preprocessing step. tuning step added makeTuneWrapper().","code":"## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE. # Tuning of the percentage of selected filters in the inner loop lrn = makeFilterWrapper(learner = \"regr.lm\", fw.method = \"FSelectorRcpp_information.gain\") ps = makeParamSet(makeDiscreteParam(\"fw.threshold\", values = seq(0, 1, 0.2))) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"CV\", iters = 3) lrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)  # Outer resampling loop outer = makeResampleDesc(\"CV\", iters = 3) r = resample(learner = lrn, task = bh.task, resampling = outer, models = TRUE, show.info = FALSE) r  ## Resample Result ## Task: BostonHousing-example ## Learner: regr.lm.filtered.tuned ## Aggr perf: mse.test.mean=23.5449481 ## Runtime: 3.85235"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"accessing-the-selected-features-and-optimal-percentage","dir":"Articles > Tutorial","previous_headings":"Filter methods with tuning","what":"Accessing the selected features and optimal percentage","title":"Nested Resampling","text":"example kept complete model (makeWrappedModel())s. examples show extract information model (makeWrappedModel())s. result feature selection can extracted function getFilteredFeatures(). Almost always 13 features selected. tune results (TuneResult()) optimization paths (ParamHelpers::OptPath()) accessed.","code":"r$models ## [[1]] ## Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper ## Trained on: task.id = BostonHousing-example; obs = 337; features = 13 ## Hyperparameters:  ##  ## [[2]] ## Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper ## Trained on: task.id = BostonHousing-example; obs = 337; features = 13 ## Hyperparameters:  ##  ## [[3]] ## Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper ## Trained on: task.id = BostonHousing-example; obs = 338; features = 13 ## Hyperparameters: lapply(r$models, function(x) getFilteredFeatures(x$learner.model$next.model)) ## [[1]] ##  [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"     ##  [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"b\"       \"lstat\"   ##  ## [[2]] ##  [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"     ##  [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"b\"       \"lstat\"   ##  ## [[3]] ##  [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"     ##  [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"b\"       \"lstat\" res = lapply(r$models, getTuneResult) res ## [[1]] ## Tune result: ## Op. pars: fw.threshold=0 ## mse.test.mean=24.0256189 ##  ## [[2]] ## Tune result: ## Op. pars: fw.threshold=0 ## mse.test.mean=22.3639004 ##  ## [[3]] ## Tune result: ## Op. pars: fw.threshold=0 ## mse.test.mean=25.7198156  opt.paths = lapply(res, function(x) as.data.frame(x$opt.path)) opt.paths[[1]][, -ncol(opt.paths[[1]])] ##   fw.threshold mse.test.mean dob eol error.message ## 1            0      24.02562   1  NA          <NA> ## 2          0.2      67.30442   2  NA          <NA> ## 3          0.4      67.82551   3  NA          <NA> ## 4          0.6      67.82551   4  NA          <NA> ## 5          0.8      67.82551   5  NA          <NA> ## 6            1      67.82551   6  NA          <NA>"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"benchmark-experiments","dir":"Articles > Tutorial","previous_headings":"","what":"Benchmark experiments","title":"Nested Resampling","text":"benchmark experiment multiple learners compared one several tasks (see also section benchmarking. Nested resampling benchmark experiments achieved way resampling: First, use makeTuneWrapper() makeFeatSelWrapper() generate wrapped Learner (makeLearner())s inner resampling strategies choice. Second, call benchmark() specify outer resampling strategies tasks. inner resampling strategies resample descriptions (makeResampleDesc()). can use different inner resampling strategies different wrapped learners. example might practical fewer subsampling bootstrap iterations slower learners. larger benchmark experiments might want look section parallelization. mentioned section benchmark experiments can also use different resampling strategies different learning tasks passing list resampling descriptions instances benchmark(). see three examples show different benchmark settings: Two data sets + two classification algorithms + tuning One data set + two regression algorithms + feature selection One data set + two regression algorithms + feature filtering + tuning","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"example-1-two-tasks-two-learners-tuning","dir":"Articles > Tutorial","previous_headings":"","what":"Example 1: Two tasks, two learners, tuning","title":"Nested Resampling","text":"benchmark experiment two data sets, datasets::iris() mlbench::sonar(), two Learner (makeLearner())s, kernlab::ksvm() kknn::kknn(), tuned. inner resampling strategies use holdout kernlab::ksvm() subsampling 3 iterations kknn::kknn(). outer resampling strategies take holdout datasets::iris() bootstrap 2 iterations mlbench::sonar() data (sonar.task()). consider accuracy (acc), used tuning criterion, also calculate balanced error rate (ber). print method BenchmarkResult() shows aggregated performances outer resampling loop. might recall, mlr offers several accessor function extract information benchmark result. listed help page BenchmarkResult() many examples shown tutorial page benchmark experiments. performance values individual outer resampling runs can obtained getBMRPerformances(). Note , since used different outer resampling strategies two tasks, number rows per task differ. results parameter tuning can obtained function getBMRTuneResults(). several accessor functions clearer representation data.frame can achieved setting .df = TRUE. also possible extract tuning results individual tasks learners , shown earlier examples, inspect optimization path (ParamHelpers::OptPath()).","code":"# List of learning tasks tasks = list(iris.task, sonar.task)  # Tune svm in the inner resampling loop ps = makeParamSet(   makeDiscreteParam(\"C\", 2^(-1:1)),   makeDiscreteParam(\"sigma\", 2^(-1:1))) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"Holdout\") lrn1 = makeTuneWrapper(\"classif.ksvm\",   resampling = inner, par.set = ps, control = ctrl,   show.info = FALSE)  # Tune k-nearest neighbor in inner resampling loop ps = makeParamSet(makeDiscreteParam(\"k\", 3:5)) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"Subsample\", iters = 3) lrn2 = makeTuneWrapper(\"classif.kknn\",   resampling = inner, par.set = ps, control = ctrl,   show.info = FALSE) ## Loading required package: kknn  # Learners lrns = list(lrn1, lrn2)  # Outer resampling loop outer = list(makeResampleDesc(\"Holdout\"), makeResampleDesc(\"Bootstrap\", iters = 2)) res = benchmark(lrns, tasks, outer,   measures = list(acc, ber), show.info = FALSE,   keep.extract = TRUE) res ##         task.id         learner.id acc.test.mean ber.test.mean ## 1  iris-example classif.ksvm.tuned     0.9800000    0.02222222 ## 2  iris-example classif.kknn.tuned     0.9600000    0.04305556 ## 3 Sonar-example classif.ksvm.tuned     0.4903819    0.50000000 ## 4 Sonar-example classif.kknn.tuned     0.8725237    0.12827855 getBMRPerformances(res, as.df = TRUE) ##         task.id         learner.id iter       acc        ber ## 1  iris-example classif.ksvm.tuned    1 0.9800000 0.02222222 ## 2  iris-example classif.kknn.tuned    1 0.9600000 0.04305556 ## 3 Sonar-example classif.ksvm.tuned    1 0.5116279 0.50000000 ## 4 Sonar-example classif.ksvm.tuned    2 0.4691358 0.50000000 ## 5 Sonar-example classif.kknn.tuned    1 0.9302326 0.07142857 ## 6 Sonar-example classif.kknn.tuned    2 0.8148148 0.18512852 getBMRTuneResults(res) ## $`iris-example` ## $`iris-example`$classif.ksvm.tuned ## $`iris-example`$classif.ksvm.tuned[[1]] ## Tune result: ## Op. pars: C=2; sigma=0.5 ## mmce.test.mean=0.0294118 ##  ##  ## $`iris-example`$classif.kknn.tuned ## $`iris-example`$classif.kknn.tuned[[1]] ## Tune result: ## Op. pars: k=5 ## mmce.test.mean=0.0392157 ##  ##  ##  ## $`Sonar-example` ## $`Sonar-example`$classif.ksvm.tuned ## $`Sonar-example`$classif.ksvm.tuned[[1]] ## Tune result: ## Op. pars: C=1; sigma=2 ## mmce.test.mean=0.2285714 ##  ## $`Sonar-example`$classif.ksvm.tuned[[2]] ## Tune result: ## Op. pars: C=0.5; sigma=0.5 ## mmce.test.mean=0.2714286 ##  ##  ## $`Sonar-example`$classif.kknn.tuned ## $`Sonar-example`$classif.kknn.tuned[[1]] ## Tune result: ## Op. pars: k=5 ## mmce.test.mean=0.0714286 ##  ## $`Sonar-example`$classif.kknn.tuned[[2]] ## Tune result: ## Op. pars: k=4 ## mmce.test.mean=0.0666667 getBMRTuneResults(res, as.df = TRUE) ##         task.id         learner.id iter   C sigma mmce.test.mean  k ## 1  iris-example classif.ksvm.tuned    1 2.0   0.5     0.02941176 NA ## 2  iris-example classif.kknn.tuned    1  NA    NA     0.03921569  5 ## 3 Sonar-example classif.ksvm.tuned    1 1.0   2.0     0.22857143 NA ## 4 Sonar-example classif.ksvm.tuned    2 0.5   0.5     0.27142857 NA ## 5 Sonar-example classif.kknn.tuned    1  NA    NA     0.07142857  5 ## 6 Sonar-example classif.kknn.tuned    2  NA    NA     0.06666667  4 tune.res = getBMRTuneResults(res,   task.ids = \"Sonar-example\", learner.ids = \"classif.ksvm.tuned\",   as.df = TRUE) tune.res ##         task.id         learner.id iter   C sigma mmce.test.mean ## 1 Sonar-example classif.ksvm.tuned    1 1.0   2.0      0.2285714 ## 2 Sonar-example classif.ksvm.tuned    2 0.5   0.5      0.2714286  getNestedTuneResultsOptPathDf(res$results[[\"Sonar-example\"]][[\"classif.ksvm.tuned\"]]) ##      C sigma mmce.test.mean dob eol error.message exec.time iter ## 1  0.5   0.5      0.2285714   1  NA          <NA>     0.031    1 ## 2    1   0.5      0.2285714   2  NA          <NA>     0.031    1 ## 3    2   0.5      0.2285714   3  NA          <NA>     0.030    1 ## 4  0.5     1      0.2285714   4  NA          <NA>     0.032    1 ## 5    1     1      0.2285714   5  NA          <NA>     0.032    1 ## 6    2     1      0.2285714   6  NA          <NA>     0.045    1 ## 7  0.5     2      0.2285714   7  NA          <NA>     0.030    1 ## 8    1     2      0.2285714   8  NA          <NA>     0.030    1 ## 9    2     2      0.2285714   9  NA          <NA>     0.030    1 ## 10 0.5   0.5      0.2714286   1  NA          <NA>     0.032    2 ## 11   1   0.5      0.2714286   2  NA          <NA>     0.030    2 ## 12   2   0.5      0.2714286   3  NA          <NA>     0.033    2 ## 13 0.5     1      0.2714286   4  NA          <NA>     0.032    2 ## 14   1     1      0.2714286   5  NA          <NA>     0.031    2 ## 15   2     1      0.2714286   6  NA          <NA>     0.044    2 ## 16 0.5     2      0.2714286   7  NA          <NA>     0.028    2 ## 17   1     2      0.2714286   8  NA          <NA>     0.030    2 ## 18   2     2      0.2714286   9  NA          <NA>     0.029    2"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"example-2-one-task-two-learners-feature-selection","dir":"Articles > Tutorial","previous_headings":"","what":"Example 2: One task, two learners, feature selection","title":"Nested Resampling","text":"Let’s see can feature selection benchmark experiment: selected features can extracted function getBMRFeatSelResults(). default, nested list, first level indicating task second level indicating learner, returned. single learner , case, single task considered, setting drop = TRUE simplifies result flat list. can access results individual learners tasks inspect . tuning, can extract optimization paths. resulting data.frames contain, among others, binary columns features, indicating included linear regression model, corresponding performances. analyzeFeatSelResult() gives clearer overview.","code":"# Feature selection in inner resampling loop ctrl = makeFeatSelControlSequential(method = \"sfs\") inner = makeResampleDesc(\"Subsample\", iters = 2) lrn = makeFeatSelWrapper(\"regr.lm\", resampling = inner, control = ctrl, show.info = FALSE)  # Learners lrns = list(\"regr.rpart\", lrn)  # Outer resampling loop outer = makeResampleDesc(\"Subsample\", iters = 2) res = benchmark(   tasks = bh.task, learners = lrns, resampling = outer,   show.info = FALSE, keep.extract = TRUE)  res ##                 task.id      learner.id mse.test.mean ## 1 BostonHousing-example      regr.rpart      23.58119 ## 2 BostonHousing-example regr.lm.featsel      24.75507 getBMRFeatSelResults(res) ## $`BostonHousing-example` ## $`BostonHousing-example`$regr.rpart ## NULL ##  ## $`BostonHousing-example`$regr.lm.featsel ## $`BostonHousing-example`$regr.lm.featsel[[1]] ## FeatSel result: ## Features (7): indus, rm, age, dis, ptratio, b, lstat ## mse.test.mean=29.4228023 ##  ## $`BostonHousing-example`$regr.lm.featsel[[2]] ## FeatSel result: ## Features (9): zn, indus, chas, nox, rm, dis, ptratio, b, lstat ## mse.test.mean=26.1964766 getBMRFeatSelResults(res, drop = TRUE) ## $regr.rpart ## NULL ##  ## $regr.lm.featsel ## $regr.lm.featsel[[1]] ## FeatSel result: ## Features (7): indus, rm, age, dis, ptratio, b, lstat ## mse.test.mean=29.4228023 ##  ## $regr.lm.featsel[[2]] ## FeatSel result: ## Features (9): zn, indus, chas, nox, rm, dis, ptratio, b, lstat ## mse.test.mean=26.1964766 feats = getBMRFeatSelResults(res, learner.id = \"regr.lm.featsel\", drop = TRUE)  # Selected features in the first outer resampling iteration feats[[1]]$x ## [1] \"indus\"   \"rm\"      \"age\"     \"dis\"     \"ptratio\" \"b\"       \"lstat\"  # Resampled performance of the selected feature subset on the first inner training set feats[[1]]$y ## mse.test.mean  ##       29.4228 opt.paths = lapply(feats, function(x) as.data.frame(x$opt.path)) head(opt.paths[[1]][, -ncol(opt.paths[[1]])]) ##   crim zn indus chas nox rm age dis rad tax ptratio b lstat mse.test.mean dob ## 1    0  0     0    0   0  0   0   0   0   0       0 0     0      80.74823   1 ## 2    1  0     0    0   0  0   0   0   0   0       0 0     0      70.11222   2 ## 3    0  1     0    0   0  0   0   0   0   0       0 0     0      71.63520   2 ## 4    0  0     1    0   0  0   0   0   0   0       0 0     0      64.69041   2 ## 5    0  0     0    1   0  0   0   0   0   0       0 0     0      83.04175   2 ## 6    0  0     0    0   1  0   0   0   0   0       0 0     0      68.37808   2 ##   eol error.message ## 1   2          <NA> ## 2   2          <NA> ## 3   2          <NA> ## 4   2          <NA> ## 5   2          <NA> ## 6   2          <NA>  analyzeFeatSelResult(feats[[1]]) ## Features         : 7 ## Performance      : mse.test.mean=29.4228023 ## indus, rm, age, dis, ptratio, b, lstat ##  ## Path to optimum: ## - Features:    0  Init   :                       Perf = 80.748  Diff: NA  * ## - Features:    1  Add    : lstat                 Perf = 37.629  Diff: 43.119  * ## - Features:    2  Add    : rm                    Perf = 32.289  Diff: 5.3397  * ## - Features:    3  Add    : dis                   Perf = 31.4  Diff: 0.88931  * ## - Features:    4  Add    : indus                 Perf = 30.321  Diff: 1.0794  * ## - Features:    5  Add    : ptratio               Perf = 29.775  Diff: 0.54598  * ## - Features:    6  Add    : b                     Perf = 29.469  Diff: 0.3058  * ## - Features:    7  Add    : age                   Perf = 29.423  Diff: 0.046036  * ##  ## Stopped, because no improving feature was found."},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/nested_resampling.html","id":"example-3-one-task-two-learners-feature-filtering-with-tuning","dir":"Articles > Tutorial","previous_headings":"","what":"Example 3: One task, two learners, feature filtering with tuning","title":"Nested Resampling","text":"minimal example feature filtering tuning feature subset size.","code":"# Feature filtering with tuning in the inner resampling loop lrn = makeFilterWrapper(learner = \"regr.lm\", fw.method = \"FSelectorRcpp_information.gain\") ps = makeParamSet(makeDiscreteParam(\"fw.abs\", values = seq_len(getTaskNFeats(bh.task)))) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"CV\", iter = 2) lrn = makeTuneWrapper(lrn,   resampling = inner, par.set = ps, control = ctrl,   show.info = FALSE)  # Learners lrns = list(\"regr.rpart\", lrn)  # Outer resampling loop outer = makeResampleDesc(\"Subsample\", iter = 3) res = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE) ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  ## Warning in .information_gain.data.frame(x = x, y = y, type = type, equal = ## equal, : Dependent variable is a numeric! It will be converted to factor with ## simple factor(y). We do not discretize dependent variable in FSelectorRcpp by ## default! You can choose equal frequency binning discretization by setting equal ## argument to TRUE.  res ##                 task.id             learner.id mse.test.mean ## 1 BostonHousing-example             regr.rpart      22.16021 ## 2 BostonHousing-example regr.lm.filtered.tuned      27.79428 # Performances on individual outer test data sets getBMRPerformances(res, as.df = TRUE) ##                 task.id             learner.id iter      mse ## 1 BostonHousing-example             regr.rpart    1 23.14016 ## 2 BostonHousing-example             regr.rpart    2 29.91067 ## 3 BostonHousing-example             regr.rpart    3 13.42981 ## 4 BostonHousing-example regr.lm.filtered.tuned    1 29.75924 ## 5 BostonHousing-example regr.lm.filtered.tuned    2 38.05312 ## 6 BostonHousing-example regr.lm.filtered.tuned    3 15.57047"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"sampling-based-approaches","dir":"Articles > Tutorial","previous_headings":"","what":"Sampling-based approaches","title":"Imbalanced Classification Problems","text":"basic idea sampling methods simply adjust proportion classes order increase weight minority class observations within model. sampling-based approaches can divided three different categories: Undersampling methods: Elimination randomly chosen cases majority class decrease effect classifier. cases minority class kept. Oversampling methods: Generation additional cases (copies, artificial observations) minority class increase effect classifier. cases majority class kept. Hybrid methods: Mixture - oversampling strategies. methods directly access underlying data “rearrange” . way sampling done part preprocesssing can therefore combined every appropriate classifier. mlr currently supports first two approaches.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"simple-over--and-undersampling","dir":"Articles > Tutorial","previous_headings":"Sampling-based approaches","what":"(Simple) over- and undersampling","title":"Imbalanced Classification Problems","text":"mentioned undersampling always refers majority class, oversampling affects minority class. use undersampling, randomly chosen observations majority class eliminated. (simple) oversampling observations minority class considered least fitting model. addition, exact copies minority class cases created random sampling repetitions. First, let’s take look effect classification task. Based simulated ClassifTask (Task()) imbalanced classes two new tasks (task., task.) created via mlr functions oversample() undersample(), respectively. Please note undersampling rate 0 1, 1 means undersampling 0.5 implies reduction majority class size 50 percent. Correspondingly, oversampling rate must greater equal 1, 1 means oversampling 2 result doubling minority class size. result performance improve model applied new data. case performance measure considered carefully. misclassification rate (mmce) evaluates overall accuracy predictions, balanced error rate (ber) area ROC Curve (auc) might suitable , misclassifications within class separately taken account.","code":"data.imbal.train = rbind(   data.frame(x = rnorm(100, mean = 1), class = \"A\"),   data.frame(x = rnorm(5000, mean = 2), class = \"B\") ) task = makeClassifTask(data = data.imbal.train, target = \"class\") task.over = oversample(task, rate = 8) task.under = undersample(task, rate = 1/8)  table(getTaskTargets(task)) ##  ##    A    B  ##  100 5000  table(getTaskTargets(task.over)) ##  ##    A    B  ##  800 5000  table(getTaskTargets(task.under)) ##  ##   A   B  ## 100 625 lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") mod = train(lrn, task) mod.over = train(lrn, task.over) mod.under = train(lrn, task.under) data.imbal.test = rbind(   data.frame(x = rnorm(10, mean = 1), class = \"A\"),   data.frame(x = rnorm(500, mean = 2), class = \"B\") )  performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##       mmce        ber        auc  ## 0.01960784 0.50000000 0.50000000  performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##       mmce        ber        auc  ## 0.04705882 0.41600000 0.69080000  performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##      mmce       ber       auc  ## 0.0372549 0.5090000 0.7265000"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"over--and-undersampling-wrappers","dir":"Articles > Tutorial","previous_headings":"Sampling-based approaches","what":"Over- and undersampling wrappers","title":"Imbalanced Classification Problems","text":"Alternatively, mlr also offers integration - undersampling via wrapper approach. way - undersampling can applied already existing learners extend functionality. example given repeated , time extended learners instead modified tasks (see makeOversampleWrapper() makeUndersampleWrapper()). Just like undersampling rate 0 1, oversampling rate lower boundary 1.","code":"lrn.over = makeOversampleWrapper(lrn, osw.rate = 8) lrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8) mod = train(lrn, task) mod.over = train(lrn.over, task) mod.under = train(lrn.under, task)  performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##       mmce        ber        auc  ## 0.01960784 0.50000000 0.50000000  performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##       mmce        ber        auc  ## 0.02941176 0.45600000 0.76880000  performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##       mmce        ber        auc  ## 0.03529412 0.31200000 0.79960000"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"extensions-to-oversampling","dir":"Articles > Tutorial","previous_headings":"Sampling-based approaches","what":"Extensions to oversampling","title":"Imbalanced Classification Problems","text":"Two extensions (simple) oversampling available mlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"smote-synthetic-minority-oversampling-technique","dir":"Articles > Tutorial","previous_headings":"Sampling-based approaches > Extensions to oversampling","what":"1. SMOTE (Synthetic Minority Oversampling Technique)","title":"Imbalanced Classification Problems","text":"duplicating minority class observations can lead overfitting, within SMOTE “new cases” constructed different way. new observation, one randomly chosen minority class observation well one randomly chosen next neighbours interpolated, finally new artificial observation minority class created. smote() function mlr handles numeric well factor features, gower distance used nearest neighbour calculation. factor level new artificial case sampled given levels two input observations. Analogous oversampling, SMOTE preprocessing possible via modification task. Alternatively, new wrapped learner can created via makeSMOTEWrapper(). default number nearest neighbours considered within algorithm set 5.","code":"task.smote = smote(task, rate = 8, nn = 5) table(getTaskTargets(task)) ##  ##    A    B  ##  100 5000  table(getTaskTargets(task.smote)) ##  ##    A    B  ##  800 5000 lrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5) mod.smote = train(lrn.smote, task) performance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc)) ##       mmce        ber        auc  ## 0.03137255 0.50600000 0.70930000"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"overbagging","dir":"Articles > Tutorial","previous_headings":"Sampling-based approaches > Extensions to oversampling","what":"2. Overbagging","title":"Imbalanced Classification Problems","text":"Another extension oversampling consists combination sampling bagging approach. iteration bagging process, minority class observations oversampled given rate obw.rate. majority class cases can either taken account iteration (obw.maxcl = \"\") bootstrapped replacement increase variability training data sets iterations (obw.maxcl = \"boot\"). construction Overbagging Wrapper works similar makeBaggingWrapper(). First existing mlr learner passed makeOverBaggingWrapper(). number iterations fitted models can set via obw.iters. binary classification prediction based majority voting create discrete label. Corresponding probabilities predicted considering proportions predicted labels. Please note benefit sampling process highly dependent specific learner shown following example. First, let’s take look tree learner without overbagging: Now let’s consider random forest initial learner: overbagging slighty improves performance decision tree, AUC decreases second example additional overbagging applied. RF already strong learner (bagged one well), bagging step isn’t helpful usually won’t improve model.","code":"lrn = makeLearner(\"classif.rpart\", predict.type = \"response\") obw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) lrn = setPredictType(lrn, \"prob\") rdesc = makeResampleDesc(\"CV\", iters = 5) r1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,   measures = list(mmce, ber, auc)) r1$aggr ## mmce.test.mean  ber.test.mean  auc.test.mean  ##     0.01960784     0.50000000     0.50000000  obw.lrn = setPredictType(obw.lrn, \"prob\") r2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,   measures = list(mmce, ber, auc)) r2$aggr ## mmce.test.mean  ber.test.mean  auc.test.mean  ##      0.0327451      0.4781435      0.5361176 lrn = makeLearner(\"classif.ranger\") obw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)  lrn = setPredictType(lrn, \"prob\") r1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,   measures = list(mmce, ber, auc)) r1$aggr ## mmce.test.mean  ber.test.mean  auc.test.mean  ##     0.03196078     0.49333337     0.58766687  obw.lrn = setPredictType(obw.lrn, \"prob\") r2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,   measures = list(mmce, ber, auc)) r2$aggr ## mmce.test.mean  ber.test.mean  auc.test.mean  ##     0.04117647     0.49834049     0.49953403"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"tuning-the-probability-threshold","dir":"Articles > Tutorial","previous_headings":"","what":"Tuning the probability threshold","title":"Imbalanced Classification Problems","text":"binary classification, default probability value prediction either classified “1” “0” 0.50. means estimate >= 0.50 observation put class “1” lower values get assigned class “0”. reach better performance binary classification, can helpful also optimize probability threshold split made. can especially helpful response unbalanced. enable , argument tune.threshold needs set TRUE chosen makeTuneControl* function. script tuning (course) nested. happens : tuner evaluates certain learner configuration via (inner) two-fold CV. predictions optimal threshold selected, learner config (calling tuneThreshold() ResamplePrediction object, generated configuration evaluation). optimal learner config, end tuning, also know selected threshold. model trained complete outer training data set threshold set tuning prediction made outer test set.","code":"## Resampling: cross-validation ## Measures:             mmce ## [Resample] iter 1:    0.0541069 ## [Resample] iter 2:    0.0606654 ## [Resample] iter 3:    0.0475880 ##  ## Aggregated Result: mmce.test.mean=0.0541201 ## lrn = makeLearner(\"classif.gbm\", predict.type = \"prob\", distribution = \"bernoulli\") ps = makeParamSet(   makeIntegerParam(\"interaction.depth\", lower = 1, upper = 5) ) ctrl = makeTuneControlRandom(maxit = 2, tune.threshold = TRUE) lrn = makeTuneWrapper(lrn, par.set = ps, control = ctrl, resampling = cv2) r = resample(lrn, spam.task, cv3, extract = getTuneResult) ## Resampling: cross-validation ## Measures:             mmce ## [Tune] Started tuning learner classif.gbm for parameter set: ##                      Type len Def Constr Req Tunable Trafo ## interaction.depth integer   -   - 1 to 5   -    TRUE     - ## With control class: TuneControlRandom ## Imputation value: 1 ## [Tune-x] 1: interaction.depth=5 ## [Tune-y] 1: mmce.test.mean=0.0599739; time: 0.1 min ## [Tune-x] 2: interaction.depth=3 ## [Tune-y] 2: mmce.test.mean=0.0625815; time: 0.0 min ## [Tune] Result: interaction.depth=5 : mmce.test.mean=0.0599739 ## [Resample] iter 1:    0.0482714 ## [Tune] Started tuning learner classif.gbm for parameter set: ##                      Type len Def Constr Req Tunable Trafo ## interaction.depth integer   -   - 1 to 5   -    TRUE     - ## With control class: TuneControlRandom ## Imputation value: 1 ## [Tune-x] 1: interaction.depth=1 ## [Tune-y] 1: mmce.test.mean=0.0652075; time: 0.0 min ## [Tune-x] 2: interaction.depth=3 ## [Tune-y] 2: mmce.test.mean=0.0586882; time: 0.0 min ## [Tune] Result: interaction.depth=3 : mmce.test.mean=0.0586882 ## [Resample] iter 2:    0.0560626 ## [Tune] Started tuning learner classif.gbm for parameter set: ##                      Type len Def Constr Req Tunable Trafo ## interaction.depth integer   -   - 1 to 5   -    TRUE     - ## With control class: TuneControlRandom ## Imputation value: 1 ## [Tune-x] 1: interaction.depth=3 ## [Tune-y] 1: mmce.test.mean=0.0577155; time: 0.0 min ## [Tune-x] 2: interaction.depth=2 ## [Tune-y] 2: mmce.test.mean=0.0606496; time: 0.0 min ## [Tune] Result: interaction.depth=3 : mmce.test.mean=0.0577155 ## [Resample] iter 3:    0.0625815 ##  ## Aggregated Result: mmce.test.mean=0.0556385 ##  print(r$extract) ## [[1]] ## Tune result: ## Op. pars: interaction.depth=5 ## Threshold: 0.48 ## mmce.test.mean=0.0599739 ##  ## [[2]] ## Tune result: ## Op. pars: interaction.depth=3 ## Threshold: 0.48 ## mmce.test.mean=0.0586882 ##  ## [[3]] ## Tune result: ## Op. pars: interaction.depth=3 ## Threshold: 0.52 ## mmce.test.mean=0.0577155"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"cost-based-approaches","dir":"Articles > Tutorial","previous_headings":"","what":"Cost-based approaches","title":"Imbalanced Classification Problems","text":"contrast sampling, cost-based approaches usually require particular learners, can deal different class-dependent costs Cost-Sensitive Classification.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/over_and_undersampling.html","id":"weighted-classes-wrapper","dir":"Articles > Tutorial","previous_headings":"Cost-based approaches","what":"Weighted classes wrapper","title":"Imbalanced Classification Problems","text":"Another approach independent underlying classifier assign costs class weights, observation receives weight, depending class belongs . Similar sampling-based approaches, effect minority class observations thereby increased simply higher weight instances vice versa majority class observations. way every learner supports weights can extended wrapper approach. learner direct parameter class weights, supports observation weights, weights depending class internally set wrapper. binary classification, single number passed classifier corresponds weight positive / majority class, negative / minority class receives weight 1. actually, real costs used within approach, cost ratio taken account. underlying learner already parameter class weighting (e.g., class.weights \"classif.ksvm\"), wcw.weight basically passed specific class weighting parameter.","code":"lrn = makeLearner(\"classif.logreg\") wcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01) lrn = makeLearner(\"classif.ksvm\") wcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/parallelization.html","id":"parallelization-levels","dir":"Articles > Tutorial","previous_headings":"","what":"Parallelization levels","title":"Parallelization","text":"offer different parallelization levels fine grained control parallelization. E.g., want parallelize benchmark() function iterations want parallelize resampling (resample()) learner instead, can specifically pass level \"mlr.resample\" parallelStart* (parallelMap::parallelStart()) function. Currently following levels supported: details please see parallelization() documentation page.","code":"parallelGetRegisteredLevels() ## mlr: mlr.benchmark, mlr.resample, mlr.selectFeatures, mlr.tuneParams, mlr.ensemble"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/parallelization.html","id":"custom-learners-and-parallelization","dir":"Articles > Tutorial","previous_headings":"","what":"Custom learners and parallelization","title":"Parallelization","text":"implemented custom learner , locally, currently need export slave. see error calling, e.g., parallelized version resample() like : simply add following line somewhere calling parallelMap::parallelStart().","code":"no applicable method for 'trainLearner' applied to an object of class <my_new_learner> parallelExport(\"trainLearner.<my_new_learner>\", \"predictLearner.<my_new_learner>\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/parallelization.html","id":"the-end","dir":"Articles > Tutorial","previous_headings":"","what":"The end","title":"Parallelization","text":"details, consult parallelMap tutorial help (?parallelMap()).","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/partial_dependence.html","id":"generating-partial-dependences","dir":"Articles > Tutorial","previous_headings":"","what":"Generating partial dependences","title":"Exploring Learner Predictions","text":"implementation, following mlr’s visualization pattern, consists mentioned function generatePartialDependenceData(), well two visualization functions, plotPartialDependence() plotPartialDependenceGGVIS(). former generates input (objects class PartialDependenceData()) latter. first step executed generatePartialDependenceData() generate feature grid every element character vector features passed. data given input argument, can Task() data.frame. feature grid can generated several ways. uniformly spaced grid length gridsize (default 10) empirical minimum empirical maximum created default, arguments fmin fmax may used override empirical default (lengths fmin fmax must match length features). Alternatively feature data can resampled, either using bootstrap subsampling. noted , \\(X_s\\) unidimensional. , interaction flag must set TRUE. individual feature grids combined using Cartesian product, estimator applied, producing partial dependence every combination unique feature values. interaction flag FALSE (default) default \\(X_s\\) assumed unidimensional, partial dependencies generated feature separately. resulting output interaction = FALSE column feature, NA feature used. step estimation \\(\\hat{f}_{X_s}\\) set predictions length \\(N\\) generated. default mean prediction used. classification predict.type = \"prob\" entails mean class probabilities. However, summaries predictions may used. regression survival tasks function used must either return one number three, , latter, numbers must sorted lowest highest. classification tasks function must return number level target feature. noted, fun argument can function returns three numbers (sorted low high) regression task. allows exploration relative feature importance. feature relatively important, bounds necessarily tighter feature accounts variance predictions, .e., “used” learner. directly setting fun = var identifies regions interaction \\(X_s\\) \\(X_c\\). addition bounds based summary distribution conditional expectation observation, learners can estimate variance predictions can also used. argument bounds numeric vector length two added (first number negative) point prediction produce confidence interval partial dependence. default .025 .975 quantiles Gaussian distribution. previously mentioned aggregation function used, .e., identity, conditional expectation \\(\\hat{f}^{()}_{X_s}\\) estimated. individual = TRUE generatePartialDependenceData() returns \\(n\\) partial dependence estimates made point prediction grid constructed features. resulting output, particularly element data returned object, additional column idx gives index observation row pertains. classification tasks index references class observation index. Partial derivatives can also computed individual partial dependence estimates aggregate partial dependence. restricted single feature time. derivatives individual partial dependence estimates can useful finding regions interaction feature derivative estimated features excluded.","code":"lrn.classif = makeLearner(\"classif.ksvm\", predict.type = \"prob\") fit.classif = train(lrn.classif, iris.task) pd = generatePartialDependenceData(fit.classif, iris.task, \"Petal.Width\") ## Loading required package: mmpf pd ## PartialDependenceData ## Task: iris-example ## Features: Petal.Width ## Target: Petal.Width ## Derivative: FALSE ## Interaction: FALSE ## Individual: FALSE ##     Class Probability Petal.Width ## 1: setosa   0.4958234   0.1000000 ## 2: setosa   0.4558458   0.3666667 ## 3: setosa   0.3909992   0.6333333 ## 4: setosa   0.3175975   0.9000000 ## 5: setosa   0.2296299   1.1666667 ## 6: setosa   0.1552914   1.4333333 ## ... (#rows: 30, #cols: 3) pd.lst = generatePartialDependenceData(fit.classif, iris.task, c(\"Petal.Width\", \"Petal.Length\"), FALSE) head(pd.lst$data) ##     Class Probability Petal.Width Petal.Length ## 1: setosa   0.4958234   0.1000000           NA ## 2: setosa   0.4558458   0.3666667           NA ## 3: setosa   0.3909992   0.6333333           NA ## 4: setosa   0.3175975   0.9000000           NA ## 5: setosa   0.2296299   1.1666667           NA ## 6: setosa   0.1552914   1.4333333           NA  tail(pd.lst$data) ##        Class Probability Petal.Width Petal.Length ## 1: virginica   0.2072951          NA     3.622222 ## 2: virginica   0.3102762          NA     4.277778 ## 3: virginica   0.4262447          NA     4.933333 ## 4: virginica   0.5688485          NA     5.588889 ## 5: virginica   0.6722758          NA     6.244444 ## 6: virginica   0.6735450          NA     6.900000 pd.int = generatePartialDependenceData(fit.classif, iris.task, c(\"Petal.Width\", \"Petal.Length\"), TRUE) pd.int ## PartialDependenceData ## Task: iris-example ## Features: Petal.Width, Petal.Length ## Target: Petal.Width, Petal.Length ## Derivative: FALSE ## Interaction: TRUE ## Individual: FALSE ##     Class Probability Petal.Width Petal.Length ## 1: setosa   0.6322134         0.1     1.000000 ## 2: setosa   0.6322390         0.1     1.655556 ## 3: setosa   0.5933773         0.1     2.311111 ## 4: setosa   0.5055548         0.1     2.966667 ## 5: setosa   0.3857175         0.1     3.622222 ## 6: setosa   0.2995966         0.1     4.277778 ## ... (#rows: 300, #cols: 4) lrn.regr = makeLearner(\"regr.ksvm\") fit.regr = train(lrn.regr, bh.task) pd.regr = generatePartialDependenceData(fit.regr, bh.task, \"lstat\", fun = median) pd.regr ## PartialDependenceData ## Task: BostonHousing-example ## Features: lstat ## Target: lstat ## Derivative: FALSE ## Interaction: FALSE ## Individual: FALSE ##        medv     lstat ## 1: 24.92549  1.730000 ## 2: 23.76082  5.756667 ## 3: 22.37301  9.783333 ## 4: 20.69748 13.810000 ## 5: 19.57175 17.836667 ## 6: 18.95547 21.863333 ## ... (#rows: 10, #cols: 2) pd.ci = generatePartialDependenceData(fit.regr, bh.task, \"lstat\",   fun = function(x) quantile(x, c(.25, .5, .75))) pd.ci ## PartialDependenceData ## Task: BostonHousing-example ## Features: lstat ## Target: lstat ## Derivative: FALSE ## Interaction: FALSE ## Individual: FALSE ##        medv Function     lstat ## 1: 21.34127 medv.25%  1.730000 ## 2: 20.77390 medv.25%  5.756667 ## 3: 19.85575 medv.25%  9.783333 ## 4: 18.70219 medv.25% 13.810000 ## 5: 16.54983 medv.25% 17.836667 ## 6: 14.80144 medv.25% 21.863333 ## ... (#rows: 30, #cols: 3) pd.classif = generatePartialDependenceData(fit.classif, iris.task, \"Petal.Length\", fun = median) pd.classif ## PartialDependenceData ## Task: iris-example ## Features: Petal.Length ## Target: Petal.Length ## Derivative: FALSE ## Interaction: FALSE ## Individual: FALSE ##     Class Probability Petal.Length ## 1: setosa  0.27958460     1.000000 ## 2: setosa  0.25023369     1.655556 ## 3: setosa  0.19970584     2.311111 ## 4: setosa  0.12498617     2.966667 ## 5: setosa  0.06180763     3.622222 ## 6: setosa  0.02892394     4.277778 ## ... (#rows: 30, #cols: 3) fit.se = train(makeLearner(\"regr.randomForest\", predict.type = \"se\"), bh.task) pd.se = generatePartialDependenceData(fit.se, bh.task, c(\"lstat\", \"crim\")) head(pd.se$data) ##       lower     medv    upper     lstat crim ## 1: 12.58683 31.71303 50.83924  1.730000   NA ## 2: 14.26528 26.01166 37.75805  5.756667   NA ## 3: 13.45446 23.56351 33.67256  9.783333   NA ## 4: 14.26956 22.10487 29.94017 13.810000   NA ## 5: 12.90632 20.40292 27.89953 17.836667   NA ## 6: 11.79286 19.75005 27.70724 21.863333   NA  tail(pd.se$data) ##       lower     medv    upper lstat     crim ## 1: 10.64117 21.69222 32.74327    NA 39.54849 ## 2: 10.62463 21.68560 32.74656    NA 49.43403 ## 3: 10.60889 21.67844 32.74800    NA 59.31957 ## 4: 10.51115 21.64946 32.78778    NA 69.20512 ## 5: 10.52775 21.65359 32.77944    NA 79.09066 ## 6: 10.52775 21.65359 32.77944    NA 88.97620 pd.ind.regr = generatePartialDependenceData(fit.regr, bh.task, \"lstat\", individual = TRUE) pd.ind.regr ## PartialDependenceData ## Task: BostonHousing-example ## Features: lstat ## Target: lstat ## Derivative: FALSE ## Interaction: FALSE ## Individual: TRUE ##        medv n     lstat ## 1: 29.48248 1  1.730000 ## 2: 27.70980 1  5.756667 ## 3: 25.43516 1  9.783333 ## 4: 22.99795 1 13.810000 ## 5: 20.73801 1 17.836667 ## 6: 18.92351 1 21.863333 ## ... (#rows: 5060, #cols: 3) pd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, \"Petal.Length\", individual = TRUE) pd.ind.classif ## PartialDependenceData ## Task: iris-example ## Features: Petal.Length ## Target: Petal.Length ## Derivative: FALSE ## Interaction: FALSE ## Individual: TRUE ##     Class Probability n Petal.Length ## 1: setosa  0.30069555 1     1.000000 ## 2: setosa  0.24545844 1     1.655556 ## 3: setosa  0.13791545 1     2.311111 ## 4: setosa  0.04918890 1     2.966667 ## 5: setosa  0.01702062 1     3.622222 ## 6: setosa  0.01018729 1     4.277778 ## ... (#rows: 4500, #cols: 4) pd.regr.der = generatePartialDependenceData(fit.regr, bh.task, \"lstat\", derivative = TRUE) head(pd.regr.der$data) ##          medv     lstat ## 1: -0.2502111  1.730000 ## 2: -0.3561552  5.756667 ## 3: -0.4159729  9.783333 ## 4: -0.4218586 13.810000 ## 5: -0.3767553 17.836667 ## 6: -0.2914229 21.863333 pd.regr.der.ind = generatePartialDependenceData(fit.regr, bh.task, \"lstat\", derivative = TRUE,   individual = TRUE) head(pd.regr.der.ind$data) ##          medv  n     lstat ## 1: -0.1703899 15  1.730000 ## 2: -0.1649074 15  5.756667 ## 3: -0.1677604 15  9.783333 ## 4: -0.1790711 15 13.810000 ## 5: -0.1864755 15 17.836667 ## 6: -0.1706008 15 21.863333 pd.classif.der = generatePartialDependenceData(fit.classif, iris.task, \"Petal.Width\", derivative = TRUE) head(pd.classif.der$data) ##     Class Probability Petal.Width ## 1: setosa -0.08628906   0.1000000 ## 2: setosa -0.20992196   0.3666667 ## 3: setosa -0.26049715   0.6333333 ## 4: setosa -0.30221163   0.9000000 ## 5: setosa -0.33394805   1.1666667 ## 6: setosa -0.20736953   1.4333333 pd.classif.der.ind = generatePartialDependenceData(fit.classif, iris.task, \"Petal.Width\", derivative = TRUE,   individual = TRUE) head(pd.classif.der.ind$data) ##     Class  Probability  n Petal.Width ## 1: setosa -0.448458780 94   0.1000000 ## 2: setosa -0.695200956 94   0.3666667 ## 3: setosa -0.306311779 94   0.6333333 ## 4: setosa -0.069066116 94   0.9000000 ## 5: setosa -0.004061345 94   1.1666667 ## 6: setosa  0.038224205 94   1.4333333"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/partial_dependence.html","id":"plotting-partial-dependences","dir":"Articles > Tutorial","previous_headings":"","what":"Plotting partial dependences","title":"Exploring Learner Predictions","text":"Results generatePartialDependenceData() generateFunctionalANOVAData() can visualized plotPartialDependence() plotPartialDependenceGGVIS(). one feature regression task output line plot, point point corresponding feature’s grid.  classification task, line drawn class, gives estimated partial probability class particular point feature grid.  regression tasks, fun argument generatePartialDependenceData() used, bounds automatically displayed using gray ribbon.  goes plots partial dependences learner predict.type = \"se\".  multiple features passed generatePartialDependenceData() interaction = FALSE, facetting used display estimated bivariate relationship.  interaction = TRUE call generatePartialDependenceData(), one variable must chosen used facetting, subplot value chosen feature’s grid created, wherein feature’s partial dependences within facetting feature’s value shown. Note type plot limited two features.  plotPartialDependenceGGVIS() can used similarly, however, since ggvis currently lacks subplotting/facetting capabilities, argument interact maps one feature interactive sidebar user can select value one feature. individual = TRUE individual conditional expectation curve plotted.  Plotting partial derivative functions works partial dependence. estimates derivative mean aggregated partial dependence function, individual partial dependence functions regression classification task respectively.","code":"plotPartialDependence(pd.regr) plotPartialDependence(pd.classif) plotPartialDependence(pd.ci) plotPartialDependence(pd.se) plotPartialDependence(pd.lst) plotPartialDependence(pd.int, facet = \"Petal.Length\") plotPartialDependenceGGVIS(pd.int, interact = \"Petal.Length\") plotPartialDependence(pd.ind.regr) plotPartialDependence(pd.regr.der)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"available-performance-measures","dir":"Articles > Tutorial","previous_headings":"","what":"Available performance measures","title":"Evaluating Learner Performance","text":"mlr provides large number performance measures types learning problems. Typical performance measures classification mean misclassification error (mmce), accuracy (acc) measures based ROC analysis. regression mean squared errors (mse) mean absolute errors (mae) usually considered. clustering tasks, measures Dunn index (G1 index) provided, survival predictions, Concordance Index (cindex) supported, cost-sensitive predictions misclassification penalty (mcp) others. also possible access time train learner (timetrain), time compute prediction (timepredict) sum (timeboth) performance measures. see performance measures implemented, look table performance measures measures() documentation page. want implement additional measure include measure non-standard misclassification costs, see section creating custom measures.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"listing-measures","dir":"Articles > Tutorial","previous_headings":"","what":"Listing measures","title":"Evaluating Learner Performance","text":"properties requirements individual measures shown table performance measures. like list available measures certain properties suitable certain learning Task() use function listMeasures(). convenience exists default measure type learning problem, calculated nothing else specified. defaults chose commonly used measures respective types, e.g., mean squared error (mse) regression misclassification rate (mmce) classification. help page function getDefaultMeasure() lists defaults types learning problems. function returns default measure given task type, Task() Learner().","code":"# Performance measures for classification with multiple classes listMeasures(\"classif\", properties = \"classif.multi\") ##  [1] \"featperc\"         \"mmce\"             \"lsr\"              \"bac\"              ##  [5] \"qsr\"              \"timeboth\"         \"multiclass.aunp\"  \"timetrain\"        ##  [9] \"multiclass.aunu\"  \"ber\"              \"timepredict\"      \"multiclass.brier\" ## [13] \"ssr\"              \"acc\"              \"logloss\"          \"wkappa\"           ## [17] \"multiclass.au1p\"  \"multiclass.au1u\"  \"kappa\" # Performance measure suitable for the iris classification task listMeasures(iris.task) ##  [1] \"featperc\"         \"mmce\"             \"lsr\"              \"bac\"              ##  [5] \"qsr\"              \"timeboth\"         \"multiclass.aunp\"  \"timetrain\"        ##  [9] \"multiclass.aunu\"  \"ber\"              \"timepredict\"      \"multiclass.brier\" ## [13] \"ssr\"              \"acc\"              \"logloss\"          \"wkappa\"           ## [17] \"multiclass.au1p\"  \"multiclass.au1u\"  \"kappa\" # Get default measure for iris.task getDefaultMeasure(iris.task) ## Name: Mean misclassification error ## Performance measure: mmce ## Properties: classif,classif.multi,req.pred,req.truth ## Minimize: TRUE ## Best: 0; Worst: 1 ## Aggregated by: test.mean ## Arguments:  ## Note: Defined as: mean(response != truth)  # Get the default measure for linear regression getDefaultMeasure(makeLearner(\"regr.lm\")) ## Name: Mean of squared errors ## Performance measure: mse ## Properties: regr,req.pred,req.truth ## Minimize: TRUE ## Best: 0; Worst: Inf ## Aggregated by: test.mean ## Arguments:  ## Note: Defined as: mean((response - truth)^2)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"calculate-performance-measures","dir":"Articles > Tutorial","previous_headings":"","what":"Calculate performance measures","title":"Evaluating Learner Performance","text":"following example fit gradient boosting machine (gbm::gbm()) subset BostonHousing (mlbench::BostonHousing()) data set calculate default measure mean squared error (mse) remaining observations. following code computes median squared errors (medse) instead. course, can also calculate multiple performance measures simply passing list measures can also include measure. Calculate mean squared error, median squared error mean absolute error (mae). types learning problems measures, calculating performance basically works way.","code":"n = getTaskSize(bh.task) lrn = makeLearner(\"regr.gbm\", n.trees = 1000) mod = train(lrn, task = bh.task, subset = seq(1, n, 2)) pred = predict(mod, task = bh.task, subset = seq(2, n, 2))  performance(pred) ##      mse  ## 14.38596 performance(pred, measures = medse) ##    medse  ## 4.209155 performance(pred, measures = list(mse, medse, mae)) ##       mse     medse       mae  ## 14.385956  4.209155  2.716451"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"requirements-of-performance-measures","dir":"Articles > Tutorial","previous_headings":"","what":"Requirements of performance measures","title":"Evaluating Learner Performance","text":"Note order calculate performance measures required pass Task() fitted model (makeWrappedModel()) addition Prediction(). example order assess time needed training (timetrain), fitted model passed. many performance measures cluster analysis Task() required. Moreover, measures require certain type prediction. example binary classification order calculate AUC (auc) – area ROC (receiver operating characteristic) curve – make sure posterior probabilities predicted. information ROC analysis, see section ROC analysis. Also bear mind many performance measures available classification, e.g., false positive rate (fpr), suitable binary problems.","code":"performance(pred, measures = timetrain, model = mod) ## timetrain ##     0.055 lrn = makeLearner(\"cluster.kmeans\", centers = 3) mod = train(lrn, mtcars.task) pred = predict(mod, task = mtcars.task)  # Calculate the G1 index performance(pred, measures = G1, task = mtcars.task) ##       G1  ## 61.17497 lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") mod = train(lrn, task = sonar.task) pred = predict(mod, task = sonar.task)  performance(pred, measures = auc) ##       auc  ## 0.9224018"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"access-a-performance-measure","dir":"Articles > Tutorial","previous_headings":"","what":"Access a performance measure","title":"Evaluating Learner Performance","text":"Performance measures mlr objects class Measure (makeMeasure()). interested properties requirements single measure can access directly. See help page Measure (makeMeasure()) information individual slots.","code":"# Mean misclassification error str(mmce) ## List of 10 ##  $ id        : chr \"mmce\" ##  $ minimize  : logi TRUE ##  $ properties: chr [1:4] \"classif\" \"classif.multi\" \"req.pred\" \"req.truth\" ##  $ fun       :function (task, model, pred, feats, extra.args)   ##  $ extra.args: list() ##  $ best      : num 0 ##  $ worst     : num 1 ##  $ name      : chr \"Mean misclassification error\" ##  $ note      : chr \"Defined as: mean(response != truth)\" ##  $ aggr      :List of 4 ##   ..$ id        : chr \"test.mean\" ##   ..$ name      : chr \"Test mean\" ##   ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)   ##   ..$ properties: chr \"req.test\" ##   ..- attr(*, \"class\")= chr \"Aggregation\" ##  - attr(*, \"class\")= chr \"Measure\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"binary-classification","dir":"Articles > Tutorial","previous_headings":"","what":"Binary classification","title":"Evaluating Learner Performance","text":"binary classification specialized techniques exist analyze performance.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"plot-performance-versus-threshold","dir":"Articles > Tutorial","previous_headings":"","what":"Plot performance versus threshold","title":"Evaluating Learner Performance","text":"may recall (see previous section making predictions) binary classification can adjust threshold used map probabilities class labels. Helpful regard functions generateThreshVsPerfData() plotThreshVsPerf(), generate plot, respectively, learner performance versus threshold. performance plots automatic threshold tuning see section ROC analysis. following example consider mlbench::Sonar() data set plot false positive rate (fpr), false negative rate (fnr) well misclassification rate (mmce) possible threshold values.  experimental ggvis plotting function plotThreshVsPerfGGVIS() performs similarly plotThreshVsPerf() instead creating facetted subplots visualize multiple learners /multiple measures, one mapped interactive sidebar selects display.","code":"lrn = makeLearner(\"classif.lda\", predict.type = \"prob\") n = getTaskSize(sonar.task) mod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2)) pred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))  # Performance for the default threshold 0.5 performance(pred, measures = list(fpr, fnr, mmce)) ##       fpr       fnr      mmce  ## 0.2500000 0.3035714 0.2788462 # Plot false negative and positive rates as well as the error rate versus the threshold d = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce)) plotThreshVsPerf(d) plotThreshVsPerfGGVIS(d)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/performance.html","id":"roc-measures","dir":"Articles > Tutorial","previous_headings":"","what":"ROC measures","title":"Evaluating Learner Performance","text":"binary classification large number specialized measures exist, can nicely formatted one matrix, see example receiver operating characteristic page wikipedia. can generate similiar table calculateROCMeasures() function. top left \\(2 \\times 2\\) matrix confusion matrix, shows relative frequency correctly incorrectly classified observations. right large number performance measures can inferred confusion matrix added. default additional info measures printed. can turn using abbreviations argument print (calculateROCMeasures()) method: print(r, abbreviations = FALSE).","code":"r = calculateROCMeasures(pred) r ##     predicted ## true M         R                             ##    M 39        17        tpr: 0.7  fnr: 0.3  ##    R 12        36        fpr: 0.25 tnr: 0.75 ##      ppv: 0.76 for: 0.32 lrp: 2.79 acc: 0.72 ##      fdr: 0.24 npv: 0.68 lrm: 0.4  dor: 6.88 ##  ##  ## Abbreviations: ## tpr - True positive rate (Sensitivity, Recall) ## fpr - False positive rate (Fall-out) ## fnr - False negative rate (Miss rate) ## tnr - True negative rate (Specificity) ## ppv - Positive predictive value (Precision) ## for - False omission rate ## lrp - Positive likelihood ratio (LR+) ## fdr - False discovery rate ## npv - Negative predictive value ## acc - Accuracy ## lrm - Negative likelihood ratio (LR-) ## dor - Diagnostic odds ratio"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/predict.html","id":"accessing-the-prediction","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing the prediction","title":"Predicting Outcomes for New Data","text":"Function predict() returns named list class Prediction(). important element $data data.frame contains columns true values target variable (case supervised learning problems) predictions. Use .data.frame (Prediction()) direct access. following predictions BostonHousing (mlbench::BostonHousing()) iris (datasets::iris()) data sets shown. may recall, predictions first case made Task() second case data.frame. can see predicting Task(), resulting data.frame contains additional column, called id, tells us element original data set prediction corresponds . direct way access true predicted values target variable(s) provided functions getPredictionTruth (getPredictionResponse()) [getPredictionResponse()].","code":"### Result of predict with data passed via task argument head(as.data.frame(task.pred)) ##    id truth response ## 2   2  21.6 22.52737 ## 4   4  33.4 36.06190 ## 6   6  28.7 24.75354 ## 8   8  27.1 16.90299 ## 10 10  18.9 17.25558 ## 12 12  18.9 20.54365  ### Result of predict with data passed via newdata argument head(as.data.frame(newdata.pred)) ##    response ## 2         1 ## 4         1 ## 6         1 ## 8         1 ## 10        1 ## 12        1 head(getPredictionTruth(task.pred)) ## [1] 21.6 33.4 28.7 27.1 18.9 18.9  head(getPredictionResponse(task.pred)) ## [1] 22.52737 36.06190 24.75354 16.90299 17.25558 20.54365"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/predict.html","id":"regression-extracting-standard-errors","dir":"Articles > Tutorial","previous_headings":"","what":"Regression: Extracting standard errors","title":"Predicting Outcomes for New Data","text":"learners provide standard errors predictions, can accessed mlr. overview given calling function listLearners() setting properties = \"se\". assigning FALSE check.packages learners packages installed included overview. example train linear regression model (stats::lm()) BostonHousing (bh.task()) dataset. order calculate standard errors set predict.type \"se\": standard errors can extracted using getPredictionSE().","code":"listLearners(\"regr\", check.packages = FALSE, properties = \"se\")[c(\"class\", \"name\")] ##                class ## 1         regr.bcart ## 2           regr.bgp ## 3        regr.bgpllm ## 4           regr.blm ## 5          regr.btgp ## 6       regr.btgpllm ## 7          regr.btlm ## 8           regr.crs ## 9       regr.gausspr ## 10          regr.glm ## 11        regr.GPfit ## 12           regr.km ## 13         regr.laGP ## 14           regr.lm ## 15 regr.randomForest ## 16       regr.ranger ##                                                                       name ## 1                                                            Bayesian CART ## 2                                                Bayesian Gaussian Process ## 3        Bayesian Gaussian Process with jumps to the Limiting Linear Model ## 4                                                    Bayesian Linear Model ## 5                                          Bayesian Treed Gaussian Process ## 6  Bayesian Treed Gaussian Process with jumps to the Limiting Linear Model ## 7                                              Bayesian Treed Linear Model ## 8                                                       Regression Splines ## 9                                                       Gaussian Processes ## 10                                           Generalized Linear Regression ## 11                                                        Gaussian Process ## 12                                                                 Kriging ## 13                                      Local Approximate Gaussian Process ## 14                                                Simple Linear Regression ## 15                                                           Random Forest ## 16                                                          Random Forests ### Create learner and specify predict.type lrn.lm = makeLearner(\"regr.lm\", predict.type = 'se') mod.lm = train(lrn.lm, bh.task, subset = train.set) task.pred.lm = predict(mod.lm, task = bh.task, subset = test.set) task.pred.lm  ## Prediction: 253 observations ## predict.type: se ## threshold:  ## time: 0.00 ##    id truth response        se ## 2   2  21.6 24.83734 0.7501615 ## 4   4  33.4 28.38206 0.8742590 ## 6   6  28.7 25.16725 0.8652139 ## 8   8  27.1 19.38145 1.1963265 ## 10 10  18.9 18.66449 1.1793944 ## 12 12  18.9 21.25802 1.0727918 ## ... (#rows: 253, #cols: 4) head(getPredictionSE(task.pred.lm)) ## [1] 0.7501615 0.8742590 0.8652139 1.1963265 1.1793944 1.0727918"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/predict.html","id":"classification-and-clustering-extracting-probabilities","dir":"Articles > Tutorial","previous_headings":"","what":"Classification and clustering: Extracting probabilities","title":"Predicting Outcomes for New Data","text":"predicted probabilities can extracted Prediction() using function getPredictionProbabilities(). another cluster analysis example. use fuzzy c-means clustering (e1071::cmeans()) mtcars (datasets::mtcars()) data set. classification problems things worth mentioning. default, class labels predicted. order get predicted posterior probabilities create Learner (makeLearner()) appropriate predict.type. addition probabilities, class labels predicted choosing class maximum probability breaking ties random. mentioned , predicted posterior probabilities can accessed via getPredictionProbabilities() function.","code":"lrn = makeLearner(\"cluster.cmeans\", predict.type = \"prob\") mod = train(lrn, mtcars.task)  pred = predict(mod, task = mtcars.task) head(getPredictionProbabilities(pred)) ##                             1          2 ## Mazda RX4         0.020400964 0.97959904 ## Mazda RX4 Wag     0.020360747 0.97963925 ## Datsun 710        0.007341207 0.99265879 ## Hornet 4 Drive    0.457052250 0.54294775 ## Hornet Sportabout 0.981291168 0.01870883 ## Valiant           0.242514386 0.75748561 ### Linear discriminant analysis on the iris data set mod = train(\"classif.lda\", task = iris.task)  pred = predict(mod, task = iris.task) pred ## Prediction: 150 observations ## predict.type: response ## threshold:  ## time: 0.00 ##   id  truth response ## 1  1 setosa   setosa ## 2  2 setosa   setosa ## 3  3 setosa   setosa ## 4  4 setosa   setosa ## 5  5 setosa   setosa ## 6  6 setosa   setosa ## ... (#rows: 150, #cols: 3) lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") mod = train(lrn, iris.task)  pred = predict(mod, newdata = iris) head(as.data.frame(pred)) ##    truth prob.setosa prob.versicolor prob.virginica response ## 1 setosa           1               0              0   setosa ## 2 setosa           1               0              0   setosa ## 3 setosa           1               0              0   setosa ## 4 setosa           1               0              0   setosa ## 5 setosa           1               0              0   setosa ## 6 setosa           1               0              0   setosa head(getPredictionProbabilities(pred)) ##   setosa versicolor virginica ## 1      1          0         0 ## 2      1          0         0 ## 3      1          0         0 ## 4      1          0         0 ## 5      1          0         0 ## 6      1          0         0"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/predict.html","id":"classification-confusion-matrix","dir":"Articles > Tutorial","previous_headings":"","what":"Classification: Confusion matrix","title":"Predicting Outcomes for New Data","text":"confusion matrix can obtained calling calculateConfusionMatrix(). columns represent predicted rows true class labels. can see number correctly classified observations diagonal matrix. Misclassified observations -diagonal. total number errors single (true predicted) classes shown -err.- row column, respectively. get relative frequencies additional absolute numbers can set relative = TRUE. possible normalize either row column, therefore every element relative confusion matrix contains two values. first relative frequency grouped row (true label) second value grouped column (predicted label). want access relative values directly can $relative.row $relative.col members returned object conf.matrix. details see ConfusionMatrix() documentation page. Finally, can also add absolute number observations predicted true class label matrix (absolute relative) setting sums = TRUE.","code":"calculateConfusionMatrix(pred) ##             predicted ## true         setosa versicolor virginica -err.- ##   setosa         50          0         0      0 ##   versicolor      0         49         1      1 ##   virginica       0          5        45      5 ##   -err.-          0          5         1      6 conf.matrix = calculateConfusionMatrix(pred, relative = TRUE) conf.matrix ## Relative confusion matrix (normalized by row/column): ##             predicted ## true         setosa    versicolor virginica -err.-    ##   setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00      ##   versicolor 0.00/0.00 0.98/0.91  0.02/0.02 0.02      ##   virginica  0.00/0.00 0.10/0.09  0.90/0.98 0.10      ##   -err.-          0.00      0.09       0.02 0.04      ##  ##  ## Absolute confusion matrix: ##             predicted ## true         setosa versicolor virginica -err.- ##   setosa         50          0         0      0 ##   versicolor      0         49         1      1 ##   virginica       0          5        45      5 ##   -err.-          0          5         1      6 conf.matrix$relative.row ##            setosa versicolor virginica -err- ## setosa          1       0.00      0.00  0.00 ## versicolor      0       0.98      0.02  0.02 ## virginica       0       0.10      0.90  0.10 calculateConfusionMatrix(pred, relative = TRUE, sums = TRUE) ## Relative confusion matrix (normalized by row/column): ##             predicted ## true         setosa    versicolor virginica -err.-    -n-  ##   setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00      50   ##   versicolor 0.00/0.00 0.98/0.91  0.02/0.02 0.02      54   ##   virginica  0.00/0.00 0.10/0.09  0.90/0.98 0.10      46   ##   -err.-          0.00      0.09       0.02 0.04      <NA> ##   -n-        50        50         50        <NA>      150  ##  ##  ## Absolute confusion matrix: ##            setosa versicolor virginica -err.- -n- ## setosa         50          0         0      0  50 ## versicolor      0         49         1      1  50 ## virginica       0          5        45      5  50 ## -err.-          0          5         1      6  NA ## -n-            50         54        46     NA 150"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/predict.html","id":"classification-adjusting-the-decision-threshold","dir":"Articles > Tutorial","previous_headings":"","what":"Classification: Adjusting the decision threshold","title":"Predicting Outcomes for New Data","text":"can set threshold value used map predicted posterior probabilities class labels. Note purpose need create Learner (makeLearner()) predicts probabilities. binary classification, threshold determines positive class predicted. default 0.5. Now, set threshold positive class 0.9 (, example assigned positive class posterior probability exceeds 0.9). two classes positive one can seen accessing Task(). illustrate binary classification, use Sonar (mlbench::Sonar()) data set mlbench package. Note binary case getPredictionProbabilities() default extracts posterior probabilities positive class . works similarly multiclass classification. threshold given named vector specifying values probability divided. class maximum resulting value selected. interested tuning threshold (vector) look section performance curves threshold tuning.","code":"lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") mod = train(lrn, task = sonar.task)  ### Label of the positive class getTaskDesc(sonar.task)$positive ## [1] \"M\"  ### Default threshold pred1 = predict(mod, sonar.task) pred1$threshold ##   M   R  ## 0.5 0.5  ### Set the threshold value for the positive class pred2 = setThreshold(pred1, 0.9) pred2$threshold ##   M   R  ## 0.9 0.1  pred2 ## Prediction: 208 observations ## predict.type: prob ## threshold: M=0.90,R=0.10 ## time: 0.00 ##   id truth    prob.M    prob.R response ## 1  1     R 0.1060606 0.8939394        R ## 2  2     R 0.7333333 0.2666667        R ## 3  3     R 0.0000000 1.0000000        R ## 4  4     R 0.1060606 0.8939394        R ## 5  5     R 0.9250000 0.0750000        M ## 6  6     R 0.0000000 1.0000000        R ## ... (#rows: 208, #cols: 5)  ### We can also set the effect in the confusion matrix calculateConfusionMatrix(pred1) ##         predicted ## true      M  R -err.- ##   M      95 16     16 ##   R      10 87     10 ##   -err.- 10 16     26  calculateConfusionMatrix(pred2) ##         predicted ## true      M  R -err.- ##   M      84 27     27 ##   R       6 91      6 ##   -err.-  6 27     33 head(getPredictionProbabilities(pred1)) ## [1] 0.1060606 0.7333333 0.0000000 0.1060606 0.9250000 0.0000000  ### But we can change that, too head(getPredictionProbabilities(pred1, cl = c(\"M\", \"R\"))) ##           M         R ## 1 0.1060606 0.8939394 ## 2 0.7333333 0.2666667 ## 3 0.0000000 1.0000000 ## 4 0.1060606 0.8939394 ## 5 0.9250000 0.0750000 ## 6 0.0000000 1.0000000 lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") mod = train(lrn, iris.task) pred = predict(mod, newdata = iris) pred$threshold ##     setosa versicolor  virginica  ##  0.3333333  0.3333333  0.3333333 table(as.data.frame(pred)$response) ##  ##     setosa versicolor  virginica  ##         50         54         46 pred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1)) pred$threshold ##     setosa versicolor  virginica  ##       0.01      50.00       1.00 table(as.data.frame(pred)$response) ##  ##     setosa versicolor  virginica  ##         50          0        100"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/predict.html","id":"visualizing-the-prediction","dir":"Articles > Tutorial","previous_headings":"","what":"Visualizing the prediction","title":"Predicting Outcomes for New Data","text":"function plotLearnerPrediction() allows visualize predictions, e.g., teaching purposes exploring models. trains chosen learning method 1 2 selected features displays predictions ggplot2::ggplot(). classification, get scatter plot 2 features (default first 2 data set). type symbol shows true class labels data points. Symbols white border indicate misclassified observations. posterior probabilities (learner consideration supports ) represented background color higher saturation means larger probabilities. plot title displays ID Learner (makeLearner()) (following example CART), parameters, training performance cross-validation performance. mmce stands mean misclassification error, .e., error rate. See sections performance resampling explanations.  clustering also get scatter plot two selected features. color points indicates predicted cluster.  regression, two types plots. 1D plot shows target values relation single feature, regression curve , chosen learner supports , estimated standard error.  2D variant, classification case, generates scatter plot 2 features. fill color dots illustrates value target variable \"medv\", background colors show estimated mean. plot represent estimated standard error.","code":"lrn = makeLearner(\"classif.rpart\", id = \"CART\") plotLearnerPrediction(lrn, task = iris.task) lrn = makeLearner(\"cluster.kmeans\") plotLearnerPrediction(lrn, task = mtcars.task, features = c(\"disp\", \"drat\"), cv = 0) plotLearnerPrediction(\"regr.lm\", features = \"lstat\", task = bh.task) plotLearnerPrediction(\"regr.lm\", features = c(\"lstat\", \"rm\"), task = bh.task)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"fusing-learners-with-preprocessing","dir":"Articles > Tutorial","previous_headings":"","what":"Fusing learners with preprocessing","title":"Data Preprocessing","text":"mlr’s wrapper functionality permits combine learners preprocessing steps. means preprocessing “belongs” learner done time learner trained predictions made. , one hand, practical. don’t need change data learning Task()s ’s quite easy combine different learners different preprocessing steps. hand helps avoid common mistake evaluating performance learner preprocessing: Preprocessing often seen completely independent later applied learning algorithms. estimating performance learner, e.g., cross-validation preprocessing done beforehand full data set training/predicting learner done train/test sets. Depending exactly done preprocessing can lead overoptimistic results. example imputation mean done whole data set evaluating learner performance using information test data training, can cause overoptimistic performance results. clarify things one distinguish data-dependent data-independent preprocessing steps: Data-dependent steps way learn data give different results applied different data sets. Data-independent steps always lead results. Clearly, correcting errors data removing data columns like Ids used learning, data-independent. Imputation missing values mean, mentioned , data-dependent. Imputation fixed constant, however, . get honest estimate learner performance combined preprocessing, data-dependent preprocessing steps must included resampling. automatically done fusing learner preprocessing. end mlr provides two wrappers: makePreprocWrapperCaret() interface preprocessing options offered caret’s caret::preProcess() function. makePreprocWrapper() permits write custom preprocessing methods defining actions taken training prediction. mentioned specified preprocessing steps “belong” wrapped Learner (makeLearner()). contrast preprocessing options listed like normalizeFeatures() Task() remains unchanged, preprocessing done globally, .e., whole data set, every pair training/test data sets , e.g., resampling, parameters controlling preprocessing , e.g., percentage outliers removed can tuned together base learner parameters. start examples makePreprocWrapperCaret().","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"preprocessing-with-makepreprocwrappercaret","dir":"Articles > Tutorial","previous_headings":"","what":"Preprocessing with makePreprocWrapperCaret","title":"Data Preprocessing","text":"makePreprocWrapperCaret() interface caret’s caret::preProcess() function provides many different options like imputation missing values, data transformations scaling features certain range Box-Cox dimensionality reduction via Independent Principal Component Analysis. possible options see help page function caret::preProcess(). Note usage makePreprocWrapperCaret() slightly different caret::preProcess(). makePreprocWrapperCaret() takes (almost) formal arguments caret::preProcess(), names prefixed ppc.. exception: makePreprocWrapperCaret() method argument. Instead preprocessing options passed caret::preProcess()’s method argument given individual logical parameters makePreprocWrapperCaret(). example following call caret::preProcess() x matrix data.frame thus translate learner mlr Learner (makeLearner()) name learner class like \"classif.lda\". enable multiple preprocessing options (like knn imputation principal component analysis ) executed certain order detailed help page function caret::preProcess(). following show example principal components analysis (PCA) used dimensionality reduction. never applied blindly, can beneficial learners get problems high dimensionality can profit rotating data. consider sonar.task(), poses binary classification problem 208 observations 60 features. fuse quadratic discriminant analysis (MASS::qda()) package MASS principal components preprocessing step. threshold set 0.9, .e., principal components necessary explain cumulative percentage 90% total variance kept. data automatically standardized prior PCA. wrapped learner trained sonar.task(). inspecting underlying MASS::qda() model, see first 22 principal components used training. performances MASS::qda() without PCA preprocessing compared benchmark experiment. Note use stratified resampling prevent errors MASS::qda() due small number observations either class. PCA preprocessing case turns really beneficial performance Quadratic Discriminant Analysis.","code":"preProcess(x, method = c(\"knnImpute\", \"pca\"), pcaComp = 10) makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10) sonar.task ## Supervised task: Sonar-example ## Type: classif ## Target: Class ## Observations: 208 ## Features: ##    numerics     factors     ordered functionals  ##          60           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ##   M   R  ## 111  97  ## Positive class: M lrn = makePreprocWrapperCaret(\"classif.qda\", ppc.pca = TRUE, ppc.thresh = 0.9) lrn ## Learner classif.qda.preproc from package MASS ## Type: classif ## Name: ; Short name:  ## Class: PreprocWrapperCaret ## Properties: twoclass,multiclass,numerics,factors,prob ## Predict-Type: response ## Hyperparameters: ppc.BoxCox=FALSE,ppc.YeoJohnson=FALSE,ppc.expoTrans=FALSE,ppc.center=TRUE,ppc.scale=TRUE,ppc.range=FALSE,ppc.knnImpute=FALSE,ppc.bagImpute=FALSE,ppc.medianImpute=FALSE,ppc.pca=TRUE,ppc.ica=FALSE,ppc.spatialSign=FALSE,ppc.corr=FALSE,ppc.zv=FALSE,ppc.nzv=FALSE,ppc.thresh=0.9,ppc.na.remove=TRUE,ppc.k=5,ppc.fudge=0.2,ppc.numUnique=3,ppc.cutoff=0.9,ppc.freqCut=19,ppc.uniqueCut=10 mod = train(lrn, sonar.task) mod ## Model for learner.id=classif.qda.preproc; learner.class=PreprocWrapperCaret ## Trained on: task.id = Sonar-example; obs = 208; features = 60 ## Hyperparameters: ppc.BoxCox=FALSE,ppc.YeoJohnson=FALSE,ppc.expoTrans=FALSE,ppc.center=TRUE,ppc.scale=TRUE,ppc.range=FALSE,ppc.knnImpute=FALSE,ppc.bagImpute=FALSE,ppc.medianImpute=FALSE,ppc.pca=TRUE,ppc.ica=FALSE,ppc.spatialSign=FALSE,ppc.corr=FALSE,ppc.zv=FALSE,ppc.nzv=FALSE,ppc.thresh=0.9,ppc.na.remove=TRUE,ppc.k=5,ppc.fudge=0.2,ppc.numUnique=3,ppc.cutoff=0.9,ppc.freqCut=19,ppc.uniqueCut=10  getLearnerModel(mod) ## Model for learner.id=classif.qda; learner.class=classif.qda ## Trained on: task.id = Sonar-example; obs = 208; features = 22 ## Hyperparameters:  getLearnerModel(mod, more.unwrap = TRUE) ## Call: ## qda(f, data = getTaskData(.task, .subset, recode.target = \"drop.levels\")) ##  ## Prior probabilities of groups: ##         M         R  ## 0.5336538 0.4663462  ##  ## Group means: ##          PC1        PC2        PC3         PC4         PC5         PC6 ## M  0.5976122 -0.8058235  0.9773518  0.03794232 -0.04568166 -0.06721702 ## R -0.6838655  0.9221279 -1.1184128 -0.04341853  0.05227489  0.07691845 ##          PC7         PC8        PC9       PC10        PC11          PC12 ## M  0.2278162 -0.01034406 -0.2530606 -0.1793157 -0.04084466 -0.0004789888 ## R -0.2606969  0.01183702  0.2895848  0.2051963  0.04673977  0.0005481212 ##          PC13       PC14        PC15        PC16        PC17        PC18 ## M -0.06138758 -0.1057137  0.02808048  0.05215865 -0.07453265  0.03869042 ## R  0.07024765  0.1209713 -0.03213333 -0.05968671  0.08528994 -0.04427460 ##          PC19         PC20        PC21         PC22 ## M -0.01192247  0.006098658  0.01263492 -0.001224809 ## R  0.01364323 -0.006978877 -0.01445851  0.001401586 rin = makeResampleInstance(\"CV\", iters = 3, stratify = TRUE, task = sonar.task) res = benchmark(list(\"classif.qda\", lrn), sonar.task, rin, show.info = FALSE) res ##         task.id          learner.id mmce.test.mean ## 1 Sonar-example         classif.qda      0.2932367 ## 2 Sonar-example classif.qda.preproc      0.1779848"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"joint-tuning-of-preprocessing-options-and-learner-parameters","dir":"Articles > Tutorial","previous_headings":"","what":"Joint tuning of preprocessing options and learner parameters","title":"Data Preprocessing","text":"Let’s see can optimize bit. threshold value 0.9 chosen arbitrarily led 22 60 principal components. maybe lower higher number principal components used. Moreover, qda (MASS::qda()) several options control class covariance matrices class probabilities estimated. preprocessing learner parameters can tuned jointly. let’s first get overview parameters wrapped learner using function getParamSet(). parameters prefixed ppc. belong preprocessing. method, nu predict.method MASS::qda() parameters. Instead tuning PCA threshold (ppc.thresh) tune number principal components (ppc.pcaComp) directly. Moreover, MASS::qda() try two different ways estimate posterior probabilities (parameter predict.method): usual plug-estimates unbiased estimates. perform grid search set resolution 10. demonstration. might want use finer resolution. seems preference lower number principal components (<27) \"plug-\" \"debiased\" \"plug-\" achieving slightly lower error rates.","code":"getParamSet(lrn) ##                      Type len     Def                      Constr Req Tunable ## ppc.BoxCox        logical   -   FALSE                           -   -    TRUE ## ppc.YeoJohnson    logical   -   FALSE                           -   -    TRUE ## ppc.expoTrans     logical   -   FALSE                           -   -    TRUE ## ppc.center        logical   -    TRUE                           -   -    TRUE ## ppc.scale         logical   -    TRUE                           -   -    TRUE ## ppc.range         logical   -   FALSE                           -   -    TRUE ## ppc.knnImpute     logical   -   FALSE                           -   -    TRUE ## ppc.bagImpute     logical   -   FALSE                           -   -    TRUE ## ppc.medianImpute  logical   -   FALSE                           -   -    TRUE ## ppc.pca           logical   -   FALSE                           -   -    TRUE ## ppc.ica           logical   -   FALSE                           -   -    TRUE ## ppc.spatialSign   logical   -   FALSE                           -   -    TRUE ## ppc.corr          logical   -   FALSE                           -   -    TRUE ## ppc.zv            logical   -   FALSE                           -   -    TRUE ## ppc.nzv           logical   -   FALSE                           -   -    TRUE ## ppc.thresh        numeric   -    0.95                    0 to Inf   -    TRUE ## ppc.pcaComp       integer   -       -                    1 to Inf   -    TRUE ## ppc.na.remove     logical   -    TRUE                           -   -    TRUE ## ppc.k             integer   -       5                    1 to Inf   -    TRUE ## ppc.fudge         numeric   -     0.2                    0 to Inf   -    TRUE ## ppc.numUnique     integer   -       3                    1 to Inf   -    TRUE ## ppc.n.comp        integer   -       -                    1 to Inf   -    TRUE ## ppc.cutoff        numeric   -     0.9                      0 to 1   -    TRUE ## ppc.freqCut       numeric   -      19                    1 to Inf   -    TRUE ## ppc.uniqueCut     numeric   -      10                    0 to Inf   -    TRUE ## method           discrete   -  moment            moment,mle,mve,t   -    TRUE ## nu                numeric   -       5                    2 to Inf   Y    TRUE ## predict.method   discrete   - plug-in plug-in,predictive,debiased   -    TRUE ##                  Trafo ## ppc.BoxCox           - ## ppc.YeoJohnson       - ## ppc.expoTrans        - ## ppc.center           - ## ppc.scale            - ## ppc.range            - ## ppc.knnImpute        - ## ppc.bagImpute        - ## ppc.medianImpute     - ## ppc.pca              - ## ppc.ica              - ## ppc.spatialSign      - ## ppc.corr             - ## ppc.zv               - ## ppc.nzv              - ## ppc.thresh           - ## ppc.pcaComp          - ## ppc.na.remove        - ## ppc.k                - ## ppc.fudge            - ## ppc.numUnique        - ## ppc.n.comp           - ## ppc.cutoff           - ## ppc.freqCut          - ## ppc.uniqueCut        - ## method               - ## nu                   - ## predict.method       - ps = makeParamSet(   makeIntegerParam(\"ppc.pcaComp\", lower = 1, upper = getTaskNFeats(sonar.task)),   makeDiscreteParam(\"predict.method\", values = c(\"plug-in\", \"debiased\")) ) ctrl = makeTuneControlGrid(resolution = 10) res = tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE) res ## Tune result: ## Op. pars: ppc.pcaComp=21; predict.method=plug-in ## mmce.test.mean=0.1779848  as.data.frame(res$opt.path)[1:3] ##    ppc.pcaComp predict.method mmce.test.mean ## 1            1        plug-in      0.4518288 ## 2            8        plug-in      0.2449275 ## 3           14        plug-in      0.2021394 ## 4           21        plug-in      0.1779848 ## 5           27        plug-in      0.2212560 ## 6           34        plug-in      0.2452726 ## 7           40        plug-in      0.2500345 ## 8           47        plug-in      0.2452726 ## 9           53        plug-in      0.2549344 ## 10          60        plug-in      0.2932367 ## 11           1       debiased      0.4375431 ## 12           8       debiased      0.2833678 ## 13          14       debiased      0.2453416 ## 14          21       debiased      0.2837129 ## 15          27       debiased      0.2547274 ## 16          34       debiased      0.2886128 ## 17          40       debiased      0.2741891 ## 18          47       debiased      0.3075914 ## 19          53       debiased      0.2642512 ## 20          60       debiased      0.2830918"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"writing-a-custom-preprocessing-wrapper","dir":"Articles > Tutorial","previous_headings":"","what":"Writing a custom preprocessing wrapper","title":"Data Preprocessing","text":"options offered makePreprocWrapperCaret() enough, can write preprocessing wrapper using function makePreprocWrapper(). described tutorial section wrapped learners wrappers implemented using train predict method. case preprocessing wrappers methods specify transform data training prediction completely user-defined. show create preprocessing wrapper centers scales data training/predicting. learning methods , e.g., k nearest neighbors, support vector machines neural networks usually require scaled features. Many, , built-scaling option training data set scaled model fitting test data set scaled accordingly, using scaling parameters training stage, making predictions. following show add scaling option Learner (makeLearner()) coupling function base::scale(). Note chose simple example demonstration. Centering/scaling data also possible makePreprocWrapperCaret().","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"specifying-the-train-function","dir":"Articles > Tutorial","previous_headings":"","what":"Specifying the train function","title":"Data Preprocessing","text":"train function function following arguments: data data.frame columns features target variable. target string denotes name target variable data. args list arguments parameters influence preprocessing. must return list elements $data $control, $data preprocessed data set $control stores information required preprocess data prediction. train function scaling example given . calls base::scale() numerical features returns scaled training data corresponding scaling parameters. args contains center scale arguments function base::scale() slot $control stores scaling parameters used prediction stage. Regarding latter note center scale arguments base::scale() can either logical value numeric vector length equal number numeric columns data, respectively. logical value passed args store column means standard deviations/root mean squares $center $scale slots returned $control object.","code":"trainfun = function(data, target, args = list(center, scale)) {   # Identify numerical features   cns = colnames(data)   nums = setdiff(cns[sapply(data, is.numeric)], target)   # Extract numerical features from the data set and call scale   x = as.matrix(data[, nums, drop = FALSE])   x = scale(x, center = args$center, scale = args$scale)   # Store the scaling parameters in control   # These are needed to preprocess the data before prediction   control = args   if (is.logical(control$center) && control$center)     control$center = attr(x, \"scaled:center\")   if (is.logical(control$scale) && control$scale)     control$scale = attr(x, \"scaled:scale\")   # Recombine the data   data = data[, setdiff(cns, nums), drop = FALSE]   data = cbind(data, as.data.frame(x))   return(list(data = data, control = control)) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"specifying-the-predict-function","dir":"Articles > Tutorial","previous_headings":"","what":"Specifying the predict function","title":"Data Preprocessing","text":"predict function following arguments: data data.frame containing feature values (prediction target values naturally known). target string indicating name target variable. args args passed train function. control object returned train function. returns preprocessed data. scaling example predict function scales numerical features using parameters training stage stored control.","code":"predictfun = function(data, target, args, control) {   # Identify numerical features   cns = colnames(data)   nums = cns[sapply(data, is.numeric)]   # Extract numerical features from the data set and call scale   x = as.matrix(data[, nums, drop = FALSE])   x = scale(x, center = control$center, scale = control$scale)   # Recombine the data   data = data[, setdiff(cns, nums), drop = FALSE]   data = cbind(data, as.data.frame(x))   return(data) }"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"creating-the-preprocessing-wrapper","dir":"Articles > Tutorial","previous_headings":"","what":"Creating the preprocessing wrapper","title":"Data Preprocessing","text":"create preprocessing wrapper regression neural network (nnet::nnet()) (scaling option) base learner. train predict functions defined passed makePreprocWrapper() via train predict arguments. par.vals list parameter values relayed args argument train function. Let’s compare cross-validated mean squared error (mse) Boston Housing data set (mlbench::BostonHousing()) without scaling.","code":"lrn = makeLearner(\"regr.nnet\", trace = FALSE, decay = 1e-02) lrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun,   par.vals = list(center = TRUE, scale = TRUE)) lrn ## Learner regr.nnet.preproc from package nnet ## Type: regr ## Name: ; Short name:  ## Class: PreprocWrapper ## Properties: numerics,factors,weights ## Predict-Type: response ## Hyperparameters: size=3,trace=FALSE,decay=0.01 rdesc = makeResampleDesc(\"CV\", iters = 3)  r = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE) r  ## Resample Result ## Task: BostonHousing-example ## Learner: regr.nnet.preproc ## Aggr perf: mse.test.mean=26.6997095 ## Runtime: 0.083282  lrn = makeLearner(\"regr.nnet\", trace = FALSE, decay = 1e-02) r = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE) r  ## Resample Result ## Task: BostonHousing-example ## Learner: regr.nnet ## Aggr perf: mse.test.mean=56.8645496 ## Runtime: 0.0463462"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"joint-tuning-of-preprocessing-and-learner-parameters","dir":"Articles > Tutorial","previous_headings":"","what":"Joint tuning of preprocessing and learner parameters","title":"Data Preprocessing","text":"Often ’s clear preprocessing options work best certain learning algorithm. already shown number principal components makePreprocWrapperCaret() can tune easily together hyperparameters learner. scaling example can try nnet::nnet() works best centering scaling data ’s better omit one two operations preprocessing . order tune center scale add appropriate LearnerParam (ParamHelpers::LearnerParam())s parameter set (ParamHelpers::ParamSet()) wrapped learner. mentioned base::scale() allows numeric logical center scale arguments. want use latter option declare center scale logical learner parameters. Now simple grid search decay parameter nnet::nnet() center scale parameters.","code":"lrn = makeLearner(\"regr.nnet\", trace = FALSE) lrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun,   par.set = makeParamSet(     makeLogicalLearnerParam(\"center\"),     makeLogicalLearnerParam(\"scale\")   ),   par.vals = list(center = TRUE, scale = TRUE))  lrn ## Learner regr.nnet.preproc from package nnet ## Type: regr ## Name: ; Short name:  ## Class: PreprocWrapper ## Properties: numerics,factors,weights ## Predict-Type: response ## Hyperparameters: size=3,trace=FALSE,center=TRUE,scale=TRUE  getParamSet(lrn) ##            Type len    Def      Constr Req Tunable Trafo ## center  logical   -      -           -   -    TRUE     - ## scale   logical   -      -           -   -    TRUE     - ## size    integer   -      3    0 to Inf   -    TRUE     - ## maxit   integer   -    100    1 to Inf   -    TRUE     - ## skip    logical   -  FALSE           -   -    TRUE     - ## rang    numeric   -    0.7 -Inf to Inf   -    TRUE     - ## decay   numeric   -      0    0 to Inf   -    TRUE     - ## Hess    logical   -  FALSE           -   -    TRUE     - ## trace   logical   -   TRUE           -   -   FALSE     - ## MaxNWts integer   -   1000    1 to Inf   -   FALSE     - ## abstol  numeric   - 0.0001 -Inf to Inf   -    TRUE     - ## reltol  numeric   -  1e-08 -Inf to Inf   -    TRUE     - rdesc = makeResampleDesc(\"Holdout\") ps = makeParamSet(   makeDiscreteParam(\"decay\", c(0, 0.05, 0.1)),   makeLogicalParam(\"center\"),   makeLogicalParam(\"scale\") ) ctrl = makeTuneControlGrid() res = tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)  res ## Tune result: ## Op. pars: decay=0; center=FALSE; scale=TRUE ## mse.test.mean=15.3770780  df = as.data.frame(res$opt.path) df[, -ncol(df)] ##    decay center scale mse.test.mean dob eol error.message ## 1      0   TRUE  TRUE      23.87357   1  NA          <NA> ## 2   0.05   TRUE  TRUE      20.76474   2  NA          <NA> ## 3    0.1   TRUE  TRUE      21.80929   3  NA          <NA> ## 4      0  FALSE  TRUE      15.37708   4  NA          <NA> ## 5   0.05  FALSE  TRUE      18.07821   5  NA          <NA> ## 6    0.1  FALSE  TRUE      17.91147   6  NA          <NA> ## 7      0   TRUE FALSE      46.35254   7  NA          <NA> ## 8   0.05   TRUE FALSE      44.92389   8  NA          <NA> ## 9    0.1   TRUE FALSE      24.37692   9  NA          <NA> ## 10     0  FALSE FALSE      74.10216  10  NA          <NA> ## 11  0.05  FALSE FALSE      38.53006  11  NA          <NA> ## 12   0.1  FALSE FALSE      32.56318  12  NA          <NA>"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/preproc.html","id":"preprocessing-wrapper-functions","dir":"Articles > Tutorial","previous_headings":"","what":"Preprocessing wrapper functions","title":"Data Preprocessing","text":"written preprocessing wrapper might want use time time ’s good idea encapsulate function shown . think preprocessing method something others might want use well integrated mlr just contact us.","code":"makePreprocWrapperScale = function(learner, center = TRUE, scale = TRUE) {   trainfun = function(data, target, args = list(center, scale)) {     cns = colnames(data)     nums = setdiff(cns[sapply(data, is.numeric)], target)     x = as.matrix(data[, nums, drop = FALSE])     x = scale(x, center = args$center, scale = args$scale)     control = args     if (is.logical(control$center) && control$center)       control$center = attr(x, \"scaled:center\")     if (is.logical(control$scale) && control$scale)       control$scale = attr(x, \"scaled:scale\")     data = data[, setdiff(cns, nums), drop = FALSE]     data = cbind(data, as.data.frame(x))     return(list(data = data, control = control))   }   predictfun = function(data, target, args, control) {     cns = colnames(data)     nums = cns[sapply(data, is.numeric)]     x = as.matrix(data[, nums, drop = FALSE])     x = scale(x, center = control$center, scale = control$scale)     data = data[, setdiff(cns, nums), drop = FALSE]     data = cbind(data, as.data.frame(x))     return(data)   }   makePreprocWrapper(     learner,     train = trainfun,     predict = predictfun,     par.set = makeParamSet(       makeLogicalLearnerParam(\"center\"),       makeLogicalLearnerParam(\"scale\")     ),     par.vals = list(center = center, scale = scale)   ) }  lrn = makePreprocWrapperScale(\"classif.lda\") train(lrn, iris.task) ## Model for learner.id=classif.lda.preproc; learner.class=PreprocWrapper ## Trained on: task.id = iris-example; obs = 150; features = 4 ## Hyperparameters: center=TRUE,scale=TRUE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"defining-the-resampling-strategy","dir":"Articles > Tutorial","previous_headings":"","what":"Defining the resampling strategy","title":"Resampling","text":"mlr resampling strategy can defined via function makeResampleDesc(). requires string specifies resampling method , depending selected strategy, information like number iterations. supported resampling strategies : Cross-validation (\"CV\"), Leave-one-cross-validation (\"LOO\"), Repeated cross-validation (\"RepCV\"), --bag bootstrap variants like b632 (\"Bootstrap\"), Subsampling, also called Monte-Carlo cross-validation (\"Subsample\"), Holdout (training/test) (\"Holdout\"). example want use 3-fold cross-validation type: holdout estimation use: order save typing mlr contains pre-defined resample descriptions common strategies like holdout (hout (makeResampleDesc())) well cross-validation different numbers folds (e.g., cv5 (makeResampleDesc()) cv10 (makeResampleDesc())).","code":"# 3-fold cross-validation rdesc = makeResampleDesc(\"CV\", iters = 3) rdesc ## Resample description: cross-validation with 3 iterations. ## Predict: test ## Stratification: FALSE # Holdout estimation rdesc = makeResampleDesc(\"Holdout\") rdesc ## Resample description: holdout with 0.67 split rate. ## Predict: test ## Stratification: FALSE hout ## Resample description: holdout with 0.67 split rate. ## Predict: test ## Stratification: FALSE  cv3 ## Resample description: cross-validation with 3 iterations. ## Predict: test ## Stratification: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"performing-the-resampling","dir":"Articles > Tutorial","previous_headings":"","what":"Performing the resampling","title":"Resampling","text":"Function resample() evaluates Learner (makeLearner()) given machine learning Task() using selected resampling strategy (makeResampleDesc()). first example, performance linear regression (stats::lm()) BostonHousing (mlbench::BostonHousing()) data set calculated using 3-fold cross-validation. Generally, \\(K\\)-fold cross-validation data set \\(D\\) partitioned \\(K\\) subsets (approximately) equal size. \\(b\\)-th \\(K\\) iterations, \\(b\\)-th subset used testing, union remaining parts forms training set. usual, can either pass Learner (makeLearner()) object resample() , done , provide class name \"regr.lm\" learner. Since performance measure specified default regression learners (mean squared error, mse) calculated. result r object class resample() result. contains performance results learner additional information like runtime, predicted values, optionally models fitted single resampling iterations. r$measures.test gives performance 3 test data sets. r$aggr shows aggregated performance value. name \"mse.test.mean\" indicates performance measure, mse, method, test.mean (aggregations()), used aggregate 3 individual performances. test.mean (aggregations()) default aggregation scheme performance measures , name implies, takes mean performances test data sets. Resampling mlr works way types learning problems learners. classification example classification tree (rpart) (rpart::rpart()) evaluated Sonar (mlbench::sonar()) data set subsampling 5 iterations. subsampling iteration data set \\(D\\) randomly partitioned training test set according given percentage, e.g., 2/3 training 1/3 test set. just one iteration, strategy commonly called holdout test sample estimation. can calculate several measures passing list Measures (makeMeasure())s resample(). , error rate (mmce), false positive false negative rates (fpr, fnr), time takes train learner (timetrain) estimated subsampling 5 iterations. want add measures afterwards, use addRRMeasure(). default, resample() prints progress messages intermediate results. can turn setting show.info = FALSE, done code chunk . (interested suppressing messages permanently look tutorial page configuring mlr.) example, Learner (makeLearner()) explicitly constructed. convenience can also specify learner string pass learner parameters via ... argument resample().","code":"## Resampling: cross-validation ## Measures:             mse ## [Resample] iter 1:    19.8630628 ## [Resample] iter 2:    29.4831894 ## [Resample] iter 3:    21.2694775 ##  ## Aggregated Result: mse.test.mean=23.5385766 ## # Specify the resampling strategy (3-fold cross-validation) rdesc = makeResampleDesc(\"CV\", iters = 3)  # Calculate the performance r = resample(\"regr.lm\", bh.task, rdesc) ## Resampling: cross-validation ## Measures:             mse ## [Resample] iter 1:    25.1371739 ## [Resample] iter 2:    23.1279497 ## [Resample] iter 3:    21.9152672 ## ## Aggregated Result: mse.test.mean=23.3934636 ##  r ## Resample Result ## Task: BostonHousing-example ## Learner: regr.lm ## Aggr perf: mse.test.mean=23.3934636 ## Runtime: 0.0375051 # Peak into r names(r) ##  [1] \"learner.id\"     \"task.id\"        \"task.desc\"      \"measures.train\" ##  [5] \"measures.test\"  \"aggr\"           \"pred\"           \"models\"         ##  [9] \"err.msgs\"       \"err.dumps\"      \"extract\"        \"runtime\"  r$aggr ## mse.test.mean  ##      23.53858  r$measures.test ##   iter      mse ## 1    1 19.86306 ## 2    2 29.48319 ## 3    3 21.26948 # Subsampling with 5 iterations and default split ratio 2/3 rdesc = makeResampleDesc(\"Subsample\", iters = 5)  # Subsampling with 5 iterations and 4/5 training data rdesc = makeResampleDesc(\"Subsample\", iters = 5, split = 4/5)  # Classification tree with information splitting criterion lrn = makeLearner(\"classif.rpart\", parms = list(split = \"information\"))  # Calculate the performance measures r = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain)) ## Resampling: subsampling ## Measures:             mmce        fpr         fnr         timetrain ## [Resample] iter 1:    0.4047619   0.5416667   0.2222222   0.0110000 ## [Resample] iter 2:    0.1666667   0.1200000   0.2352941   0.0070000 ## [Resample] iter 3:    0.3333333   0.1333333   0.4444444   0.0100000 ## [Resample] iter 4:    0.2380952   0.3913043   0.0526316   0.0280000 ## [Resample] iter 5:    0.3095238   0.2800000   0.3529412   0.0080000 ## ## Aggregated Result: mmce.test.mean=0.2904762,fpr.test.mean=0.2932609,fnr.test.mean=0.2615067,timetrain.test.mean=0.0128000 ##  r ## Resample Result ## Task: Sonar-example ## Learner: classif.rpart ## Aggr perf: mmce.test.mean=0.2904762,fpr.test.mean=0.2932609,fnr.test.mean=0.2615067,timetrain.test.mean=0.0128000 ## Runtime: 0.10692 # Add balanced error rate (ber) and time used to predict addRRMeasure(r, list(ber, timepredict))  ## Resample Result ## Task: Sonar-example ## Learner: classif.rpart ## Aggr perf: mmce.test.mean=0.2904762,fpr.test.mean=0.2932609,fnr.test.mean=0.2615067,timetrain.test.mean=0.0128000,ber.test.mean=0.2773838,timepredict.test.mean=0.0032000 ## Runtime: 0.10692 r = resample(\"classif.rpart\", parms = list(split = \"information\"), sonar.task, rdesc,   measures = list(mmce, fpr, fnr, timetrain), show.info = FALSE)  r  ## Resample Result ## Task: Sonar-example ## Learner: classif.rpart ## Aggr perf: mmce.test.mean=0.2428571,fpr.test.mean=0.2968173,fnr.test.mean=0.1970195,timetrain.test.mean=0.0084000 ## Runtime: 0.0791025"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"accessing-resample-results","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing resample results","title":"Resampling","text":"Apart learner performance can extract information resample results, example predicted values models fitted individual resample iterations.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"predictions","dir":"Articles > Tutorial","previous_headings":"Accessing resample results","what":"Predictions","title":"Resampling","text":"Per default, resample() result contains predictions made resampling. want keep , e.g., order conserve memory, set keep.pred = FALSE calling resample(). predictions stored slot $pred resampling result, can also accessed function getRRPredictions(). pred object class resample() Prediction. Just Prediction() object (see tutorial page making predictions element $data data.frame contains predictions case supervised learning problem true values target variable(s). can use .data.frame (Prediction() directly access $data slot. Moreover, getter functions Prediction() objects like getPredictionResponse() getPredictionProbabilities() applicable. columns iter set data.frame indicate resampling iteration data set (train test) prediction made. default, predictions made test sets . predictions training set required, set predict = \"train\" (predictions train set ) predict = \"\" (predictions train test sets) makeResampleDesc(). case, necessary bootstrap methods (b632 b632+) examples shown later . , use simple Holdout, .e., split data training test set, resampling strategy make predictions sets. (Please note nonetheless misclassification rate r$aggr estimated test data . calculate performance measures training sets shown .) second function extract predictions resample results getRRPredictionList() returns list predictions split data set (train/test) resampling iteration.","code":"r$pred ## Resampled Prediction for: ## Resample description: subsampling with 5 iterations and 0.80 split rate. ## Predict: test ## Stratification: FALSE ## predict.type: response ## threshold: ## time (mean): 0.00 ##    id truth response iter  set ## 1  18     R        M    1 test ## 2 189     M        M    1 test ## 3  88     R        R    1 test ## 4 121     M        R    1 test ## 5 165     M        R    1 test ## 6 111     M        M    1 test ## ... (#rows: 210, #cols: 5)  pred = getRRPredictions(r) pred ## Resampled Prediction for: ## Resample description: subsampling with 5 iterations and 0.80 split rate. ## Predict: test ## Stratification: FALSE ## predict.type: response ## threshold: ## time (mean): 0.00 ##    id truth response iter  set ## 1  18     R        M    1 test ## 2 189     M        M    1 test ## 3  88     R        R    1 test ## 4 121     M        R    1 test ## 5 165     M        R    1 test ## 6 111     M        M    1 test ## ... (#rows: 210, #cols: 5) head(as.data.frame(pred)) ##    id truth response iter  set ## 1  66     R        R    1 test ## 2 170     M        M    1 test ## 3  90     R        M    1 test ## 4  26     R        R    1 test ## 5 187     M        R    1 test ## 6  89     R        M    1 test  head(getPredictionTruth(pred)) ## [1] R M R R M R ## Levels: M R  head(getPredictionResponse(pred)) ## [1] R M M R R M ## Levels: M R # Make predictions on both training and test sets rdesc = makeResampleDesc(\"Holdout\", predict = \"both\")  r = resample(\"classif.lda\", iris.task, rdesc, show.info = FALSE) r ## Resample Result ## Task: iris-example ## Learner: classif.lda ## Aggr perf: mmce.test.mean=0.0200000 ## Runtime: 0.00993848  r$measures.train ##   iter mmce ## 1    1 0.02 predList = getRRPredictionList(r) predList ## $train ## $train$`1` ## Prediction: 100 observations ## predict.type: response ## threshold: ## time: 0.00 ##      id      truth   response ## 96   96 versicolor versicolor ## 130 130  virginica  virginica ## 120 120  virginica  virginica ## 77   77 versicolor versicolor ## 23   23     setosa     setosa ## 59   59 versicolor versicolor ## ... (#rows: 100, #cols: 3) ## ## ## $test ## $test$`1` ## Prediction: 50 observations ## predict.type: response ## threshold: ## time: 0.00 ##      id      truth   response ## 92   92 versicolor versicolor ## 58   58 versicolor versicolor ## 48   48     setosa     setosa ## 103 103  virginica  virginica ## 70   70 versicolor versicolor ## 82   82 versicolor versicolor ## ... (#rows: 50, #cols: 3)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"learner-models","dir":"Articles > Tutorial","previous_headings":"Accessing resample results","what":"Learner models","title":"Resampling","text":"resampling iteration Learner (makeLearner()) fitted respective training set. default, resulting WrappedModel (makeWrappedModel())s included resample() result slot $models empty. order keep , set models = TRUE calling resample(), following survival analysis example.","code":"# 3-fold cross-validation rdesc = makeResampleDesc(\"CV\", iters = 3)  r = resample(\"surv.coxph\", lung.task, rdesc, show.info = FALSE, models = TRUE) r$models ## [[1]] ## Model for learner.id=surv.coxph; learner.class=surv.coxph ## Trained on: task.id = lung-example; obs = 111; features = 8 ## Hyperparameters:  ##  ## [[2]] ## Model for learner.id=surv.coxph; learner.class=surv.coxph ## Trained on: task.id = lung-example; obs = 111; features = 8 ## Hyperparameters:  ##  ## [[3]] ## Model for learner.id=surv.coxph; learner.class=surv.coxph ## Trained on: task.id = lung-example; obs = 112; features = 8 ## Hyperparameters:"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"the-extract-option","dir":"Articles > Tutorial","previous_headings":"Accessing resample results","what":"The extract option","title":"Resampling","text":"Keeping complete fitted models can memory-intensive objects large number resampling iterations high. Alternatively, can use extract argument resample() retain information need. end need pass function extract applied WrappedModel (makeWrappedModel()) object fitted resampling iteration. , cluster datasets::mtcars() data using \\(k\\)-means algorithm \\(k = 3\\) keep cluster centers. second example, extract variable importances fitted regression trees using function getFeatureImportance(). (detailed information topic see feature selection page.) also convenience function getResamplingIndices() extract resampling indices ResampleResult object:","code":"# 3-fold cross-validation rdesc = makeResampleDesc(\"CV\", iters = 3)  # Extract the compute cluster centers r = resample(\"cluster.kmeans\", mtcars.task, rdesc, show.info = FALSE,   centers = 3, extract = function(x) getLearnerModel(x)$centers) r$extract ## [[1]] ##        mpg cyl     disp        hp     drat       wt     qsec        vs ## 1 26.45556   4 107.7556  83.33333 4.094444 2.291444 19.15889 0.8888889 ## 2 14.94444   8 338.2889 208.33333 3.178889 3.894889 16.69889 0.0000000 ## 3 19.57500   6 202.6500 112.00000 3.415000 3.247500 18.89500 0.7500000 ##          am     gear     carb ## 1 0.6666667 4.111111 1.666667 ## 2 0.1111111 3.222222 3.555556 ## 3 0.2500000 3.500000 2.500000 ##  ## [[2]] ##        mpg      cyl      disp       hp     drat     wt     qsec        vs ## 1 18.32857 6.571429 202.40000 142.2857 3.465714 3.3200 17.85429 0.4285714 ## 2 26.73333 4.000000  96.96667  77.5000 4.178333 2.1125 18.70167 0.8333333 ## 3 14.96250 8.000000 387.75000 218.6250 3.238750 4.1955 16.65750 0.0000000 ##          am     gear     carb ## 1 0.2857143 3.714286 3.571429 ## 2 0.8333333 4.000000 1.333333 ## 3 0.2500000 3.500000 3.750000 ##  ## [[3]] ##        mpg cyl     disp        hp     drat       wt     qsec  vs        am ## 1 20.46000   6 178.1200 125.60000 3.684000 2.984000 17.34400 0.4 0.6000000 ## 2 26.87143   4 108.7714  86.14286 3.948571 2.426857 19.48286 1.0 0.7142857 ## 3 15.12222   8 354.2889 208.22222 3.306667 3.983333 16.71889 0.0 0.1111111 ##       gear     carb ## 1 4.000000 3.800000 ## 2 4.142857 1.571429 ## 3 3.222222 3.333333 # Extract the variable importance in a regression tree r = resample(\"regr.rpart\", bh.task, rdesc, show.info = FALSE, extract = getFeatureImportance) r$extract ## [[1]] ## FeatureImportance: ## Task: BostonHousing-example ##  ## Learner: regr.rpart ## Measure: NA ## Contrast: NA ## Aggregation: function (x)  x ## Replace: NA ## Number of Monte-Carlo iterations: NA ## Local: FALSE ## # A tibble: 6 × 2 ##   variable importance ##   <chr>         <dbl> ## 1 crim          3102. ## 2 zn            1442. ## 3 indus         4513. ## 4 chas           407. ## 5 nox           4080. ## 6 rm           17791. ##  ## [[2]] ## FeatureImportance: ## Task: BostonHousing-example ##  ## Learner: regr.rpart ## Measure: NA ## Contrast: NA ## Aggregation: function (x)  x ## Replace: NA ## Number of Monte-Carlo iterations: NA ## Local: FALSE ## # A tibble: 6 × 2 ##   variable importance ##   <chr>         <dbl> ## 1 crim          3456. ## 2 zn            1220. ## 3 indus         3973. ## 4 chas           193. ## 5 nox           2218. ## 6 rm           15578. ##  ## [[3]] ## FeatureImportance: ## Task: BostonHousing-example ##  ## Learner: regr.rpart ## Measure: NA ## Contrast: NA ## Aggregation: function (x)  x ## Replace: NA ## Number of Monte-Carlo iterations: NA ## Local: FALSE ## # A tibble: 6 × 2 ##   variable importance ##   <chr>         <dbl> ## 1 crim          5432. ## 2 zn            2233. ## 3 indus         4145. ## 4 chas             0  ## 5 nox           3114. ## 6 rm            8294. getResamplingIndices(r) ## $train.inds ## $train.inds[[1]] ##   [1] 366 235  79 466 361  88  16 346 218 438 444 397  55 456 327 226  38 172 ##  [19] 252 500 450 464 149 136  71  47 423 208 203 462 205 116 350 129 261 243 ##  [37] 490 241 406 430 340 420  10 277 100 190  26 188 437 130 282 225 328 317 ##  [55]  95  51 398 237 285 146  24 238 223   5 152 300 232 151 169 383 470  42 ##  [73]  83 322 179 198 162 103 220 382 202 240 125 443 256  43  32  77 275 426 ##  [91] 181 273 451 142 332 442 257 119 489  39 305  63 127 263 424 289  60  78 ## [109]  59 314 148  90 387 455 411 502  65 267 269 176  31 484  70 196 435 439 ## [127] 492 410 473 313 154 506 210 377 499 482  96 431 452  49  92 178 270 265 ## [145] 219 461 297 415 120  58 333 117 497 349 141 266 445 164  36 329 389  81 ## [163] 339  98 348 380 474  13 221 414 264 375 352 107  12 308 280 384 177 295 ## [181] 143 165 126 227 189 393 447 183  50 290 209 360 504  27 139 402 255 422 ## [199] 312 315 372 251 491 104 416 400 138 501 330 454 485 199 417 302 498  56 ## [217] 413 460   2 428 351 156 356 163 215 197 394 288 354 376 448 171 287 390 ## [235] 242 370   7 303 167  45  91 353 344 102 403 274  64 106  76 294 419 378 ## [253] 228 204  73 379 284 463 161 355 323 272  87 111 418  53  21 316  94 486 ## [271] 131 381 293 425  85 388 214 345 276 182  61 108 325 145  68 246 121  19 ## [289] 427   6 234 259  35 341 133 391  67 175 421 195  99 216 365 503 248  44 ## [307] 173 459 236  11 286  52 296 335 475 144 359 432 429 331 114 123 113 311 ## [325]   4 186  86 187 279 268 140 409 363 206  84   3 192 ##  ## $train.inds[[2]] ##   [1] 235  79 249  16 212 456 457 105  38 449 172 357  72 500  20   9 321 436 ##  [19] 458 385 200  47 208 396 193 205 350 129 261 496 241 132 278 406  25 340 ##  [37] 118 306 440 453 277  80 188  54 224 225 328  95  51 319 505 247  97 238 ##  [55] 223   5 407  62  22 300 153 309 358  46  17 383 322 198 162 441 202 240 ##  [73]  40 125 230 194 426 343 433 181 273 451 434  82 142 332 442 467 489  39 ##  [91] 127 263  48 364 367 326 101 362 347 471 338 213 124  60 401 185 314 148 ## [109]  18 387 455 411 476 502  65 488 260 267 336  34 484 410 313 154 271  29 ## [127] 210 377 499 482 320 166 307 483 431 452  92 178 211 494 270 477 170 404 ## [145] 265  30 219 304 231 461 297 495   8 117 374 262 266 164  36 368 155 329 ## [163] 334 389 412 339 337  98 134 479 380 184 115  13  57 414 264  23 352 229 ## [181] 157 384 150 177 250 165 126 227 147 258 487  50 290 465 174 292 209  93 ## [199] 504  27 139 422 310 245 222 491 299  33 416 399 138 480 501 330 199 342 ## [217] 168 493 128 137 233  41  56 180 428 156 478 163 215 197 394 376 135 386 ## [235] 287 242   7 239  69 468 353  89 472 344   1 481 102 274  64 395 110  76 ## [253]  37  74  14 298 294 419 318 228 122 371  73 463 161 355 272  87 369 111 ## [271] 112 418 254 283  53 316  94 159 131 293 425  28 324 281 345 217 109 276 ## [289] 446 182 325 145  68 158 121  19 405   6 259 341 201 291 391  67  15 421 ## [307] 195 301 503  44 244  66 236  11 286 408  52 144 432 429 253 207   4  86 ## [325] 392 187 268  75 409 363 160 373 206  84 469 192 191 ##  ## $train.inds[[3]] ##   [1] 366 249 466 361  88 346 218 212 438 444 397  55 457 105 327 226 449 357 ##  [19] 252  72  20   9 321 450 436 458 385 464 149 200 136  71 423 396 203 193 ##  [37] 462 116 243 496 490 132 278 430  25 420 118  10 306 440 453 100 190  26 ##  [55]  80  54 437 130 224 282 317 319 398 505 237 247 285 146  97  24 407  62 ##  [73]  22 152 153 232 309 358 151  46  17 169 470  42  83 179 103 441 220 382 ##  [91]  40 230 443 256  43  32  77 275 194 343 433 434  82 257 119 467 305  63 ## [109] 424  48 289 364 367 326 101 362 347 471 338 213 124 401 185  78  59  90 ## [127]  18 476 488 260 336 269  34 176  31  70 196 435 439 492 473 271 506  29 ## [145] 320 166 307  96 483  49 211 494 477 170 404  30 304 231 415 120 495  58 ## [163]   8 333 374 497 349 141 262 445 368 155 334 412  81 337 134 479 348 474 ## [181] 184 115  57 221  23 375 107  12 308 280 229 157 150 250 295 143 147 189 ## [199] 258 393 447 487 183 465 174 292 360  93 402 255 312 315 310 372 245 251 ## [217] 222 104 299  33 399 400 480 454 485 342 168 417 493 128 137 302 233 498 ## [235]  41 413 460   2 180 351 478 356 288 354 135 448 386 171 390 370 239 303 ## [253] 167  45  69 468  91  89 472   1 481 403 395 110 106  37  74  14 298 318 ## [271] 378 122 204 371 379 284 323 369 112 254 283  21 486 159 381  28  85 388 ## [289] 324 281 214 217 109 446  61 108 246 158 405 427 234  35 133 201 291  15 ## [307] 175 301  99 216 365 248 244  66 173 459 408 296 335 475 359 331 253 114 ## [325] 123 207 113 311 186 392 279 140  75 160 373 469   3 191 ##  ##  ## $test.inds ## $test.inds[[1]] ##   [1]   1   8   9  14  15  17  18  20  22  23  25  28  29  30  33  34  37  40 ##  [19]  41  46  48  54  57  62  66  69  72  74  75  80  82  89  93  97 101 105 ##  [37] 109 110 112 115 118 122 124 128 132 134 135 137 147 150 153 155 157 158 ##  [55] 159 160 166 168 170 174 180 184 185 191 193 194 200 201 207 211 212 213 ##  [73] 217 222 224 229 230 231 233 239 244 245 247 249 250 253 254 258 260 262 ##  [91] 271 278 281 283 291 292 298 299 301 304 306 307 309 310 318 319 320 321 ## [109] 324 326 334 336 337 338 342 343 347 357 358 362 364 367 368 369 371 373 ## [127] 374 385 386 392 395 396 399 401 404 405 407 408 412 433 434 436 440 441 ## [145] 446 449 453 457 458 465 467 468 469 471 472 476 477 478 479 480 481 483 ## [163] 487 488 493 494 495 496 505 ##  ## $test.inds[[2]] ##   [1]   2   3  10  12  21  24  26  31  32  35  42  43  45  49  55  58  59  61 ##  [19]  63  70  71  77  78  81  83  85  88  90  91  96  99 100 103 104 106 107 ##  [37] 108 113 114 116 119 120 123 130 133 136 140 141 143 146 149 151 152 167 ##  [55] 169 171 173 175 176 179 183 186 189 190 196 203 204 214 216 218 220 221 ##  [73] 226 232 234 237 243 246 248 251 252 255 256 257 269 275 279 280 282 284 ##  [91] 285 288 289 295 296 302 303 305 308 311 312 315 317 323 327 331 333 335 ## [109] 346 348 349 351 354 356 359 360 361 365 366 370 372 375 378 379 381 382 ## [127] 388 390 393 397 398 400 402 403 413 415 417 420 423 424 427 430 435 437 ## [145] 438 439 443 444 445 447 448 450 454 459 460 462 464 466 470 473 474 475 ## [163] 485 486 490 492 497 498 506 ##  ## $test.inds[[3]] ##   [1]   4   5   6   7  11  13  16  19  27  36  38  39  44  47  50  51  52  53 ##  [19]  56  60  64  65  67  68  73  76  79  84  86  87  92  94  95  98 102 111 ##  [37] 117 121 125 126 127 129 131 138 139 142 144 145 148 154 156 161 162 163 ##  [55] 164 165 172 177 178 181 182 187 188 192 195 197 198 199 202 205 206 208 ##  [73] 209 210 215 219 223 225 227 228 235 236 238 240 241 242 259 261 263 264 ##  [91] 265 266 267 268 270 272 273 274 276 277 286 287 290 293 294 297 300 313 ## [109] 314 316 322 325 328 329 330 332 339 340 341 344 345 350 352 353 355 363 ## [127] 376 377 380 383 384 387 389 391 394 406 409 410 411 414 416 418 419 421 ## [145] 422 425 426 428 429 431 432 442 451 452 455 456 461 463 482 484 489 491 ## [163] 499 500 501 502 503 504"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"stratification-blocking-and-grouping","dir":"Articles > Tutorial","previous_headings":"","what":"Stratification, Blocking and Grouping","title":"Resampling","text":"Stratification respect categorical variable makes sure values present training test set approximately proportion original data set. Stratification possible regard categorical target variables (thus supervised classification survival analysis) categorical explanatory variables. Blocking refers situation subsets observations belong together must separated resampling. Hence, one train/test set pair entire block either training set test set. Grouping means folds composed factor vector given user. setting repetitions possible folds predefined. approach can also used nested resampling setting. Note subtle important difference “Blocking”: “Blocking” factor levels respected splitting train test (e.g. test set composed two given factor levels) whereas “Grouping” folds strictly follow factor level grouping (meaning test set always consist one factor level).","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"stratification-with-respect-to-the-target-variables","dir":"Articles > Tutorial","previous_headings":"Stratification, Blocking and Grouping","what":"Stratification with respect to the target variable(s)","title":"Resampling","text":"classification, usually desirable proportion classes partitions original data set. particularly useful case imbalanced classes small data sets. Otherwise, may happen observations less frequent classes missing training sets can decrease performance learner, lead model crashes. order conduct stratified resampling, set stratify = TRUE makeResampleDesc(). Stratification also available survival tasks. stratification balances censoring rate.","code":"# 3-fold cross-validation rdesc = makeResampleDesc(\"CV\", iters = 3, stratify = TRUE)  r = resample(\"classif.lda\", iris.task, rdesc, show.info = FALSE) r ## Resample Result ## Task: iris-example ## Learner: classif.lda ## Aggr perf: mmce.test.mean=0.0200000 ## Runtime: 0.0176296"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"stratification-with-respect-to-explanatory-variables","dir":"Articles > Tutorial","previous_headings":"Stratification, Blocking and Grouping","what":"Stratification with respect to explanatory variables","title":"Resampling","text":"Sometimes required also stratify input data, e.g., ensure subgroups represented training test sets. stratify input columns, specify factor columns task data via stratify.cols.","code":"rdesc = makeResampleDesc(\"CV\", iters = 3, stratify.cols = \"chas\")  r = resample(\"regr.rpart\", bh.task, rdesc, show.info = FALSE) r ## Resample Result ## Task: BostonHousing-example ## Learner: regr.rpart ## Aggr perf: mse.test.mean=23.8843587 ## Runtime: 0.0268815"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"blocking-cv-with-flexible-predefined-indices","dir":"Articles > Tutorial","previous_headings":"Stratification, Blocking and Grouping","what":"Blocking: CV with flexible predefined indices","title":"Resampling","text":"observations “belong together” must separated splitting data training test sets resampling, can supply information via blocking factor creating task. performing simple “CV” resampling inspecting result, see training indices fold 1 correspond specified grouping set blocking task. initiate method, need set blocking.cv = TRUE creating resample description object. However, please note effects method: created folds size! , Fold 1 120/30 split two folds 90/60 split. caused fact supplied five groups must belong together used three fold resampling strategy .","code":"# 5 blocks containing 30 observations each task = makeClassifTask(data = iris, target = \"Species\", blocking = factor(rep(1:5, each = 30))) task ## Supervised task: iris ## Type: classif ## Target: Species ## Observations: 150 ## Features: ##    numerics     factors     ordered functionals  ##           4           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: TRUE ## Has coordinates: FALSE ## Classes: 3 ##     setosa versicolor  virginica  ##         50         50         50  ## Positive class: NA rdesc = makeResampleDesc(\"CV\", iters = 3, blocking.cv = TRUE) p = resample(\"classif.lda\", task, rdesc) ## Resampling: cross-validation ## Measures:             mmce ## [Resample] iter 1:    0.0000000 ## [Resample] iter 2:    0.0500000 ## [Resample] iter 3:    0.0500000 ##  ## Aggregated Result: mmce.test.mean=0.0333333 ##   sort(p$pred$instance$train.inds[[1]]) ##   [1]  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 ##  [19]  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 ##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 ##  [55]  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 ##  [73] 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 ##  [91] 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 ## [109] 139 140 141 142 143 144 145 146 147 148 149 150 lapply(p$pred$instance$train.inds, function(x) length(x)) ## [[1]] ## [1] 120 ##  ## [[2]] ## [1] 90 ##  ## [[3]] ## [1] 90"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"grouping-cv-with-fixed-predefined-indices","dir":"Articles > Tutorial","previous_headings":"Stratification, Blocking and Grouping","what":"Grouping: CV with fixed predefined indices","title":"Resampling","text":"second way using predefined indices resampling mlr: Constructing folds based supplied indices blocking. refer method “grouping” distinguish “blocking”. method restrictive way always use number levels supplied via blocking number folds. use method, need set fixed = TRUE instead blocking.cv creating resampling description object. can leave iters argument, set internally number supplied factor levels. can see automatically created five folds test set always corresponds one factor level. way also means repeated CV way create multiple shuffled folds fixed arrangement. However, method can also used nested resampling settings (e.g. hyperparameter tuning). inner level, factor levels honored function simply creates one fold less outer level. Please note iters argument effect makeResampleDesc() fixed = TRUE. number folds automatically set based supplied number factor levels via blocking. inner level, number folds simply one less outer level. check inner resampling indices, can call getResamplingIndices(inner = TRUE). can see every outer fold (List 5), four inner folds created respect grouping supplied via blocking argument. course can also use normal random sampling “CV” description inner level just setting fixed = FALSE.","code":"rdesc = makeResampleDesc(\"CV\", fixed = TRUE) p = resample(\"classif.lda\", task, rdesc) ## Warning in makeResampleInstance(resampling, task = task): 'Blocking' features in ## the task were detected but 'blocking.cv' was not set in 'resample()'. ## Warning in makeResampleInstance(resampling, task = task): Setting 'blocking.cv' ## to TRUE to prevent undesired behavior. Set `blocking.cv' = TRUE` in ## `makeResampleDesc()` to silence this warning'. ## Warning in instantiateResampleInstance.CVDesc(desc, size, task): Adjusting ## levels to match number of blocking levels. ## Resampling: cross-validation ## Measures:             mmce ## [Resample] iter 1:    0.0000000 ## [Resample] iter 2:    0.1000000 ## [Resample] iter 3:    0.0000000 ## [Resample] iter 4:    0.1000000 ## [Resample] iter 5:    0.0000000 ##  ## Aggregated Result: mmce.test.mean=0.0400000 ##  sort(p$pred$instance$train.inds[[1]]) ##   [1]  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 ##  [19]  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 ##  [37]  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 ##  [55]  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 ##  [73] 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 ##  [91] 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 ## [109] 139 140 141 142 143 144 145 146 147 148 149 150 lapply(p$pred$instance$train.inds, function(x) length(x)) ## [[1]] ## [1] 120 ##  ## [[2]] ## [1] 120 ##  ## [[3]] ## [1] 120 ##  ## [[4]] ## [1] 120 ##  ## [[5]] ## [1] 120 # test fixed in nested resampling lrn = makeLearner(\"classif.lda\") ctrl <- makeTuneControlRandom(maxit = 2) ps <- makeParamSet(makeNumericParam(\"nu\", lower = 2, upper = 20)) inner = makeResampleDesc(\"CV\", fixed = TRUE) outer = makeResampleDesc(\"CV\", fixed = TRUE) tune_wrapper = makeTuneWrapper(lrn, resampling = inner, par.set = ps,   control = ctrl, show.info = FALSE)  p = resample(tune_wrapper, task, outer, show.info = FALSE,   extract = getTuneResult) str(getResamplingIndices(p, inner = TRUE)) ## List of 5 ##  $ :List of 2 ##   ..$ train.inds:List of 4 ##   .. ..$ : int [1:90] 106 93 149 103 133 150 119 48 100 142 ... ##   .. ..$ : int [1:90] 106 11 93 149 29 103 133 150 27 119 ... ##   .. ..$ : int [1:90] 106 11 93 29 103 27 119 48 3 100 ... ##   .. ..$ : int [1:90] 11 149 29 133 150 27 48 3 26 142 ... ##   ..$ test.inds :List of 4 ##   .. ..$ : int [1:30] 23 22 13 18 10 16 1 30 11 15 ... ##   .. ..$ : int [1:30] 35 46 37 54 31 53 51 58 48 33 ... ##   .. ..$ : int [1:30] 138 146 149 126 123 143 124 139 136 150 ... ##   .. ..$ : int [1:30] 99 104 109 112 108 111 98 115 117 100 ... ##  $ :List of 2 ##   ..$ train.inds:List of 4 ##   .. ..$ : int [1:90] 116 40 83 103 84 97 114 53 57 47 ... ##   .. ..$ : int [1:90] 128 40 83 84 146 141 140 53 57 142 ... ##   .. ..$ : int [1:90] 116 128 83 103 84 146 97 141 140 114 ... ##   .. ..$ : int [1:90] 116 128 40 103 146 97 141 140 114 53 ... ##   ..$ test.inds :List of 4 ##   .. ..$ : int [1:30] 138 146 149 126 123 143 124 139 136 150 ... ##   .. ..$ : int [1:30] 99 104 109 112 108 111 98 115 117 100 ... ##   .. ..$ : int [1:30] 35 46 37 54 31 53 51 58 48 33 ... ##   .. ..$ : int [1:30] 74 68 78 88 67 73 62 85 86 89 ... ##  $ :List of 2 ##   ..$ train.inds:List of 4 ##   .. ..$ : int [1:90] 41 113 18 36 22 108 5 29 120 4 ... ##   .. ..$ : int [1:90] 75 82 41 113 78 36 108 61 120 57 ... ##   .. ..$ : int [1:90] 75 82 113 78 18 22 108 5 29 61 ... ##   .. ..$ : int [1:90] 75 82 41 78 18 36 22 5 29 61 ... ##   ..$ test.inds :List of 4 ##   .. ..$ : int [1:30] 74 68 78 88 67 73 62 85 86 89 ... ##   .. ..$ : int [1:30] 23 22 13 18 10 16 1 30 11 15 ... ##   .. ..$ : int [1:30] 35 46 37 54 31 53 51 58 48 33 ... ##   .. ..$ : int [1:30] 99 104 109 112 108 111 98 115 117 100 ... ##  $ :List of 2 ##   ..$ train.inds:List of 4 ##   .. ..$ : int [1:90] 123 145 109 92 84 103 70 116 90 131 ... ##   .. ..$ : int [1:90] 123 145 30 1 21 109 92 103 116 8 ... ##   .. ..$ : int [1:90] 123 145 30 1 21 84 70 8 4 90 ... ##   .. ..$ : int [1:90] 30 1 21 109 92 84 103 70 116 8 ... ##   ..$ test.inds :List of 4 ##   .. ..$ : int [1:30] 23 22 13 18 10 16 1 30 11 15 ... ##   .. ..$ : int [1:30] 74 68 78 88 67 73 62 85 86 89 ... ##   .. ..$ : int [1:30] 99 104 109 112 108 111 98 115 117 100 ... ##   .. ..$ : int [1:30] 138 146 149 126 123 143 124 139 136 150 ... ##  $ :List of 2 ##   ..$ train.inds:List of 4 ##   .. ..$ : int [1:90] 25 11 12 26 63 138 137 69 14 15 ... ##   .. ..$ : int [1:90] 44 25 33 11 12 48 51 26 34 138 ... ##   .. ..$ : int [1:90] 44 25 33 11 12 48 51 26 63 34 ... ##   .. ..$ : int [1:90] 44 33 48 51 63 34 138 137 69 46 ... ##   ..$ test.inds :List of 4 ##   .. ..$ : int [1:30] 35 46 37 54 31 53 51 58 48 33 ... ##   .. ..$ : int [1:30] 74 68 78 88 67 73 62 85 86 89 ... ##   .. ..$ : int [1:30] 138 146 149 126 123 143 124 139 136 150 ... ##   .. ..$ : int [1:30] 23 22 13 18 10 16 1 30 11 15 ..."},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"resample-descriptions-and-resample-instances","dir":"Articles > Tutorial","previous_headings":"","what":"Resample descriptions and resample instances","title":"Resampling","text":"already mentioned, can specify resampling strategy using function makeResampleDesc(). result rdesc inherits class ResampleDesc (makeResampleDesc()) (short resample description) , principle, contains necessary information resampling strategy including number iterations, proportion training test sets, stratification variables, etc. Given either size data set hand Task(), function makeResampleInstance() draws training test sets according ResampleDesc (makeResampleDesc()). result rin inherits class ResampleInstance (makeResampleInstance()) contains lists index vectors train test sets. ResampleDesc (makeResampleDesc()) passed resample(), instantiated internally. Naturally, also possible pass ResampleInstance (makeResampleInstance()) directly. separation resample descriptions, resample instances, resample() function seems overly complicated, several advantages: Resample instances readily allow paired experiments, comparing performance several learners exactly training test sets. particularly useful want add another method comparison experiment already . Moreover, can store resample instance along data order able reproduce results later . order add resampling methods can simply derive ResampleDesc (makeResampleDesc()) ResampleInstance (makeResampleInstance()) classes, neither touch resample() methods use resampling strategy. Usually, calling makeResampleInstance() train test index sets drawn randomly. Mainly holdout (test sample) estimation might want full control training tests set specify manually. can done using function makeFixedHoldoutInstance().","code":"rdesc = makeResampleDesc(\"CV\", iters = 3) rdesc ## Resample description: cross-validation with 3 iterations. ## Predict: test ## Stratification: FALSE  str(rdesc) ## List of 6 ##  $ fixed      : logi FALSE ##  $ blocking.cv: logi FALSE ##  $ id         : chr \"cross-validation\" ##  $ iters      : int 3 ##  $ predict    : chr \"test\" ##  $ stratify   : logi FALSE ##  - attr(*, \"class\")= chr [1:2] \"CVDesc\" \"ResampleDesc\"  str(makeResampleDesc(\"Subsample\", stratify.cols = \"chas\")) ## List of 8 ##  $ split        : num 0.667 ##  $ id           : chr \"subsampling\" ##  $ iters        : int 30 ##  $ predict      : chr \"test\" ##  $ stratify     : logi FALSE ##  $ stratify.cols: chr \"chas\" ##  $ fixed        : logi FALSE ##  $ blocking.cv  : logi FALSE ##  - attr(*, \"class\")= chr [1:2] \"SubsampleDesc\" \"ResampleDesc\" # Create a resample instance based an a task rin = makeResampleInstance(rdesc, iris.task) rin ## Resample instance for 150 cases. ## Resample description: cross-validation with 3 iterations. ## Predict: test ## Stratification: FALSE  str(rin) ## List of 5 ##  $ desc      :List of 6 ##   ..$ fixed      : logi FALSE ##   ..$ blocking.cv: logi FALSE ##   ..$ id         : chr \"cross-validation\" ##   ..$ iters      : int 3 ##   ..$ predict    : chr \"test\" ##   ..$ stratify   : logi FALSE ##   ..- attr(*, \"class\")= chr [1:2] \"CVDesc\" \"ResampleDesc\" ##  $ size      : int 150 ##  $ train.inds:List of 3 ##   ..$ : int [1:100] 75 43 147 7 74 55 104 111 23 9 ... ##   ..$ : int [1:100] 29 20 74 129 124 111 9 31 5 21 ... ##   ..$ : int [1:100] 29 75 43 147 20 7 129 124 55 104 ... ##  $ test.inds :List of 3 ##   ..$ : int [1:50] 4 5 6 10 15 17 19 20 21 22 ... ##   ..$ : int [1:50] 1 3 7 11 12 14 16 23 27 33 ... ##   ..$ : int [1:50] 2 8 9 13 18 24 25 26 28 30 ... ##  $ group     : Factor w/ 0 levels:  ##  - attr(*, \"class\")= chr \"ResampleInstance\"  # Create a resample instance given the size of the data set rin = makeResampleInstance(rdesc, size = nrow(iris)) str(rin) ## List of 5 ##  $ desc      :List of 6 ##   ..$ fixed      : logi FALSE ##   ..$ blocking.cv: logi FALSE ##   ..$ id         : chr \"cross-validation\" ##   ..$ iters      : int 3 ##   ..$ predict    : chr \"test\" ##   ..$ stratify   : logi FALSE ##   ..- attr(*, \"class\")= chr [1:2] \"CVDesc\" \"ResampleDesc\" ##  $ size      : int 150 ##  $ train.inds:List of 3 ##   ..$ : int [1:100] 38 94 73 82 14 77 75 150 27 85 ... ##   ..$ : int [1:100] 90 82 14 77 75 150 27 56 16 22 ... ##   ..$ : int [1:100] 38 94 73 90 85 36 19 104 127 55 ... ##  $ test.inds :List of 3 ##   ..$ : int [1:50] 3 6 10 12 13 15 19 23 25 26 ... ##   ..$ : int [1:50] 2 7 18 20 29 31 33 35 37 38 ... ##   ..$ : int [1:50] 1 4 5 8 9 11 14 16 17 21 ... ##  $ group     : Factor w/ 0 levels:  ##  - attr(*, \"class\")= chr \"ResampleInstance\"  # Access the indices of the training observations in iteration 3 rin$train.inds[[3]] ##   [1]  38  94  73  90  85  36  19 104 127  55 103  91  44  49 132  59  34  12 ##  [19]  29 145  25  81  33  86  40 117  99  62 112 119 135 125 146  20  37 107 ##  [37] 113  68 149 102 115  74 129 147 130  97 106  76  66  67  50  61   6  72 ##  [55]  42  54  45 111  52 108  95 120 101  63  31  43 141  47  51  89 142  23 ##  [73] 136 148 116 122  10  13  26 126 123   7  60 118 140 139  18  93  71   2 ##  [91]  84  15  35   3 109  79  87 124 114  57 rdesc = makeResampleDesc(\"CV\", iters = 3) rin = makeResampleInstance(rdesc, task = iris.task)  # Calculate the performance of two learners based on the same resample instance r.lda = resample(\"classif.lda\", iris.task, rin, show.info = FALSE) r.rpart = resample(\"classif.rpart\", iris.task, rin, show.info = FALSE) r.lda$aggr ## mmce.test.mean  ##           0.02  r.rpart$aggr ## mmce.test.mean  ##           0.06 rin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150) rin ## Resample instance for 150 cases. ## Resample description: holdout with 0.67 split rate. ## Predict: test ## Stratification: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"aggregating-performance-values","dir":"Articles > Tutorial","previous_headings":"","what":"Aggregating performance values","title":"Resampling","text":"resampling iteration \\(b = 1,\\ldots,B\\) get performance values \\(S(D^{*b}, D \\setminus D^{*b})\\) (measure wish calculate), aggregated overall performance. great majority common resampling strategies (like holdout, cross-validation, subsampling) performance values calculated test data sets measures aggregated taking mean (test.mean(aggregations())). performance Measure (makeMeasure()) mlr corresponding default aggregation method stored slot $aggr. default aggregation measures test.mean(aggregations()). One exception root mean square error (rmse). can change aggregation method Measure (makeMeasure()) via function setAggregation(). available aggregation schemes listed aggregations() documentation page.","code":"# Mean misclassification error mmce$aggr ## Aggregation function: test.mean  mmce$aggr$fun ## function (task, perf.test, perf.train, measure, group, pred) ## mean(perf.test) ## <bytecode: 0xc8eade8> ## <environment: namespace:mlr>  # Root mean square error rmse$aggr ## Aggregation function: test.rmse  rmse$aggr$fun ## function (task, perf.test, perf.train, measure, group, pred) ## sqrt(mean(perf.test^2)) ## <bytecode: 0x23ad72d8> ## <environment: namespace:mlr>"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"example-one-measure-with-different-aggregations","dir":"Articles > Tutorial","previous_headings":"Aggregating performance values","what":"Example: One measure with different aggregations","title":"Resampling","text":"aggregation schemes test.median (aggregations()), test.min (aggregations()), text.max (aggregations()) compute median, minimum, maximum performance values test sets.","code":"## Resampling: cross-validation ## Measures:             mse       mse       mse       mse ## [Resample] iter 1:    29.519658829.519658829.519658829.5196588 ## [Resample] iter 2:    19.585943919.585943919.585943919.5859439 ## [Resample] iter 3:    25.396095325.396095325.396095325.3960953 ##  ## Aggregated Result: mse.test.mean=24.8338993,mse.test.median=25.3960953,mse.test.min=19.5859439,mse.test.max=29.5196588 ## mseTestMedian = setAggregation(mse, test.median) mseTestMin = setAggregation(mse, test.min) mseTestMax = setAggregation(mse, test.max)  mseTestMedian ## Name: Mean of squared errors ## Performance measure: mse ## Properties: regr,req.pred,req.truth ## Minimize: TRUE ## Best: 0; Worst: Inf ## Aggregated by: test.median ## Arguments: ## Note: Defined as: mean((response - truth)^2)  rdesc = makeResampleDesc(\"CV\", iters = 3) r = resample(\"regr.lm\", bh.task, rdesc, measures = list(mse, mseTestMedian, mseTestMin, mseTestMax)) ## Resampling: cross-validation ## Measures:             mse       mse       mse       mse ## [Resample] iter 1:    24.078202624.078202624.078202624.0782026 ## [Resample] iter 2:    29.498307729.498307729.498307729.4983077 ## [Resample] iter 3:    18.689471818.689471818.689471818.6894718 ## ## Aggregated Result: mse.test.mean=24.0886607,mse.test.median=24.0782026,mse.test.min=18.6894718,mse.test.max=29.4983077 ##  r ## Resample Result ## Task: BostonHousing-example ## Learner: regr.lm ## Aggr perf: mse.test.mean=24.0886607,mse.test.median=24.0782026,mse.test.min=18.6894718,mse.test.max=29.4983077 ## Runtime: 0.0288048  r$aggr ##   mse.test.mean mse.test.median    mse.test.min    mse.test.max ##        24.08866        24.07820        18.68947        29.49831"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"example-calculating-the-training-error","dir":"Articles > Tutorial","previous_headings":"Aggregating performance values","what":"Example: Calculating the training error","title":"Resampling","text":"calculate mean misclassification error (mmce) training test data sets. Note set predict = \"\" calling makeResampleDesc() order get predictions training test sets.","code":"mmceTrainMean = setAggregation(mmce, train.mean) rdesc = makeResampleDesc(\"CV\", iters = 3, predict = \"both\") r = resample(\"classif.rpart\", iris.task, rdesc, measures = list(mmce, mmceTrainMean)) ## Resampling: cross-validation ## Measures:             mmce.train   mmce.test ## [Resample] iter 1:    0.0300000    0.0600000 ## [Resample] iter 2:    0.0400000    0.0600000 ## [Resample] iter 3:    0.0300000    0.0600000 ##  ## Aggregated Result: mmce.test.mean=0.0600000,mmce.train.mean=0.0333333 ##   r$measures.train ##   iter mmce mmce ## 1    1 0.03 0.03 ## 2    2 0.04 0.04 ## 3    3 0.03 0.03  r$aggr ##  mmce.test.mean mmce.train.mean  ##      0.06000000      0.03333333"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"example-bootstrap","dir":"Articles > Tutorial","previous_headings":"Aggregating performance values","what":"Example: Bootstrap","title":"Resampling","text":"--bag bootstrap estimation \\(B\\) new data sets \\(D^{*1}, \\ldots, D^{*B}\\) drawn data set \\(D\\) replacement, size \\(D\\). \\(b\\)-th iteration, \\(D^{*b}\\) forms training set, remaining elements \\(D\\), .e., \\(D \\setminus D^{*b}\\), form test set. b632 b632+ variants calculate convex combination training performance --bag bootstrap performance thus require predictions training sets appropriate aggregation strategy.","code":"# Use bootstrap as resampling strategy and predict on both train and test sets rdesc = makeResampleDesc(\"Bootstrap\", predict = \"both\", iters = 10)  # Set aggregation schemes for b632 and b632+ bootstrap mmceB632 = setAggregation(mmce, b632) mmceB632plus = setAggregation(mmce, b632plus)  mmceB632 ## Name: Mean misclassification error ## Performance measure: mmce ## Properties: classif,classif.multi,req.pred,req.truth ## Minimize: TRUE ## Best: 0; Worst: 1 ## Aggregated by: b632 ## Arguments:  ## Note: Defined as: mean(response != truth)  r = resample(\"classif.rpart\", iris.task, rdesc, measures = list(mmce, mmceB632, mmceB632plus),   show.info = FALSE) head(r$measures.train) ##   iter       mmce       mmce       mmce ## 1    1 0.04000000 0.04000000 0.04000000 ## 2    2 0.02666667 0.02666667 0.02666667 ## 3    3 0.04666667 0.04666667 0.04666667 ## 4    4 0.02666667 0.02666667 0.02666667 ## 5    5 0.03333333 0.03333333 0.03333333 ## 6    6 0.02000000 0.02000000 0.02000000  # Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap r$aggr ## mmce.test.mean      mmce.b632  mmce.b632plus  ##     0.05059931     0.04228276     0.04303127"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/resample.html","id":"convenience-functions","dir":"Articles > Tutorial","previous_headings":"","what":"Convenience functions","title":"Resampling","text":"functionality described page allows much control flexibility. However, quickly trying learners, can get tedious type code defining resampling strategy, setting aggregation scheme . mentioned , mlr includes pre-defined resample description objects frequently used strategies like, e.g., 5-fold cross-validation (cv5 (makeResampleDesc())). Moreover, mlr provides special functions common resampling methods, example holdout (resample()), crossval (resample()), bootstrapB632 (resample()).","code":"## Resampling: cross-validation ## Measures:             mmce      ber ## [Resample] iter 1:    0.0400000 0.0444444 ## [Resample] iter 2:    0.0000000 0.0000000 ## [Resample] iter 3:    0.0200000 0.0303030 ##  ## Aggregated Result: mmce.test.mean=0.0200000,ber.test.mean=0.0249158 ##  ## Resampling: OOB bootstrapping ## Measures:             mse.train   mae.train   mse.test    mae.test ## [Resample] iter 1:    23.4927114  3.6008531   25.7782322  3.7183011 ## [Resample] iter 2:    18.5886427  2.9238599   27.5961506  3.7304597 ## [Resample] iter 3:    20.6455357  3.1039182   26.2183904  3.6727966 ##  ## Aggregated Result: mse.b632plus=24.5409802,mae.b632plus=3.5372475 ## crossval(\"classif.lda\", iris.task, iters = 3, measures = list(mmce, ber)) ## Resampling: cross-validation ## Measures:             mmce      ber ## [Resample] iter 1:    0.0200000 0.0238095 ## [Resample] iter 2:    0.0400000 0.0370370 ## [Resample] iter 3:    0.0000000 0.0000000 ## ## Aggregated Result: mmce.test.mean=0.0200000,ber.test.mean=0.0202822 ## ## Resample Result ## Task: iris-example ## Learner: classif.lda ## Aggr perf: mmce.test.mean=0.0200000,ber.test.mean=0.0202822 ## Runtime: 0.0205457  bootstrapB632plus(\"regr.lm\", bh.task, iters = 3, measures = list(mse, mae)) ## Resampling: OOB bootstrapping ## Measures:             mse.train   mae.train   mse.test    mae.test ## [Resample] iter 1:    24.6425511  3.4107320   16.3415466  3.0123263 ## [Resample] iter 2:    17.0963191  2.9809210   29.6056968  3.7131236 ## [Resample] iter 3:    23.1440608  3.5079975   24.4183753  3.3467443 ## ## Aggregated Result: mse.b632plus=22.9359054,mae.b632plus=3.3459751 ## ## Resample Result ## Task: BostonHousing-example ## Learner: regr.lm ## Aggr perf: mse.b632plus=22.9359054,mae.b632plus=3.3459751 ## Runtime: 0.0419419"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"performance-plots-with-plotroccurves","dir":"Articles > Tutorial","previous_headings":"","what":"Performance plots with plotROCCurves","title":"ROC Analysis and Performance Curves","text":"might recall generateThreshVsPerfData() calculates one several performance measures sequence decision thresholds 0 1. provides S3 methods objects class Prediction(), ResampleResult() BenchmarkResult() (resulting predict (predict.WrappedModel()), resample() benchmark()). plotROCCurves() plots result generateThreshVsPerfData() using ggplot2.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"example-1-single-predictions","dir":"Articles > Tutorial","previous_headings":"","what":"Example 1: Single predictions","title":"ROC Analysis and Performance Curves","text":"consider Sonar (mlbench::Sonar()) data set package mlbench, poses binary classification problem (sonar.task()) apply linear discriminant analysis (MASS::lda()). Since want plot ROC curves calculate false true positive rates (fpr tpr). Additionally, also compute error rates (mmce). generateThreshVsPerfData() returns object class ThreshVsPerfData (generateThreshVsPerfData()) contains performance values $data element. Per default, plotROCCurves() plots performance values first two measures passed generateThreshVsPerfData(). first shown x-axis, second y-axis. Moreover, diagonal line represents performance random classifier added. can remove diagonal setting diagonal = FALSE.  corresponding area curve (auc) can calculated usual calling performance(). plotROCCurves() always requires pair performance measures plotted . want plot individual measures versus decision threshold can use function plotThreshVsPerf().  Additional linear discriminant analysis (MASS::lda()) try support vector machine RBF kernel (kernlab::ksvm()). order compare performance two learners might want display two corresponding ROC curves one plot. purpose just pass named list Prediction()s generateThreshVsPerfData().  ’s clear plot kernlab::ksvm() slightly higher AUC lda (MASS::lda()). Based $data member df can easily generate custom plots. curves two learners superposed.  easily possible generate performance plots passing appropriate performance measures generateThreshVsPerfData() plotROCCurves(). , generate precision/recall graph (precision = positive predictive value = ppv, recall = tpr) sensitivity/specificity plot (sensitivity = tpr, specificity = tnr).","code":"n = getTaskSize(sonar.task) train.set = sample(n, size = round(2/3 * n)) test.set = setdiff(seq_len(n), train.set)  lrn1 = makeLearner(\"classif.lda\", predict.type = \"prob\") mod1 = train(lrn1, sonar.task, subset = train.set) pred1 = predict(mod1, task = sonar.task, subset = test.set) df = generateThreshVsPerfData(pred1, measures = list(fpr, tpr, mmce)) plotROCCurves(df) mlr::performance(pred1, mlr::auc) ##       auc  ## 0.7871622 plotThreshVsPerf(df) lrn2 = makeLearner(\"classif.ksvm\", predict.type = \"prob\") mod2 = train(lrn2, sonar.task, subset = train.set) pred2 = predict(mod2, task = sonar.task, subset = test.set) df = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr)) plotROCCurves(df) mlr::performance(pred2, mlr::auc) ##       auc  ## 0.8834459 qplot(x = fpr, y = tpr, color = learner, data = df$data, geom = \"path\") df = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(ppv, tpr, tnr))  # Precision/recall graph plotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE)  # Sensitivity/specificity plot plotROCCurves(df, measures = list(tnr, tpr), diagonal = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"example-2-benchmark-experiment","dir":"Articles > Tutorial","previous_headings":"","what":"Example 2: Benchmark experiment","title":"ROC Analysis and Performance Curves","text":"analysis example can improved little. Instead writing individual code training/prediction learner, can become tedious quickly, can use function benchmark() (see also Benchmark Experiments) , ideally, support vector machine tuned. consider Sonar (mlbench::Sonar()) data set apply MASS::lda() well kernlab::ksvm(). first generate tuning wrapper (makeTuneWrapper()) kernlab::ksvm(). cost parameter tuned (demonstration purposes small) parameter grid. assume interested good performance complete threshold range therefore tune regard auc. error rate (mmce) threshold value 0.5 reported well. actual benchmark experiment conducted. resampling strategy use 5-fold cross-validation calculate auc well error rate (threshold/cutoff value 0.5). Calling generateThreshVsPerfData() plotROCCurves() benchmark result (BenchmarkResult()) produces plot ROC curves learners experiment.  Per default, generateThreshVsPerfData() calculates aggregated performances according chosen resampling strategy (5-fold cross-validation) aggregation scheme (test.mean (aggregations())) threshold sequence. way get threshold-averaged ROC curves. want plot individual ROC curves resample iteration set aggregate = FALSE.  applies plotThreshVsPerf().  alternative averaging just merge 5 test folds draw single ROC curve. Merging can achieved manually changing class attribute prediction objects ResamplePrediction() Prediction(). , predictions extracted BenchmarkResult() via function getBMRPredictions(), class changed ROC curves created. Averaging methods normally preferred (cp. Fawcett, 2006), permit assess variability, needed properly compare classifier performance.  , can easily create standard evaluation plots passing appropriate performance measures generateThreshVsPerfData() plotROCCurves().","code":"# Tune wrapper for ksvm rdesc.inner = makeResampleDesc(\"Holdout\") ms = list(mlr::auc, mmce) ps = makeParamSet(   makeDiscreteParam(\"C\", 2^(-1:1)) ) ctrl = makeTuneControlGrid() lrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE) # Benchmark experiment lrns = list(lrn1, lrn2) rdesc.outer = makeResampleDesc(\"CV\", iters = 5)  bmr = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE) bmr ##         task.id         learner.id auc.test.mean mmce.test.mean ## 1 Sonar-example        classif.lda     0.7950028      0.2547038 ## 2 Sonar-example classif.ksvm.tuned     0.9243248      0.1632985 df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce)) plotROCCurves(df) df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE) plotROCCurves(df) plotThreshVsPerf(df) +   theme(strip.text.x = element_text(size = 7)) # Extract predictions preds = getBMRPredictions(bmr, drop = TRUE)  # Change the class attribute preds2 = lapply(preds, function(x) {class(x) = \"Prediction\"; return(x)})  # Draw ROC curves df = generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce)) plotROCCurves(df)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"performance-plots-with-asrocrprediction","dir":"Articles > Tutorial","previous_headings":"","what":"Performance plots with asROCRPrediction","title":"ROC Analysis and Performance Curves","text":"Drawing performance plots package ROCR works three basic commands: ROCR::prediction(): Create ROCR prediction object. ROCR::performance(): Calculate one performance measures given prediction object. ROCR::plot(): Generate performance plot. mlr’s function asROCRPrediction() converts mlr Prediction() object ROCR prediction (ROCR::prediction-class()) object, can easily generate performance plots steps 2. 3. . ROCR’s plot (ROCR::plot-methods()) method nice features (yet) available plotROCCurves(), example plotting convex hull ROC curves. examples shown .","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"example-1-single-predictions-continued","dir":"Articles > Tutorial","previous_headings":"","what":"Example 1: Single predictions (continued)","title":"ROC Analysis and Performance Curves","text":"go back first example trained predicted MASS::lda() sonar classification task (sonar.task()). use asROCRPrediction() convert lda prediction, let ROCR calculate true false positive rate plot ROC curve.  ROC curve, make use graphical parameters: ROC curve color-coded threshold selected threshold values printed curve. Additionally, convex hull (black broken line) ROC curve drawn.","code":"n = getTaskSize(sonar.task) train.set = sample(n, size = round(2/3 * n)) test.set = setdiff(seq_len(n), train.set)  # Train and predict linear discriminant analysis lrn1 = makeLearner(\"classif.lda\", predict.type = \"prob\") mod1 = train(lrn1, sonar.task, subset = train.set) pred1 = predict(mod1, task = sonar.task, subset = test.set) # Convert prediction ROCRpred1 = asROCRPrediction(pred1)  # Calculate true and false positive rate ROCRperf1 = ROCR::performance(ROCRpred1, \"tpr\", \"fpr\")  # Draw ROC curve ROCR::plot(ROCRperf1) # Draw ROC curve ROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)  # Draw convex hull of ROC curve ch = ROCR::performance(ROCRpred1, \"rch\") ROCR::plot(ch, add = TRUE, lty = 2)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"example-2-benchmark-experiments-continued","dir":"Articles > Tutorial","previous_headings":"","what":"Example 2: Benchmark experiments (continued)","title":"ROC Analysis and Performance Curves","text":"consider benchmark experiment conducted earlier. first extract predictions getBMRPredictions() convert via function asROCRPrediction(). draw vertically averaged ROC curves (solid lines) well ROC curves individual resampling iterations (broken lines). Moreover, standard error bars plotted selected true positive rates (0.1, 0.2, …, 0.9). See ROCR’s plot (ROCR::plot-methods()) function details.  order create evaluation plots like precision/recall graphs just change performance measures calling ROCR::performance(). (Note use measures provided ROCR listed ROCR::performance() mlr’s performance measures.)  want plot performance measure versus threshold, specify one measure calling ROCR::performance(). average accuracy 5 cross-validation iterations plotted threshold. Moreover, boxplots certain threshold values (0.1, 0.2, …, 0.9) drawn.","code":"# Extract predictions preds = getBMRPredictions(bmr, drop = TRUE)  # Convert predictions ROCRpreds = lapply(preds, asROCRPrediction)  # Calculate true and false positive rate ROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \"tpr\", \"fpr\")) # lda average ROC curve plot(ROCRperfs[[1]], col = \"blue\", avg = \"vertical\", spread.estimate = \"stderror\",   show.spread.at = seq(0.1, 0.8, 0.1), plotCI.col = \"blue\", plotCI.lwd = 2, lwd = 2) # lda individual ROC curves plot(ROCRperfs[[1]], col = \"blue\", lty = 2, lwd = 0.25, add = TRUE)  # ksvm average ROC curve plot(ROCRperfs[[2]], col = \"red\", avg = \"vertical\", spread.estimate = \"stderror\",   show.spread.at = seq(0.1, 0.6, 0.1), plotCI.col = \"red\", plotCI.lwd = 2, lwd = 2, add = TRUE) # ksvm individual ROC curves plot(ROCRperfs[[2]], col = \"red\", lty = 2, lwd = 0.25, add = TRUE)  legend(\"bottomright\", legend = getBMRLearnerIds(bmr), lty = 1, lwd = 2, col = c(\"blue\", \"red\")) # Extract and convert predictions preds = getBMRPredictions(bmr, drop = TRUE) ROCRpreds = lapply(preds, asROCRPrediction)  # Calculate precision and recall ROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \"prec\", \"rec\"))  # Draw performance plot plot(ROCRperfs[[1]], col = \"blue\", avg = \"threshold\") plot(ROCRperfs[[2]], col = \"red\", avg = \"threshold\", add = TRUE) legend(\"bottomleft\", legend = getBMRLearnerIds(bmr), lty = 1, col = c(\"blue\", \"red\")) # Extract and convert predictions preds = getBMRPredictions(bmr, drop = TRUE) ROCRpreds = lapply(preds, asROCRPrediction)  # Calculate accuracy ROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \"acc\"))  # Plot accuracy versus threshold plot(ROCRperfs[[1]], avg = \"vertical\", spread.estimate = \"boxplot\", lwd = 2, col = \"blue\",   show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab = \"Threshold\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/roc_analysis.html","id":"viper-charts","dir":"Articles > Tutorial","previous_headings":"","what":"Viper charts","title":"ROC Analysis and Performance Curves","text":"mlr also supports ViperCharts plotting ROC performance curves. Like generateThreshVsPerfData() S3 methods objects class Prediction(), ResampleResult() BenchmarkResult(). plots benchmark experiment (Example 2) generated. Note besides ROC curves get several plots like lift charts cost curves. details, see plotViperCharts().","code":"z = plotViperCharts(bmr, chart = \"rocc\", browse = FALSE) ## Error in plotViperCharts(bmr, chart = \"rocc\", browse = FALSE): could not find function \"plotViperCharts\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"task-types-and-creation","dir":"Articles > Tutorial","previous_headings":"","what":"Task types and creation","title":"Learning Tasks","text":"tasks organized hierarchy, generic Task() top. following tasks can instantiated inherit virtual superclass Task(): RegrTask() regression problems, ClassifTask() binary multi-class classification problems class-dependent costs can handled well), SurvTask() survival analysis, ClusterTask() cluster analysis, MultilabelTask() multilabel classification problems, CostSensTask() general cost sensitive classification (example-specific costs). create task, just call make<TaskType>, e.g., makeClassifTask(). tasks require identifier (argument id) base::data.frame() (argument data). ID provided automatically generated using variable name data. ID later used name results, example benchmark experiments, annotate plots. Depending nature learning problem, additional arguments may required discussed following sections.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"regression","dir":"Articles > Tutorial","previous_headings":"","what":"Regression","title":"Learning Tasks","text":"supervised learning like regression (well classification survival analysis) , addition data, specify name target variable. can see, Task() records type learning problem basic information data set, e.g., types features (base::numeric() vectors, base::factors() ordered factors), number observations, whether missing values present. Creating tasks classification survival analysis follows scheme, data type target variables included data simply different. learning problems specifics described .","code":"data(BostonHousing, package = \"mlbench\") regr.task = makeRegrTask(id = \"bh\", data = BostonHousing, target = \"medv\") regr.task ## Supervised task: bh ## Type: regr ## Target: medv ## Observations: 506 ## Features: ##    numerics     factors     ordered functionals  ##          12           1           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"classification","dir":"Articles > Tutorial","previous_headings":"","what":"Classification","title":"Learning Tasks","text":"classification target column factor. following example define classification task mlbench::BreastCancer() data set exclude variable Id model fitting evaluation. binary classification two classes usually referred positive negative class positive class category greater interest. relevant many performance measures like true positive rate ROC analysis. Moreover, mlr, possible, permits set options (like setThreshold() makeWeightedClassesWrapper()) returns plots results (like class posterior probabilities) positive class . makeClassifTask() default selects first factor level target variable positive class, example benign. Class malignant can manually selected follows:","code":"data(BreastCancer, package = \"mlbench\") df = BreastCancer df$Id = NULL classif.task = makeClassifTask(id = \"BreastCancer\", data = df, target = \"Class\") classif.task ## Supervised task: BreastCancer ## Type: classif ## Target: Class ## Observations: 699 ## Features: ##    numerics     factors     ordered functionals  ##           0           4           5           0  ## Missings: TRUE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ##    benign malignant  ##       458       241  ## Positive class: benign classif.task = makeClassifTask(id = \"BreastCancer\", data = df, target = \"Class\", positive = \"malignant\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"survival-analysis","dir":"Articles > Tutorial","previous_headings":"","what":"Survival analysis","title":"Learning Tasks","text":"Survival tasks use two target columns. left right censored problems consist survival time binary event indicator. interval censored data two target columns must specified \"interval2\" format (see survival::Surv()). type censoring can specified via argument censoring, defaults \"rcens\" right censored data.","code":"data(cancer, package = \"survival\") lung$status = (lung$status == 2) # convert to logical surv.task = makeSurvTask(data = lung, target = c(\"time\", \"status\")) surv.task ## Supervised task: lung ## Type: surv ## Target: time,status ## Events: 165 ## Observations: 228 ## Features: ##    numerics     factors     ordered functionals  ##           8           0           0           0  ## Missings: TRUE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"multilabel-classification","dir":"Articles > Tutorial","previous_headings":"","what":"Multilabel classification","title":"Learning Tasks","text":"multilabel classification object can belong one category time. data expected contain many target columns class labels. target columns logical vectors indicate class labels present. names target columns taken class labels need passed target argument makeMultilabelTask(). following example get data yeast data set, extract label names, pass target argument makeMultilabelTask(). See also tutorial page multilabel.","code":"yeast = getTaskData(yeast.task)  labels = colnames(yeast)[1:14] yeast.task = makeMultilabelTask(id = \"multi\", data = yeast, target = labels) yeast.task ## Supervised task: multi ## Type: multilabel ## Target: label1,label2,label3,label4,label5,label6,label7,label8,label9,label10,label11,label12,label13,label14 ## Observations: 2417 ## Features: ##    numerics     factors     ordered functionals  ##         103           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 14 ##  label1  label2  label3  label4  label5  label6  label7  label8  label9 label10  ##     762    1038     983     862     722     597     428     480     178     253  ## label11 label12 label13 label14  ##     289    1816    1799      34"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"cluster-analysis","dir":"Articles > Tutorial","previous_headings":"","what":"Cluster analysis","title":"Learning Tasks","text":"cluster analysis unsupervised, mandatory argument construct cluster analysis task data. create learning task data set datasets::mtcars().","code":"data(mtcars, package = \"datasets\") cluster.task = makeClusterTask(data = mtcars) cluster.task ## Unsupervised task: mtcars ## Type: cluster ## Observations: 32 ## Features: ##    numerics     factors     ordered functionals  ##          11           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"cost-sensitive-classification","dir":"Articles > Tutorial","previous_headings":"","what":"Cost-sensitive classification","title":"Learning Tasks","text":"standard objective classification obtain high prediction accuracy, .e., minimize number errors. types misclassification errors thereby deemed equally severe. However, many applications different kinds errors cause different costs. case class-dependent costs, solely depend actual predicted class labels, sufficient create ordinary ClassifTask(). order handle example-specific costs necessary generate CostSensTask(). scenario, example \\((x, y)\\) associated individual cost vector length \\(K\\) \\(K\\) denoting number classes. \\(k\\)-th component indicates cost assigning \\(x\\) class \\(k\\). Naturally, assumed cost intended class label \\(y\\) minimal. cost vector contains relevant information intended class \\(y\\), feature values \\(x\\) cost matrix, contains cost vectors examples data set, required create CostSensTask(). following example use datasets::iris() data artificial cost matrix (generated proposed Beygelzimer et al., 2005): details see page cost sensitive classification.","code":"df = iris cost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] df$Species = NULL  costsens.task = makeCostSensTask(data = df, cost = cost) costsens.task ## Supervised task: df ## Type: costsens ## Observations: 150 ## Features: ##    numerics     factors     ordered functionals  ##           4           0           0           0  ## Missings: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 3 ## y1, y2, y3"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"further-settings","dir":"Articles > Tutorial","previous_headings":"","what":"Further settings","title":"Learning Tasks","text":"Task() help page also lists several arguments describe details learning problem. example, include blocking factor task. indicate observations “belong together” separated splitting data training test sets resampling. Another option assign weights observations. can simply indicate observation frequencies result sampling scheme used collect data. Note use option weights really belong task. plan train learning algorithms different weights Task(), mlr offers several ways set observation class weights (supervised classification). See example tutorial page training function makeWeightedClassesWrapper().","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"accessing-a-learning-task","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing a learning task","title":"Learning Tasks","text":"provide many operators access elements stored Task(). important ones listed documentation Task() getTaskData(). access TaskDesc() contains basic information task can use: Note TaskDesc() slightly different elements different types Task()s. Frequently required elements can also accessed directly. Moreover, mlr provides several functions extract data Task(). Note getTaskData() offers many options converting data set convenient format. especially comes handy integrate new learner another R package mlr. regard function getTaskFormula() also useful.","code":"getTaskDesc(classif.task) ## $id ## [1] \"BreastCancer\" ##  ## $type ## [1] \"classif\" ##  ## $target ## [1] \"Class\" ##  ## $size ## [1] 699 ##  ## $n.feat ##    numerics     factors     ordered functionals  ##           0           4           5           0  ##  ## $has.missings ## [1] TRUE ##  ## $has.weights ## [1] FALSE ##  ## $has.blocking ## [1] FALSE ##  ## $has.coordinates ## [1] FALSE ##  ## $class.levels ## [1] \"benign\"    \"malignant\" ##  ## $positive ## [1] \"malignant\" ##  ## $negative ## [1] \"benign\" ##  ## $class.distribution ##  ##    benign malignant  ##       458       241  ##  ## attr(,\"class\") ## [1] \"ClassifTaskDesc\"    \"SupervisedTaskDesc\" \"TaskDesc\" # Get the ID getTaskId(classif.task) ## [1] \"BreastCancer\"  # Get the type of task getTaskType(classif.task) ## [1] \"classif\"  # Get the names of the target columns getTaskTargetNames(classif.task) ## [1] \"Class\"  # Get the number of observations getTaskSize(classif.task) ## [1] 699  # Get the number of input variables getTaskNFeats(classif.task) ## [1] 9  # Get the class levels in classif.task getTaskClassLevels(classif.task) ## [1] \"benign\"    \"malignant\" # Accessing the data set in classif.task str(getTaskData(classif.task)) ## 'data.frame':    699 obs. of  10 variables: ##  $ Cl.thickness   : Ord.factor w/ 10 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 5 5 3 6 4 8 1 2 2 4 ... ##  $ Cell.size      : Ord.factor w/ 10 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 1 4 1 8 1 10 1 1 1 2 ... ##  $ Cell.shape     : Ord.factor w/ 10 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 1 4 1 8 1 10 1 2 1 1 ... ##  $ Marg.adhesion  : Ord.factor w/ 10 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 1 5 1 1 3 8 1 1 1 1 ... ##  $ Epith.c.size   : Ord.factor w/ 10 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 2 7 2 3 2 7 2 2 2 2 ... ##  $ Bare.nuclei    : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 10 2 4 1 10 10 1 1 1 ... ##  $ Bl.cromatin    : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 3 3 3 3 3 9 3 3 1 2 ... ##  $ Normal.nucleoli: Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 1 7 1 7 1 1 1 1 ... ##  $ Mitoses        : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 5 1 ... ##  $ Class          : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...  # Get the names of the input variables in cluster.task getTaskFeatureNames(cluster.task) ##  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\" ## [11] \"carb\"  # Get the values of the target variables in surv.task head(getTaskTargets(surv.task)) ##   time status ## 1  306   TRUE ## 2  455   TRUE ## 3 1010  FALSE ## 4  210   TRUE ## 5  883   TRUE ## 6 1022  FALSE  # Get the cost matrix in costsens.task head(getTaskCosts(costsens.task)) ##      y1        y2         y3 ## [1,]  0 1694.9063 1569.15053 ## [2,]  0  995.0545   18.85981 ## [3,]  0  775.8181 1558.13177 ## [4,]  0  492.8980 1458.78130 ## [5,]  0  222.1929 1260.26371 ## [6,]  0  779.9889  961.82166"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"modifying-a-learning-task","dir":"Articles > Tutorial","previous_headings":"","what":"Modifying a learning task","title":"Learning Tasks","text":"mlr provides several functions alter existing Task(), often convenient creating new Task() scratch. examples. functions detailed explanations look data preprocessing page.","code":"# Select observations and/or features cluster.task = subsetTask(cluster.task, subset = 4:17)  # It may happen, especially after selecting observations, that features are constant. # These should be removed. removeConstantFeatures(cluster.task) ## Removing 1 columns: am ## Unsupervised task: mtcars ## Type: cluster ## Observations: 14 ## Features: ##    numerics     factors     ordered functionals  ##          10           0           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE  # Remove selected features dropFeatures(surv.task, c(\"meal.cal\", \"wt.loss\")) ## Supervised task: lung ## Type: surv ## Target: time,status ## Events: 165 ## Observations: 228 ## Features: ##    numerics     factors     ordered functionals  ##           6           0           0           0  ## Missings: TRUE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE  # Standardize numerical features task = normalizeFeatures(cluster.task, method = \"range\") summary(getTaskData(task)) ##       mpg              cyl              disp              hp         ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   ##  1st Qu.:0.3161   1st Qu.:0.5000   1st Qu.:0.1242   1st Qu.:0.2801   ##  Median :0.5107   Median :1.0000   Median :0.4076   Median :0.6311   ##  Mean   :0.4872   Mean   :0.7143   Mean   :0.4430   Mean   :0.5308   ##  3rd Qu.:0.6196   3rd Qu.:1.0000   3rd Qu.:0.6618   3rd Qu.:0.7473   ##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   ##       drat              wt              qsec              vs         ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   ##  1st Qu.:0.2672   1st Qu.:0.1275   1st Qu.:0.2302   1st Qu.:0.0000   ##  Median :0.3060   Median :0.1605   Median :0.3045   Median :0.0000   ##  Mean   :0.4544   Mean   :0.3268   Mean   :0.3752   Mean   :0.4286   ##  3rd Qu.:0.7026   3rd Qu.:0.3727   3rd Qu.:0.4908   3rd Qu.:1.0000   ##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   ##        am           gear             carb        ##  Min.   :0.5   Min.   :0.0000   Min.   :0.0000   ##  1st Qu.:0.5   1st Qu.:0.0000   1st Qu.:0.3333   ##  Median :0.5   Median :0.0000   Median :0.6667   ##  Mean   :0.5   Mean   :0.2857   Mean   :0.6429   ##  3rd Qu.:0.5   3rd Qu.:0.7500   3rd Qu.:1.0000   ##  Max.   :0.5   Max.   :1.0000   Max.   :1.0000"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/task.html","id":"example-tasks-and-convenience-functions","dir":"Articles > Tutorial","previous_headings":"","what":"Example tasks and convenience functions","title":"Learning Tasks","text":"convenience mlr provides pre-defined Task()s type learning problem. also used throughout tutorial order get shorter readable code. list Task()s can found Appendix. Moreover, mlr’s function convertMLBenchObjToTask() can generate Task()s data sets data generating functions package mlbench::mlbench().","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/train.html","id":"accessing-learner-models","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing learner models","title":"Training a Learner","text":"Function train() returns object class WrappedModel (makeWrappedModel()), encapsulates fitted model, .e., output underlying R learning method. Additionally, contains information Learner (makeLearner()), Task(), features observations used training, training time. WrappedModel (makeWrappedModel()) can subsequently used make prediction (predict.WrappedModel()) new observations. fitted model slot $learner.model WrappedModel (makeWrappedModel()) object can accessed using function getLearnerModel. following example cluster Ruspini (cluster::ruspini()) data set (four groups two features) \\(K\\)-means \\(K = 4\\) extract output underlying stats::kmeans() function.","code":"data(ruspini, package = \"cluster\") plot(y ~ x, ruspini) # Generate the task ruspini.task = makeClusterTask(data = ruspini)  # Generate the learner lrn = makeLearner(\"cluster.kmeans\", centers = 4)  # Train the learner mod = train(lrn, ruspini.task) mod ## Model for learner.id=cluster.kmeans; learner.class=cluster.kmeans ## Trained on: task.id = ruspini; obs = 75; features = 2 ## Hyperparameters: centers=4  # Peak into mod names(mod) ## [1] \"learner\"       \"learner.model\" \"task.desc\"     \"subset\"        ## [5] \"features\"      \"factor.levels\" \"time\"          \"dump\"  mod$learner ## Learner cluster.kmeans from package stats,clue ## Type: cluster ## Name: K-Means; Short name: kmeans ## Class: cluster.kmeans ## Properties: numerics,prob ## Predict-Type: response ## Hyperparameters: centers=4  mod$features ## [1] \"x\" \"y\"  # Extract the fitted model getLearnerModel(mod) ## K-means clustering with 4 clusters of sizes 23, 17, 20, 15 ##  ## Cluster means: ##          x        y ## 1 43.91304 146.0435 ## 2 98.17647 114.8824 ## 3 20.15000  64.9500 ## 4 68.93333  19.4000 ##  ## Clustering vector: ##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  ##  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  1  1  1  1  1  1  ## 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  ##  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  ## 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  ##  2  2  2  2  2  2  2  2  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  ##  ## Within cluster sum of squares by cluster: ## [1] 3176.783 4558.235 3689.500 1456.533 ##  (between_SS / total_SS =  94.7 %) ##  ## Available components: ##  ## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\" ## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/train.html","id":"further-options-and-comments","dir":"Articles > Tutorial","previous_headings":"","what":"Further options and comments","title":"Training a Learner","text":"default, whole data set Task() used training. subset argument train() takes logical integer vector indicates observations use, example want split data training test set want fit separate models different subgroups data. fit linear regression model (stats::lm()) BostonHousing (mlbench::BostonHousing()) data set (bh.task()) randomly select 1/3 data set training. Note, later, standard resampling strategies supported. Therefore usually subset data . Moreover, learner supports , can specify observation weights reflect relevance observations training process. Weights can useful many regards, example express reliability training observations, reduce influence outliers , data collected longer time period, increase influence recent data. supervised classification weights can used incorporate misclassification costs account class imbalance. example mlbench::BreastCancer() data set class benign almost twice frequent class malignant. order grant classes equal importance training classifier can weight examples according inverse class frequencies data set shown following R code. Note, later, mlr offers much functionality deal imbalanced classification problems. another side remark advanced readers: varying weights calls train(), also implement variant general boosting type algorithm arbitrary mlr base learners. may recall, also possible set observation weights creating Task(). general rule, specify make*Task (Task()) weights really “belong” task always used. Otherwise, pass train(). weights train() take precedence weights Task().","code":"# Get the number of observations n = getTaskSize(bh.task)  # Use 1/3 of the observations for training train.set = sample(n, size = n/3)  # Train the learner mod = train(\"regr.lm\", bh.task, subset = train.set) mod ## Model for learner.id=regr.lm; learner.class=regr.lm ## Trained on: task.id = BostonHousing-example; obs = 168; features = 13 ## Hyperparameters: # Calculate the observation weights target = getTaskTargets(bc.task) tab = as.numeric(table(target)) w = 1/tab[target]  train(\"classif.rpart\", task = bc.task, weights = w) ## Model for learner.id=classif.rpart; learner.class=classif.rpart ## Trained on: task.id = BreastCancer-example; obs = 683; features = 9 ## Hyperparameters: xval=0"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/tune.html","id":"specifying-the-search-space","dir":"Articles > Tutorial","previous_headings":"","what":"Specifying the search space","title":"Tuning Hyperparameters","text":"first must define space search tuning learner. example, maybe want tune several specific values hyperparameter perhaps want define space \\(10^{-10}\\) \\(10^{10}\\) let optimization algorithm decide points choose. order define search space, create ParamSet (ParamHelpers::makeParamSet()) object, describes parameter space wish search. done via function ParamHelpers::makeParamSet(). example, define search space just values 0.5, 1.0, 1.5, 2.0 C gamma. Notice name parameter ’s defined kernlab package: also define continuous search space (using makeNumericParam (ParamHelpers::makeNumericParam()) instead makeDiscreteParam (ParamHelpers::makeDiscreteParam())) \\(10^{-10}\\) \\(10^{10}\\) parameters use trafo argument (trafo short transformation). Transformations work like : optimizers basically see parameters original scale (\\(-10\\) \\(10\\) case) produce values scale search. Right passed learning algorithm, transformation function applied. Notice time use makeNumericParam (ParamHelpers::makeNumericParam()): Many parameters can created, check examples ParamHelpers::makeParamSet(). order standardize workflow across several packages, whenever parameters underlying R functions passed list structure, mlr tries give direct access parameter get rid list structure! case kpar argument kernlab::ksvm()) list kernel parameters like sigma. allows us interface learners different packages way defining parameters tune!","code":"discrete_ps = makeParamSet(   makeDiscreteParam(\"C\", values = c(0.5, 1.0, 1.5, 2.0)),   makeDiscreteParam(\"sigma\", values = c(0.5, 1.0, 1.5, 2.0)) ) print(discrete_ps) ##           Type len Def      Constr Req Tunable Trafo ## C     discrete   -   - 0.5,1,1.5,2   -    TRUE     - ## sigma discrete   -   - 0.5,1,1.5,2   -    TRUE     - num_ps = makeParamSet(   makeNumericParam(\"C\", lower = -10, upper = 10, trafo = function(x) 10^x),   makeNumericParam(\"sigma\", lower = -10, upper = 10, trafo = function(x) 10^x) )"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/tune.html","id":"specifying-the-optimization-algorithm","dir":"Articles > Tutorial","previous_headings":"","what":"Specifying the optimization algorithm","title":"Tuning Hyperparameters","text":"Now specified search space, need choose optimization algorithm parameters pass kernlab::ksvm()) learner. Optimization algorithms considered TuneControl() objects mlr. grid search one standard – albeit slow – ways choose appropriate set parameters given search space. case discrete_ps , since manually specified values, grid search simply cross product. create grid search object using defaults, noting \\(4 \\times 4 = 16\\) combinations case discrete_ps: case num_ps , since specified upper lower bounds search space, grid search create grid using equally-sized steps. default, grid search span space 10 equal-sized steps. number steps can changed resolution argument. change 15 equal-sized steps space defined within ParamSet (ParamHelpers::makeParamSet()) object. num_ps, means 15 steps form 10 ^ seq(-10, 10, length.= 15): Many types optimization algorithms available. Check TuneControl() examples. Since grid search normally slow practice, ’ll also examine random search. case discrete_ps, random search randomly choose specified values. maxit argument controls amount iterations. case num_ps, random search randomly choose points within space according specified bounds. Perhaps case want increase amount iterations ensure adequately cover space:","code":"ctrl = makeTuneControlGrid() ctrl = makeTuneControlGrid(resolution = 15L) ctrl = makeTuneControlRandom(maxit = 10L) ctrl = makeTuneControlRandom(maxit = 200L)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/tune.html","id":"performing-the-tuning","dir":"Articles > Tutorial","previous_headings":"","what":"Performing the tuning","title":"Tuning Hyperparameters","text":"Now specified search space optimization algorithm, ’s time perform tuning. need define resampling strategy make note performance measure. use 3-fold cross-validation assess quality specific parameter setting. need create resampling description just like resampling part tutorial. Finally, combining previous pieces, can tune SVM parameters calling tuneParams(). use discrete_ps grid search: tuneParams() simply performs cross-validation every element cross-product selects parameter setting best mean performance. performance measure specified, default error rate (mmce) used. Note measure (makeMeasure()) “knows” minimized maximized tuning. course, can pass measures also list measures tuneParams(). latter case first measure optimized tuning, others simply evaluated. interested optimizing several measures simultaneously look Advanced Tuning. example calculate accuracy (acc) instead error rate. use function setAggregation(), described resampling page, additionally obtain standard deviation accuracy. also use random search 100 iterations num_set defined set show.info FALSE hide output 100 iterations:","code":"rdesc = makeResampleDesc(\"CV\", iters = 3L) discrete_ps = makeParamSet(   makeDiscreteParam(\"C\", values = c(0.5, 1.0, 1.5, 2.0)),   makeDiscreteParam(\"sigma\", values = c(0.5, 1.0, 1.5, 2.0)) ) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"CV\", iters = 3L) res = tuneParams(\"classif.ksvm\", task = iris.task, resampling = rdesc,   par.set = discrete_ps, control = ctrl) ## [Tune] Started tuning learner classif.ksvm for parameter set: ##           Type len Def      Constr Req Tunable Trafo ## C     discrete   -   - 0.5,1,1.5,2   -    TRUE     - ## sigma discrete   -   - 0.5,1,1.5,2   -    TRUE     - ## With control class: TuneControlGrid ## Imputation value: 1 ## [Tune-x] 1: C=0.5; sigma=0.5 ## [Tune-y] 1: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 2: C=1; sigma=0.5 ## [Tune-y] 2: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 3: C=1.5; sigma=0.5 ## [Tune-y] 3: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 4: C=2; sigma=0.5 ## [Tune-y] 4: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 5: C=0.5; sigma=1 ## [Tune-y] 5: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune-x] 6: C=1; sigma=1 ## [Tune-y] 6: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 7: C=1.5; sigma=1 ## [Tune-y] 7: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 8: C=2; sigma=1 ## [Tune-y] 8: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 9: C=0.5; sigma=1.5 ## [Tune-y] 9: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune-x] 10: C=1; sigma=1.5 ## [Tune-y] 10: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune-x] 11: C=1.5; sigma=1.5 ## [Tune-y] 11: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune-x] 12: C=2; sigma=1.5 ## [Tune-y] 12: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune-x] 13: C=0.5; sigma=2 ## [Tune-y] 13: mmce.test.mean=0.0600000; time: 0.0 min ## [Tune-x] 14: C=1; sigma=2 ## [Tune-y] 14: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune-x] 15: C=1.5; sigma=2 ## [Tune-y] 15: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune-x] 16: C=2; sigma=2 ## [Tune-y] 16: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune] Result: C=1.5; sigma=0.5 : mmce.test.mean=0.0400000  res ## Tune result: ## Op. pars: C=1.5; sigma=0.5 ## mmce.test.mean=0.0400000 # error rate mmce$minimize ## [1] TRUE  # accuracy acc$minimize ## [1] FALSE num_ps = makeParamSet(   makeNumericParam(\"C\", lower = -10, upper = 10, trafo = function(x) 10^x),   makeNumericParam(\"sigma\", lower = -10, upper = 10, trafo = function(x) 10^x) ) ctrl = makeTuneControlRandom(maxit = 100L) res = tuneParams(\"classif.ksvm\", task = iris.task, resampling = rdesc, par.set = num_ps,   control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE) res ## Tune result: ## Op. pars: C=15.5; sigma=0.0345 ## acc.test.mean=0.9600000,acc.test.sd=0.0000000"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/tune.html","id":"accessing-the-tuning-result","dir":"Articles > Tutorial","previous_headings":"","what":"Accessing the tuning result","title":"Tuning Hyperparameters","text":"result object TuneResult() allows access best found settings $x estimated performance $y. can generate Learner (makeLearner()) optimal hyperparameter settings follows: can proceed usual. refit predict learner complete iris (datasets::iris()) data set: wanted inspect points search path, just optimal?","code":"res$x ## $C ## [1] 15.52092 ##  ## $sigma ## [1] 0.03449146  res$y ## acc.test.mean   acc.test.sd  ##          0.96          0.00 lrn = setHyperPars(makeLearner(\"classif.ksvm\"), C = res$x$C, sigma = res$x$sigma) lrn ## Learner classif.ksvm from package kernlab ## Type: classif ## Name: Support Vector Machines; Short name: ksvm ## Class: classif.ksvm ## Properties: twoclass,multiclass,numerics,factors,prob,class.weights ## Predict-Type: response ## Hyperparameters: fit=FALSE,C=15.5,sigma=0.0345 m = train(lrn, iris.task) predict(m, task = iris.task)  ## Prediction: 150 observations ## predict.type: response ## threshold:  ## time: 0.00 ##   id  truth response ## 1  1 setosa   setosa ## 2  2 setosa   setosa ## 3  3 setosa   setosa ## 4  4 setosa   setosa ## 5  5 setosa   setosa ## 6  6 setosa   setosa ## ... (#rows: 150, #cols: 3)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/tune.html","id":"investigating-hyperparameter-tuning-effects","dir":"Articles > Tutorial","previous_headings":"","what":"Investigating hyperparameter tuning effects","title":"Tuning Hyperparameters","text":"can inspect points evaluated search using generateHyperParsEffectData(): Note result generateHyperParsEffectData() contains parameter values original scale. order get transformed parameter values instead, use trafo argument: Note can also generate performance train data along validation/test data, discussed resampling tutorial page: can also easily visualize points evaluated using plotHyperParsEffect(). example , plot performance iterations, using res previous section instead 2 performance measures:  Note default, plot current global optima. can changed global.argument. -depth exploration generating hyperparameter tuning effects plotting data, check Hyperparameter Tuning Effects.","code":"generateHyperParsEffectData(res) ## HyperParsEffectData: ## Hyperparameters: C,sigma ## Measures: acc.test.mean,acc.test.sd ## Optimizer: TuneControlRandom ## Nested CV Used: FALSE ## Snapshot of data: ##            C      sigma acc.test.mean acc.test.sd iteration exec.time ## 1  2.3397773 -3.5124226     0.9466667  0.01154701         1     0.027 ## 2  4.0521330 -7.6211727     0.4200000  0.19078784         2     0.029 ## 3 -0.9581277 -7.0855536     0.4200000  0.19078784         3     0.029 ## 4 -9.7456136 -0.5559055     0.4200000  0.19078784         4     0.030 ## 5 -1.1553551  1.1788559     0.3600000  0.08717798         5     0.032 ## 6  2.3742768  0.2438430     0.9533333  0.01154701         6     0.030 generateHyperParsEffectData(res, trafo = TRUE) ## HyperParsEffectData: ## Hyperparameters: C,sigma ## Measures: acc.test.mean,acc.test.sd ## Optimizer: TuneControlRandom ## Nested CV Used: FALSE ## Snapshot of data: ##              C        sigma acc.test.mean acc.test.sd iteration exec.time ## 1 2.186640e+02 3.073105e-04     0.9466667  0.01154701         1     0.027 ## 2 1.127543e+04 2.392364e-08     0.4200000  0.19078784         2     0.029 ## 3 1.101216e-01 8.211953e-08     0.4200000  0.19078784         3     0.029 ## 4 1.796331e-10 2.780318e-01     0.4200000  0.19078784         4     0.030 ## 5 6.992701e-02 1.509579e+01     0.3600000  0.08717798         5     0.032 ## 6 2.367428e+02 1.753247e+00     0.9533333  0.01154701         6     0.030 rdesc2 = makeResampleDesc(\"Holdout\", predict = \"both\") res2 = tuneParams(\"classif.ksvm\", task = iris.task, resampling = rdesc2, par.set = num_ps,   control = ctrl, measures = list(acc, setAggregation(acc, train.mean)), show.info = FALSE) generateHyperParsEffectData(res2) ## HyperParsEffectData: ## Hyperparameters: C,sigma ## Measures: acc.test.mean,acc.train.mean ## Optimizer: TuneControlRandom ## Nested CV Used: FALSE ## Snapshot of data: ##           C     sigma acc.test.mean acc.train.mean iteration exec.time ## 1  0.496548 -4.788487          0.64           0.68         1     0.017 ## 2  9.095208  8.350293          0.34           1.00         2     0.017 ## 3 -1.904618  3.195934          0.34           0.68         3     0.019 ## 4  6.863312  1.695167          0.54           1.00         4     0.018 ## 5 -1.270327 -6.763619          0.64           0.68         5     0.017 ## 6 -8.247310  5.305003          0.32           0.68         6     0.018 res = tuneParams(\"classif.ksvm\", task = iris.task, resampling = rdesc, par.set = num_ps,   control = ctrl, measures = list(acc, mmce), show.info = FALSE) data = generateHyperParsEffectData(res) plotHyperParsEffect(data, x = \"iteration\", y = \"acc.test.mean\",   plot.type = \"line\")"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/tune.html","id":"further-comments","dir":"Articles > Tutorial","previous_headings":"","what":"Further comments","title":"Tuning Hyperparameters","text":"Tuning works tasks like regression, survival analysis completely similar fashion. longer running tuning experiments annoying computation stops due numerical errors. look .learner.error configureMlr() well examples given section configure mlr tutorial. might also want inform impute.val TuneControl(). continually optimize data tuning, estimated performance value might optimistically biased. clean approach ensure unbiased performance estimation nested resampling, embed whole model selection process outer resampling loop.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/usecase_regression.html","id":"define-a-task","dir":"Articles > Tutorial","previous_headings":"","what":"Define a task","title":"Use case: Regression","text":"Now, let us continue defining regression task. order get overview features type, can print regr.task. shows 12 numeric one factor variables data set.","code":"# Make a task regr.task = makeRegrTask(data = BostonHousing, target = \"medv\") regr.task ## Supervised task: BostonHousing ## Type: regr ## Target: medv ## Observations: 506 ## Features: ##    numerics     factors     ordered functionals  ##          12           1           0           0  ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/usecase_regression.html","id":"tuning","dir":"Articles > Tutorial","previous_headings":"","what":"Tuning","title":"Use case: Regression","text":"calling listLearners(\"regr\") can see learners available regression task. many learners difficult choose one optimal specific task. choose sample learners compare results. analysis uses classical linear regression model (regr.lm), SVM (kernlab::ksvm()) radial basis kernel (regr.ksvm) random forest ranger (ranger::ranger()) package (regr.ranger). order get quick overview learner-specific tunable parameters can call getLearnerParamSet() alias ParamHelpers::getParamSet(), list learner’s hyperparameters properties. setting benchmark experiment can specify hyperparameters going tuned. mlr package provides powerful tuning algorithms, iterated F-racing (irace::irace()), CMA Evolution Strategy (cmaes::cma_es()), model-based / Bayesian optimization (mlrMBO::mbo()) generalized simulated annealing (GenSA::GenSA()). See Tuning Advanced Tuning details. learner one hyperparameter tuned, .e. kernel parameter sigma SVM model number trees (num.trees) random forest model. start specifying search space parameters. makeTuneControlCMAES() set tuning method CMA Evolution Strategy (cmaes::cma_es()). Afterwards take 5-fold cross validation resampling strategy root mean squared error (rmse) optimization criterion. Finally, make tuning wrapper learner.","code":"set.seed(1234)  # Define a search space for each learner'S parameter ps_ksvm = makeParamSet(   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x) )  ps_rf = makeParamSet(   makeIntegerParam(\"num.trees\", lower = 1L, upper = 200L) )  # Choose a resampling strategy rdesc = makeResampleDesc(\"CV\", iters = 5L)  # Choose a performance measure meas = rmse  # Choose a tuning method ctrl = makeTuneControlCMAES(budget = 100L)  # Make tuning wrappers tuned.ksvm = makeTuneWrapper(learner = \"regr.ksvm\", resampling = rdesc, measures = meas,   par.set = ps_ksvm, control = ctrl, show.info = FALSE) tuned.rf = makeTuneWrapper(learner = \"regr.ranger\", resampling = rdesc, measures = meas,   par.set = ps_rf, control = ctrl, show.info = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/usecase_regression.html","id":"benchmark-experiment","dir":"Articles > Tutorial","previous_headings":"","what":"Benchmark Experiment","title":"Use case: Regression","text":"order conduct benchmark experiment, necessary choose evaluation method. use resampling strategy performance measure previous section pass tuning wrappers arguments benchmark() function.","code":"# Four learners to be compared lrns = list(makeLearner(\"regr.lm\"), tuned.ksvm, tuned.rf)  # Conduct the benchmark experiment bmr = benchmark(learners = lrns, tasks = regr.task, resamplings = rdesc, measures = rmse,    show.info = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/usecase_regression.html","id":"performance","dir":"Articles > Tutorial","previous_headings":"","what":"Performance","title":"Use case: Regression","text":"Now want evaluate results. closer look boxplot reveals RF outperforms learners specific task. Despite tuning procedure performed , benchmark experiment linear lasso regression yields similar poor results.","code":"getBMRAggrPerformances(bmr) ## $BostonHousing ## $BostonHousing$regr.lm ## rmse.test.rmse  ##       4.841535  ##  ## $BostonHousing$regr.ksvm.tuned ## rmse.test.rmse  ##       3.775318  ##  ## $BostonHousing$regr.ranger.tuned ## rmse.test.rmse  ##       3.510068 plotBMRBoxplots(bmr)"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/visualization.html","id":"generation-and-plotting-functions","dir":"Articles > Tutorial","previous_headings":"","what":"Generation and plotting functions","title":"Visualization","text":"mlr’s visualization capabilities rely generation functions generate data plots, plotting functions plot output using either ggplot2::ggplot2() ggvis::ggvis() (latter currently experimental). separation allows users easily make custom visualizations taking advantage generation functions. data transformation handled inside plotting functions reshaping. reshaped data also accessible calling plotting functions extracting data ggplot2::ggplot() object. functions named accordingly. Names generation functions start generate followed title-case description FunctionPurpose, followed Data, .e., generateFunctionPurposeData. functions output objects class FunctionPurposeData. Plotting functions prefixed plot followed purpose, .e., plotFunctionPurpose. ggvis::ggvis() plotting functions additional suffix GGVIS, .e., plotFunctionPurposeGGVIS.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/visualization.html","id":"some-examples","dir":"Articles > Tutorial","previous_headings":"","what":"Some examples","title":"Visualization","text":"example create plot classifier performance function decision threshold binary classification problem sonar.task. generation function generateThreshVsPerfData() creates object class ThreshVsPerfData contains data plot slot $data. plotting can use built-mlr function plotThreshVsPerf().  Note default Measure names used annotate panels. apply plotThreshVsPerf(), plot functions show performance measures well, example plotLearningCurve(). can use ids instead names setting pretty.names = FALSE.","code":"lrn = makeLearner(\"classif.lda\", predict.type = \"prob\") n = getTaskSize(sonar.task) mod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2)) pred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2)) d = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))  class(d) ## [1] \"ThreshVsPerfData\"  head(d$data) ##         fpr       fnr      mmce  threshold ## 1 1.0000000 0.0000000 0.4615385 0.00000000 ## 2 0.3541667 0.1964286 0.2692308 0.01010101 ## 3 0.3333333 0.2321429 0.2788462 0.02020202 ## 4 0.3333333 0.2321429 0.2788462 0.03030303 ## 5 0.3333333 0.2321429 0.2788462 0.04040404 ## 6 0.3125000 0.2321429 0.2692308 0.05050505 plotThreshVsPerf(d) fpr$name ## [1] \"False positive rate\"  fpr$id ## [1] \"fpr\""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/visualization.html","id":"customizing-plots","dir":"Articles > Tutorial","previous_headings":"","what":"Customizing plots","title":"Visualization","text":"mentioned easily possible customize built-plots making visualizations scratch based generated data. probably come often changing labels annotations. Generally, can done manipulating ggplot2::ggplot() object, example object returned plotThreshVsPerf(), using usual ggplot2::ggplot2() functions like ggplot2::labs() ggplot2::labeller(). Moreover, can change underlying data, either d$data (resulting generateThreshVsPerfData() possibly reshaped data contained ggplot2::ggplot() object (resulting plotThreshVsPerf(), often renaming columns factor levels. two examples alter axis panel labels plot. Imagine want change order panels also satisfied panel names, example find “Mean misclassification error” long prefer “Error rate” instead. Moreover, want error rate displayed first.  Using ggplot2::labeller() function requires calling ggplot2::facet_wrap() (ggplot2::facet_grid()), can useful want change panels positioned (number rows columns) influence axis limits.  Instead using built-function plotThreshVsPerf() also manually create plot based output generateThreshVsPerfData(): case plot one measure.  decoupling generation plotting functions especially practical prefer traditional graphics::graphics() lattice::lattice(). lattice::lattice() plot gives result similar plotThreshVsPerf().  Let’s conclude brief look second example. use plotPartialDependence() extract data ggplot2::ggplot() object plt use create traditional graphics::plot(), additional ggplot2::ggplot() plot.","code":"plt = plotThreshVsPerf(d, pretty.names = FALSE)  # Reshaped version of the underlying data d head(plt$data) ##    threshold measure performance ## 1 0.00000000     fpr   1.0000000 ## 2 0.01010101     fpr   0.3541667 ## 3 0.02020202     fpr   0.3333333 ## 4 0.03030303     fpr   0.3333333 ## 5 0.04040404     fpr   0.3333333 ## 6 0.05050505     fpr   0.3125000  levels(plt$data$measure) ## [1] \"fpr\"  \"fnr\"  \"mmce\"  # Rename and reorder factor levels plt$data$measure = factor(plt$data$measure, levels = c(\"mmce\", \"fpr\", \"fnr\"),   labels = c(\"Error rate\", \"False positive rate\", \"False negative rate\")) plt = plt + xlab(\"Cutoff\") + ylab(\"Performance\") plt plt = plotThreshVsPerf(d, pretty.names = FALSE)  measure_names = c(   fpr = \"False positive rate\",   fnr = \"False negative rate\",   mmce = \"Error rate\" ) # Manipulate the measure names via the labeller function and # arrange the panels in two columns and choose common axis limits for all panels plt = plt + facet_wrap( ~ measure, labeller = labeller(measure = measure_names), ncol = 2) plt = plt + xlab(\"Decision threshold\") + ylab(\"Performance\") plt ggplot(d$data, aes(threshold, fpr)) + geom_line() lattice::xyplot(fpr + fnr + mmce ~ threshold, data = d$data, type = \"l\", ylab = \"performance\",   outer = TRUE, scales = list(relation = \"free\"),   strip = strip.custom(factor.levels = sapply(d$measures, function(x) x$name),     par.strip.text = list(cex = 0.8))) sonar = getTaskData(sonar.task) pd = generatePartialDependenceData(mod, sonar, \"V11\") ## Loading required package: mmpf plt = plotPartialDependence(pd) head(plt$data) ##           M Feature     Value ## 1 0.2737158     V11 0.0289000 ## 2 0.3689970     V11 0.1072667 ## 3 0.4765742     V11 0.1856333 ## 4 0.5741233     V11 0.2640000 ## 5 0.6557857     V11 0.3423667 ## 6 0.7387962     V11 0.4207333  plt plot(M ~ Value, data = plt$data, type = \"b\", xlab = plt$data$Feature[1])"},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/visualization.html","id":"available-generation-and-plotting-functions","dir":"Articles > Tutorial","previous_headings":"","what":"Available generation and plotting functions","title":"Visualization","text":"currently available generation plotting functions listed tutorial pages provide depth descriptions listed functions referenced. Note plots, e.g., plotTuneMultiCritResult() mentioned since lack generation function. plotThreshVsPerf() plotROCCurves() operate result generateThreshVsPerfData(). Functions plotPartialDependence() plotPartialDependenceGGVIS() can applied results generatePartialDependenceData() generateFunctionalANOVAData(). ggvis::ggvis() functions experimental subject change, though work. generate interactive shiny::shiny() applications, automatically start run locally.","code":""},{"path":"https://mlr.mlr-org.com/dev/articles/tutorial/wrapper.html","id":"example-bagging-wrapper","dir":"Articles > Tutorial","previous_headings":"","what":"Example: Bagging wrapper","title":"Wrapper","text":"section exemplary describe bagging wrapper create random forest supports weights. achieve combine several decision trees rpart package create custom random forest. First, create weighted toy task. Next, use makeBaggingWrapper() create base learners bagged learner. choose set equivalents ntree (100 base learners) mtry (proportion randomly selected features). can see output, wrapped learner inherited properties base learner, especially “weights” attribute still present. can use newly constructed learner like base learners, .e. can use train(), benchmark(), resample(), etc. far quite happy new learner. hope better performance tuning hyperparameters decision trees bagging wrapper. Let’s look available hyperparameters fused learner: choose tune parameters minsplit bw.feats mmce using random search (TuneControl()) 3-fold CV: Calling train method newly constructed learner performs following steps: tuning wrapper sets parameters underlying model slot $next.learner calls train method. Next learner bagging wrapper. passed argument bw.feats used bagging wrapper training function, argument minsplit gets passed $next.learner. base wrapper function calls base learner bw.iters times stores resulting models. bagged models evaluated using mean mmce (default aggregation performance measure) new parameters selected using tuning method. repeated tuner terminates. Output tuned bagged learner.","code":"data(iris) task = makeClassifTask(data = iris, target = \"Species\", weights = as.integer(iris$Species)) base.lrn = makeLearner(\"classif.rpart\") wrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5) print(wrapped.lrn) ## Learner classif.rpart.bagged from package rpart ## Type: classif ## Name: ; Short name:  ## Class: BaggingWrapper ## Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp ## Predict-Type: response ## Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5 benchmark(tasks = task, learners = list(base.lrn, wrapped.lrn)) ## Task: iris, Learner: classif.rpart ## Resampling: cross-validation ## Measures:             mmce ## [Resample] iter 1:    0.0000000 ## [Resample] iter 2:    0.2000000 ## [Resample] iter 3:    0.2000000 ## [Resample] iter 4:    0.0666667 ## [Resample] iter 5:    0.0666667 ## [Resample] iter 6:    0.0000000 ## [Resample] iter 7:    0.0666667 ## [Resample] iter 8:    0.0000000 ## [Resample] iter 9:    0.1333333 ## [Resample] iter 10:   0.0666667 ##  ## Aggregated Result: mmce.test.mean=0.0800000 ##  ## Task: iris, Learner: classif.rpart.bagged ## Resampling: cross-validation ## Measures:             mmce ## [Resample] iter 1:    0.0000000 ## [Resample] iter 2:    0.2000000 ## [Resample] iter 3:    0.0666667 ## [Resample] iter 4:    0.0666667 ## [Resample] iter 5:    0.0000000 ## [Resample] iter 6:    0.0000000 ## [Resample] iter 7:    0.0000000 ## [Resample] iter 8:    0.0000000 ## [Resample] iter 9:    0.0666667 ## [Resample] iter 10:   0.0666667 ##  ## Aggregated Result: mmce.test.mean=0.0466667 ##  ##   task.id           learner.id mmce.test.mean ## 1    iris        classif.rpart     0.08000000 ## 2    iris classif.rpart.bagged     0.04666667 getParamSet(wrapped.lrn) ##                    Type len   Def   Constr Req Tunable Trafo ## bw.iters        integer   -    10 1 to Inf   -    TRUE     - ## bw.replace      logical   -  TRUE        -   -    TRUE     - ## bw.size         numeric   -     -   0 to 1   -    TRUE     - ## bw.feats        numeric   - 0.667   0 to 1   -    TRUE     - ## minsplit        integer   -    20 1 to Inf   -    TRUE     - ## minbucket       integer   -     - 1 to Inf   -    TRUE     - ## cp              numeric   -  0.01   0 to 1   -    TRUE     - ## maxcompete      integer   -     4 0 to Inf   -    TRUE     - ## maxsurrogate    integer   -     5 0 to Inf   -    TRUE     - ## usesurrogate   discrete   -     2    0,1,2   -    TRUE     - ## surrogatestyle discrete   -     0      0,1   -    TRUE     - ## maxdepth        integer   -    30  1 to 30   -    TRUE     - ## xval            integer   -    10 0 to Inf   -   FALSE     - ## parms           untyped   -     -        -   -    TRUE     - ctrl = makeTuneControlRandom(maxit = 10) rdesc = makeResampleDesc(\"CV\", iters = 3) par.set = makeParamSet(   makeIntegerParam(\"minsplit\", lower = 1, upper = 10),   makeNumericParam(\"bw.feats\", lower = 0.25, upper = 1) ) tuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl) print(tuned.lrn) ## Learner classif.rpart.bagged.tuned from package rpart ## Type: classif ## Name: ; Short name:  ## Class: TuneWrapper ## Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass,featimp ## Predict-Type: response ## Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5 lrn = train(tuned.lrn, task = task) ## [Tune] Started tuning learner classif.rpart.bagged for parameter set: ##             Type len Def    Constr Req Tunable Trafo ## minsplit integer   -   -   1 to 10   -    TRUE     - ## bw.feats numeric   -   - 0.25 to 1   -    TRUE     - ## With control class: TuneControlRandom ## Imputation value: 1 ## [Tune-x] 1: minsplit=5; bw.feats=0.533 ## [Tune-y] 1: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune-x] 2: minsplit=3; bw.feats=0.377 ## [Tune-y] 2: mmce.test.mean=0.0600000; time: 0.0 min ## [Tune-x] 3: minsplit=8; bw.feats=0.29 ## [Tune-y] 3: mmce.test.mean=0.0800000; time: 0.0 min ## [Tune-x] 4: minsplit=6; bw.feats=0.555 ## [Tune-y] 4: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune-x] 5: minsplit=9; bw.feats=0.699 ## [Tune-y] 5: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune-x] 6: minsplit=6; bw.feats=0.985 ## [Tune-y] 6: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune-x] 7: minsplit=10; bw.feats=0.632 ## [Tune-y] 7: mmce.test.mean=0.0400000; time: 0.0 min ## [Tune-x] 8: minsplit=1; bw.feats=0.943 ## [Tune-y] 8: mmce.test.mean=0.0600000; time: 0.0 min ## [Tune-x] 9: minsplit=9; bw.feats=0.715 ## [Tune-y] 9: mmce.test.mean=0.0533333; time: 0.0 min ## [Tune-x] 10: minsplit=1; bw.feats=0.503 ## [Tune-y] 10: mmce.test.mean=0.0466667; time: 0.0 min ## [Tune] Result: minsplit=10; bw.feats=0.632 : mmce.test.mean=0.0400000  print(lrn) ## Model for learner.id=classif.rpart.bagged.tuned; learner.class=TuneWrapper ## Trained on: task.id = iris; obs = 150; features = 4 ## Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5"},{"path":"https://mlr.mlr-org.com/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bernd Bischl. Author. Michel Lang. Author. Lars Kotthoff. Author. Patrick Schratz. Author, maintainer. Julia Schiffner. Author. Jakob Richter. Author. Zachary Jones. Author. Giuseppe Casalicchio. Author. Mason Gallo. Author. Jakob Bossek. Contributor. Erich Studerus. Contributor. Leonard Judt. Contributor. Tobias Kuehn. Contributor. Pascal Kerschke. Contributor. Florian Fendt. Contributor. Philipp Probst. Contributor. Xudong Sun. Contributor. Janek Thomas. Contributor. Bruno Vieira. Contributor. Laura Beggel. Contributor. Quay Au. Contributor. Martin Binder. Contributor. Florian Pfisterer. Contributor. Stefan Coors. Contributor. Steve Bronder. Contributor. Alexander Engelhardt. Contributor. Christoph Molnar. Contributor. Annette Spooner. Contributor.","code":""},{"path":"https://mlr.mlr-org.com/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bischl B, Lang M, Kotthoff L, Schiffner J, Richter J, Studerus E, Casalicchio G, Jones Z (2016). “mlr: Machine Learning R.” Journal Machine Learning Research, 17(170), 1-5. https://jmlr.org/papers/v17/15-066.html. Lang M, Kotthaus H, Marwedel P, Weihs C, Rahnenfuehrer J, Bischl B (2014). “Automatic model selection high-dimensional survival analysis.” Journal Statistical Computation Simulation, 85(1), 62-76. Bischl B, Kuehn T, Szepannek G (2016). “Class Imbalance Correction Classification Algorithms Credit Scoring.” Operations Research Proceedings 2014, 37-43. Springer. Bischl B, Richter J, Bossek J, Horn D, Thomas J, Lang M (2017). “mlrMBO: Modular Framework Model-Based Optimization Expensive Black-Box Functions.” arXiv preprint arXiv:1703.03373. Probst P, Au Q, Casalicchio G, Stachl C, Bischl B (2017). “Multilabel Classification R Package mlr.” arXiv preprint arXiv:1703.08991. Casalicchio G, Bossek J, Lang M, Kirchhoff D, Kerschke P, Hofner B, Seibold H, Vanschoren J, Bischl B (2017). “OpenML: R package connect machine learning platform OpenML.” Computational Statistics, 1-15.","code":"@Article{mlr,   title = {{mlr}: Machine Learning in R},   author = {Bernd Bischl and Michel Lang and Lars Kotthoff and Julia Schiffner and Jakob Richter and Erich Studerus and Giuseppe Casalicchio and Zachary M. Jones},   journal = {Journal of Machine Learning Research},   year = {2016},   volume = {17},   number = {170},   pages = {1-5},   url = {https://jmlr.org/papers/v17/15-066.html}, } @Article{automatic,   title = {Automatic model selection for high-dimensional survival analysis},   author = {Michel Lang and Helena Kotthaus and Peter Marwedel and Claus Weihs and Joerg Rahnenfuehrer and Bernd Bischl},   journal = {Journal of Statistical Computation and Simulation},   year = {2014},   volume = {85},   number = {1},   pages = {62-76},   publisher = {Taylor & Francis}, } @InCollection{bischl2016class,   title = {On Class Imbalance Correction for Classification Algorithms in Credit Scoring},   author = {Bernd Bischl and Tobias Kuehn and Gero Szepannek},   booktitle = {Operations Research Proceedings 2014},   pages = {37-43},   year = {2016},   publisher = {Springer}, } @Article{mlrmbo,   title = {mlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions},   author = {Bernd Bischl and Jakob Richter and Jakob Bossek and Daniel Horn and Janek Thomas and Michel Lang},   journal = {arXiv preprint arXiv:1703.03373},   year = {2017}, } @Article{multilabel,   title = {Multilabel Classification with R Package mlr},   author = {Philipp Probst and Quay Au and Giuseppe Casalicchio and Clemens Stachl and Bernd Bischl},   journal = {arXiv preprint arXiv:1703.08991},   year = {2017}, } @Article{openml,   title = {OpenML: An R package to connect to the machine learning platform OpenML},   author = {Giuseppe Casalicchio and Jakob Bossek and Michel Lang and Dominik Kirchhoff and Pascal Kerschke and Benjamin Hofner and Heidi Seibold and Joaquin Vanschoren and Bernd Bischl},   journal = {Computational Statistics},   pages = {1-15},   year = {2017},   publisher = {Springer}, }"},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"mlr-","dir":"","previous_headings":"","what":"Machine Learning in R","title":"Machine Learning in R","text":"Package website: release | dev Machine learning R. CRAN release site Online tutorial Cheatsheet Changelog Stackoverflow: #mlr Mattermost Blog","code":""},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"deprecated","dir":"","previous_headings":"","what":"Deprecated","title":"Machine Learning in R","text":"{mlr} considered retired mlr-org team. won’t add new features anymore fix severe bugs. suggest use new mlr3 framework now future projects. features {mlr} already implemented {mlr3}. missing crucial feature, please open issue respective mlr3 extension package hesitate follow-.","code":""},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Machine Learning in R","text":"Release Development","code":"install.packages(\"mlr\") remotes::install_github(\"mlr-org/mlr\")"},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"citing-mlr-in-publications","dir":"","previous_headings":"","what":"Citing {mlr} in publications","title":"Machine Learning in R","text":"Please cite JMLR paper [bibtex]. parts package created part publications. use parts, please cite relevant work appropriately. overview {mlr} related publications can found .","code":""},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Machine Learning in R","text":"R define standardized interface machine-learning algorithms. Therefore, non-trivial experiments, need write lengthy, tedious error-prone wrappers call different algorithms unify respective output. Additionally need implement infrastructure resample models optimize hyperparameters select features cope pre- post-processing data compare models statistically meaningful way. becomes computationally expensive, might want parallelize experiments well. often forces users make crummy trade-offs experiments due time constraints lacking expert programming skills. {mlr} provides infrastructure can focus experiments! framework provides supervised methods like classification, regression survival analysis along corresponding evaluation optimization methods, well unsupervised methods like clustering. written way can extend deviate implemented convenience methods construct complex experiments algorithms. Furthermore, package nicely connected OpenML R package online platform, aims supporting collaborative machine learning online allows easily share datasets well machine learning tasks, algorithms experiments order support reproducible research.","code":""},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Machine Learning in R","text":"Clear S3 interface R classification, regression, clustering survival analysis methods Abstract description learners tasks properties Convenience methods generic building blocks machine learning experiments Resampling methods like bootstrapping, cross-validation subsampling Extensive visualizations (e.g. ROC curves, predictions partial predictions) Simplified benchmarking across data sets learners iterated F-racing (irace) sequential model-based optimization Variable selection filters wrappers Nested resampling models tuning feature selection Cost-sensitive learning, threshold tuning imbalance correction Wrapper mechanism extend learner functionality complex ways Possibility combine different processing steps complex data mining chain can jointly optimized OpenML connector Open Machine Learning server Built-parallelization Detailed tutorial","code":""},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"miscellaneous","dir":"","previous_headings":"","what":"Miscellaneous","title":"Machine Learning in R","text":"Simple usage questions better suited Stackoverflow using mlr tag. Please note us work academia put lot work project - simply like , paid . New development efforts go {mlr3}. style guide can easily applied using mlr_style styler package. See wiki information.","code":""},{"path":"https://mlr.mlr-org.com/dev/index.html","id":"talks-workshops-etc","dir":"","previous_headings":"","what":"Talks, Workshops, etc.","title":"Machine Learning in R","text":"mlr-outreach holds outreach activities related {mlr} {mlr3}.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-1.1.html","id":"mlr-11-18","dir":"News","previous_headings":"","what":"mlr 1.1-18:","title":"Version 1.1","text":"CRAN release: 2013-08-30 Initial release CRAN","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.0.html","id":"mlr-20","dir":"News","previous_headings":"","what":"mlr 2.0:","title":"Version 2.0","text":"CRAN release: 2014-07-04 mlr now supports survival analysis models (experimental) mlr now supports cost-sensitive learning example-specific costs experimental) example tasks data sets added simple access added FeatSelWrapper getFeatSelResult performance functions now allows compute multiple measures added multiclass.roc performance measure observation weights can now also specified task added option .learner.warning configureMlr suppress warnings learners fixed bug stratified CV elements distributed evenly possible split number divide number observation added class.weights param classif.svm add fix.factors.prediction option randomForest generic standard error estimation randomForest BaggingWrapper added fixup.data option task constructors, basic data cleanup can performed show.info now option configureMlr learners now support taggable properties can queried changed. also see . listLearners(forTask) unified removed tuning via R’ optim method (makeTuneControlOptim), optimizers really make sense tuning Grid search improved one discretize parameters manually anymore (although still possible). Instead one now passes ‘resolution’ argument. Internally now use ParamHelpers::generateGridDesign . toy tasks added convenient usage: iris.task, sonar.task, bh.task also also corresponding resampling instances, directly start working, e.g., iris.rin","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.0.html","id":"new-learners-2-0","dir":"News","previous_headings":"mlr 2.0:","what":"new learners:","title":"Version 2.0","text":"classif.knn classif.IBk classif.LiblineaRBinary classif.LiblineaRLogReg classif.LiblineaRMultiClass classif.linDA classif.plr classif.plsDA classif.rrlda regr.crs regr.IBk regr.mob surv.CoxBoost surv.coxph surv.glmboost surv.glmnet surv.penalized surv.randomForestSRC","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.0.html","id":"new-measures-2-0","dir":"News","previous_headings":"mlr 2.0:","what":"new measures","title":"Version 2.0","text":"multiauc cindex meancosts, mcp","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.0.html","id":"new-functions-2-0","dir":"News","previous_headings":"mlr 2.0:","what":"new functions","title":"Version 2.0","text":"removeConstantFeatures, normalizeFeatures, dropFeatures, createDummyFeatures getTaskNFeats hasProperties, getProperties, setProperties, addProperties, removeProperties showHyperPars setId listMeasures isFailureModel plotLearnerPrediction plotThreshVsPerf holdout, subsample, crossval, repcv, bootstrapOOB, bootstrapB632, bootstrapB632plus listFilterMethods, getFilterValues, filterFeatures, makeFilterWrapper, plotFilterValues benchmark getPerformances, getAggrPerformances, getPredictions, getFilterResult, getTuneResult, getFeatSelResult oversample, undersample, makeOversampleWrapper, makeUndersampleWrapper smote, makeSmoteWrapper downsample, makeDownsampleWrapper makeWeightedClassesWrapper makeTuneControlGenSA makeModelMultiplexer, makeModelMultiplexerParamSet makeCostSensTask, makeCostSensClassifWrapper, makeCostSensRegrWrapper, makeCostsSensWeightedPairsLearner makeSurvTask impute, reimpute, makeImputeWrapper, lots impute, makeImputeMethod","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.1.html","id":"mlr-21","dir":"News","previous_headings":"","what":"mlr 2.1:","title":"Version 2.1","text":"CRAN release: 2014-07-21 mlr now supports multi-criteria tuning mlr now supports cluster analysis (experimental) improve makeWeightedClassesWrapper: Hyperparams class weighting now supported, . removed fix.factors option randomForest, added general makeLearner, now works learners. Helps feature factor levels dropped newdata prediction data.frames consistent results tuning algorithms parameters “trafos” : always return optimal settings transformed scale, opt.path original scale. fix bug feature filtering resulted NoFeatureModel resample now returns data.frame “err.mgs” error messages might occurred resampling stratified resampling survival","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.1.html","id":"new-learners-2-1","dir":"News","previous_headings":"mlr 2.1:","what":"new learners:","title":"Version 2.1","text":"classif.cforest classif.glmnet classif.plsdaCaret regr.cforest regr.glmnet regr.svm surv.cforest cluster.SimpleKMeans cluster.EM cluster.XMeans","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.1.html","id":"new-measures-2-1","dir":"News","previous_headings":"mlr 2.1:","what":"new measures","title":"Version 2.1","text":"bac db, dunn, g1, g2, silhouette","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.1.html","id":"new-functions-2-1","dir":"News","previous_headings":"mlr 2.1:","what":"new functions","title":"Version 2.1","text":"makeClusterTask removeHyperPars tuneParamsMultiCrit makeTuneMultiCritControlGrid, makeTuneMultiCritControlRandom, makeTuneMultiCritControlNSGA2 plotTuneMultiCritResult getFailureModelMsg","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"mlr-210","dir":"News","previous_headings":"","what":"mlr 2.10:","title":"Version 2.10","text":"CRAN release: 2017-02-07","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"functions---general-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"functions - general","title":"Version 2.10","text":"fixed bug resample using predict = “train” (issue #1284) update irace 2.0 – algorithmic changes irace may affect performance generateFilterValuesData: fixed bug wrt feature ordering imputeLearner: fixed bug data actually contained NAs print.Learner: learner hyperpar set value “NA” displayed printer makeLearner, setHyperPars: mistype learner hyperpar name, mlr uses fuzzy matching suggest 3 closest names message tuneParams: tuning irace now also parallelized, .e., different learner configs evaluated parallel. benchmark: mini fix, arg ‘learners’ now also accepts class strings object printers: mlr printers show head previews data.frames. now also print info total nr rows cols less confusing aggregations: better properties now, know whether require training test set evals filter methods better R docs filter randomForestSRC.var.select: new arg “method” filter mrmr: fixed smaller bugs updated properties generateLearningCurveData: also accepts single learner, require list plotThreshVsPerf: added “measures” arg plotPartialDependence: can create tile plots joint partial dependence two features multiclass classification facetting across classes generatePartialDependenceData generateFunctionalANOVAData: expanded “fun” argument allow calculation weights new “?mlrFamilies” manual page lists families functions belonging converging data.table standard internally, change API behavior outside, though generateHyperParsEffectData plotHyperParsEffect now support 2 hyperparameters linear.correlation, rank.correlation, anova.test: use Rfast instead FSelector/custom implementation now, performance much better use colAUC function instead ROCR package AUC calculation improve performance output resample performance messages every iteration now performance improvements auc measure createDummyFeatures supports vectors now removed pretty.names argument plotHyperParsEffect – labels can set though normal ggplot2 functions returned object Fixed bad bug resample, slot “runtime” ResampleResult, runtime measured seconds e.g. mins. R measures potentially mins, mlr claimed seconds. New “dummy” learners (disregard features completely) can fitted now baseline comparisons, see “featureless” learners .","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"functions---new-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"functions - new","title":"Version 2.10","text":"filter: randomForest.importance generateFeatureImportanceData: permutation-based feature importance local importance getFeatureImportanceLearner: new Learner API function getFeatureImportance: top level function extract feature importance information calculateROCMeasures calculateConfusionMatrix: new confusion-matrix like function calculates tables many receiver operator measures makeLearners: create multiple learners getLearnerId, getLearnerType, getLearnerPredictType, getLearnerPackages getLearnerParamSet, getLearnerParVals getRRPredictionList addRRMeasure plotResiduals getLearnerShortName mergeBenchmarkResults","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"functions---renamed-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"functions - renamed","title":"Version 2.10","text":"Renamed rf.importance filter (now deprecated) randomForestSRC.var.rfsrc Renamed rf.min.depth filter (now deprecated) randomForestSRC.var.select Renamed getConfMatrix (now deprecated) calculateConfusionMatrix Renamed setId (now deprecated) setLearnerId","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"functions---removed-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"functions - removed","title":"Version 2.10","text":"mergeBenchmarkResultLearner, mergeBenchmarkResultTask","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"learners---general-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"learners - general","title":"Version 2.10","text":"classif.ada: fixed param problem rpart.control params classif.cforest, regr.cforest, surv.cforest: removed parameters “minprob”, “pvalue”, “randomsplits” set internally changed user regr.GPfit: params correlation kernel classif.xgboost, regr.xgboost: can now properly handle NAs (property missing problems), added “colsample_bylevel” parameter adapted {classif,regr,surv}.ranger parameters new ranger version","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"learners---new-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"learners - new","title":"Version 2.10","text":"multilabel.cforest surv.gbm regr.cvglmnet {classif,regr,surv}.gamboost classif.earth {classif,regr}.evtree {classif,regr}.evtree","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"learners---removed-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"learners - removed","title":"Version 2.10","text":"classif.randomForestSRCSyn, regr.randomForestSRCSyn: due continued stability issues","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.10.html","id":"measures---new-2-10","dir":"News","previous_headings":"mlr 2.10:","what":"measures - new","title":"Version 2.10","text":"ssr, qsr, lsr rrse, rae, mape kappa, wkappa msle, rmsle","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"mlr-211","dir":"News","previous_headings":"","what":"mlr 2.11:","title":"Version 2.11","text":"CRAN release: 2017-03-15","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"general-2-11","dir":"News","previous_headings":"mlr 2.11:","what":"general","title":"Version 2.11","text":"internal class naming task descriptions changed causing probable incompatibilities tasks generated old versions. New option .error.dump include dumps can inspected debugger errors mlr now supports tuning Bayesian optimization mlrMBO","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"functions---general-2-11","dir":"News","previous_headings":"mlr 2.11:","what":"functions - general","title":"Version 2.11","text":"tuneParams: fixed small obscure bug logging extremely large ParamSets getBMR-operators: now support “drop” argument simplifies resulting list configureMlr: added option “.measure..applicable” handle situations performance calculated one wants NA instead error - useful , e.g., larger benchmarks tuneParams, selectFeatures: removed memory stats default output performance reasons (can restored using control object “log.fun” = “memory”) listLearners: change check.packages default FALSE tuneParams tuneParamsMultiCrit: new parameter resample.fun specify custom resampling function use. Deprecated: getTaskDescription, getBMRTaskDescriptions, getRRTaskDescription. New names: getTaskDesc, getBMRTaskDescs, getRRTaskDesc.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"functions---new-2-11","dir":"News","previous_headings":"mlr 2.11:","what":"functions - new","title":"Version 2.11","text":"getOOBPreds: get --bag predictions trained models learners store – learners new “oobpreds” property listTaskTypes, listLearnerProperties getMeasureProperties, hasMeasureProperties, listMeasureProperties makeDummyFeaturesWrapper: fuse learner dummy feature creator simplifyMeasureNames: shorten measure names actual measure, e.g. mmce.test.mean -> mmce getFailureModelDump, getPredictionDump, getRRDump: get error dumps batchmark: Function run benchmarks batchtools package high performance computing clusters makeTuneControlMBO: allows Bayesian optimization","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"measures---new-2-11","dir":"News","previous_headings":"mlr 2.11:","what":"measures - new","title":"Version 2.11","text":"kendalltau, spearmanrho","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"learners---general-2-11","dir":"News","previous_headings":"mlr 2.11:","what":"learners - general","title":"Version 2.11","text":"classif.plsdaCaret: added parameter “method”. regr.randomForest: refactored se-estimation code, improved docs default now se.method = “jackknife”. regr.xgboost, classif.xgboost: removed “factors” property learners handle categorical features – factors silently converted integers internally, may misinterpret structure data glmnet: control parameters reset factory settings applying custom settings training set back factory afterwards","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.11.html","id":"learners---removed-2-11","dir":"News","previous_headings":"mlr 2.11:","what":"learners - removed","title":"Version 2.11","text":"{classif,regr}.avNNet: longer necessary, mlr contains bagging wrapper","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"mlr-212","dir":"News","previous_headings":"","what":"mlr 2.12:","title":"Version 2.12","text":"CRAN release: 2018-03-10","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"general-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"general","title":"Version 2.12","text":"Support functional data (fda) using matrix columns added. Relaxed way wrappers can nested – explicitly forbidden combination wrap tuning wrapper around another optimization wrapper Refactored resample progress messages give better overview distinguish train test measures better calculateROCMeasures now returns absolute instead relative values Added support spatial data providing spatial partitioning methods “SpCV” “SpRepCV”. Added new spatial.task classification task. Added new spam.task classification task. Classification tasks now store class distribution class.distribution member. mlr now predicts NA data contains NA learners support missing values. Tasks now subsetted “train” function factor levels (classification tasks) based subset. means factor level distribution necessarily entire task, task descriptions models resampling reflect respective subset, task description resample predictions reflect entire task necessarily task individual model. Added support growing fixed window cross-validation forecasting new resample methods “GrowingWindowCV” “FixedWindowCV”.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"functions---general-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"functions - general","title":"Version 2.12","text":"generatePartialDependenceData: depends now “mmpf” package, removed parameter: “center”, “resample”, “fmin”, “fmax” “gridsize” added parameter: “uniform” “n” configure grid partial dependence plot batchmark: allow resample instances reduction partial results resample, performance: new flag “na.rm” remove NAs aggregation plotTuneMultiCritResultGGVIS: new parameters “point.info” “point.trafo” control interactivity calculateConfusionMatrix: new parameter “set” specify whether confusion matrix computed “train”, “test”, “” (default) PlotBMRSummary: Add parameter “shape” plotROCCurves: Add faceting argument PreprocWrapperCaret: Add param “ppc.corr”, “ppc.zv”, “ppc.nzv”, “ppc.n.comp”, “ppc.cutoff”, “ppc.freqCut”, “ppc.uniqueCut”","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"functions---new-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"functions - new","title":"Version 2.12","text":"makeClassificationViaRegressionWrapper getPredictionTaskDesc helpLearner, helpLearnerParam: open help learner get description parameters setMeasurePars makeFunctionalData hasFunctionalFeatures extractFDAFeatures, reextractFDAFeatures extractFDAFourier, extractFDAFPCA, extractFDAMultiResFeatures, extractFDAWavelets makeExtractFDAFeatMethod makeExtractFDAFeatsWrapper getTuneResultOptPath makeTuneMultiCritControlMBO: Allows model based multi-critera / multi-objective optimization using mlrMBO","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"functions---removed-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"functions - removed","title":"Version 2.12","text":"Removed plotViperCharts","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"measures---general-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"measures - general","title":"Version 2.12","text":"measure “arsq” now ID “arsq” measure “measureMultiLabelF1” renamed “measureMultilabelF1” consistency","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"measures---new-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"measures - new","title":"Version 2.12","text":"measureBER, measureRMSLE, measureF1 cindex.uno, iauc.uno","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"learners---general-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"learners - general","title":"Version 2.12","text":"unified {classif,regr,surv}.penalized{ridge,lasso,fusedlasso} {classif,regr,surv}.penalized fixed bug surv.cforest gave wrong risk predictions (#1833) fixed bug classif.xgboost returned NA predictions multi:softmax classif.lda learner: add ‘prior’ hyperparameter ranger: update hyperpar ‘respect.unordered.factors’, add ‘extratrees’ ‘num.random.splits’ h20deeplearning: Rename hyperpar ‘MeanSquare’ ‘Quadratic’ h20*: Add support “missings”","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"learners---new-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"learners - new","title":"Version 2.12","text":"classif.adaboostm1 classif.fdaknn classif.fdakernel classif.fdanp classif.fdaglm classif.mxff regr.fdaFDboost regr.mxff","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"learners---removed-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"learners - removed","title":"Version 2.12","text":"{classif,regr}.bdk: broke API, stability issues {classif,regr}.xyf: broke API, stability issues classif.hdrda: package removed CRAN surv.penalized: stability issues","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"aggregations---new-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"aggregations - new","title":"Version 2.12","text":"testgroup.sd","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.12.html","id":"filter---new-2-12","dir":"News","previous_headings":"mlr 2.12:","what":"filter - new","title":"Version 2.12","text":"auc ranger.permutation, ranger.impurity","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.13.html","id":"mlr-213","dir":"News","previous_headings":"","what":"mlr 2.13:","title":"Version 2.13","text":"CRAN release: 2018-08-28","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.13.html","id":"general-2-13","dir":"News","previous_headings":"mlr 2.13:","what":"general","title":"Version 2.13","text":"Disabled unit tests CRAN, test travis now Suppress messages show.learner.output = FALSE","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.13.html","id":"functions---general-2-13","dir":"News","previous_headings":"mlr 2.13:","what":"functions - general","title":"Version 2.13","text":"plotHyperParsEffect: add colors","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.13.html","id":"functions---new-2-13","dir":"News","previous_headings":"mlr 2.13:","what":"functions - new","title":"Version 2.13","text":"getResamplingIndices createSpatialResamplingPlots","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.13.html","id":"learners---general-2-13","dir":"News","previous_headings":"mlr 2.13:","what":"learners - general","title":"Version 2.13","text":"regr.nnet: Removed unneeded params linout, entropy, softmax censored regr.ranger: Add weight handling","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.13.html","id":"learners---removed-2-13","dir":"News","previous_headings":"mlr 2.13:","what":"learners - removed","title":"Version 2.13","text":"{classif,regr}.blackboost: broke API new release regr.elmNN : package removed CRAN classif.lqa : package removed CRAN","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"mlr-2140","dir":"News","previous_headings":"","what":"mlr 2.14.0","title":"Version 2.14","text":"CRAN release: 2019-04-25","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"general-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"general","title":"Version 2.14","text":"add option use fully predefined indices resampling (makeResampleDesc(fixed = TRUE)) (@pat-s, #2412). Task help pages now split separate ones, e.g. RegrTask, ClassifTask (@pat-s, #2564)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"functions---new-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"functions - new","title":"Version 2.14","text":"deleteCacheDir(): Clear default mlr cache directory (@pat-s, #2463) getCacheDir(): Return default mlr cache directory (@pat-s, #2463)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"functions---general-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"functions - general","title":"Version 2.14","text":"getResamplingIndices(inner = TRUE) now correctly returns inner indices (inner indices referred subset respective outer level train set) (@pat-s, #2413).","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"filter---general-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"filter - general","title":"Version 2.14","text":"Caching now used generating filter values. means filter values computed specific setting stored cache used subsequent iterations. change inherits significant speed-tuning fw.perc, fw.abs fw.threshold. can triggered new cache argument makeFilterWrapper() filterFeatures() (@pat-s, #2463).","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"filter---new-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"filter - new","title":"Version 2.14","text":"praznik_JMI praznik_DISR praznik_JMIM praznik_MIM praznik_NJMIM praznik_MRMR praznik_CMIM FSelectorRcpp_gain.ratio FSelectorRcpp_information.gain FSelectorRcpp_symuncert Additionally, filter names harmonized using following scheme: _. Exeptions filters included base R packages. case, package name omitted.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"filter---general-2-14-0-1","dir":"News","previous_headings":"mlr 2.14.0","what":"filter - general","title":"Version 2.14","text":"Added filters FSelectorRcpp_gain.ratio, FSelectorRcpp_information.gain FSelectorRcpp_symmetrical.uncertainty package FSelectorRcpp. filters ~ 100 times faster implementation FSelector pkg. Please note implementations things slightly different internally FSelectorRcpp methods seen direct replacement FSelector pkg. filter names harmonized using following scheme: _. (@pat-s, #2533) information.gain -> FSelector_information.gain gain.ratio -> FSelector_gain.ratio symmetrical.uncertainty -> FSelector_symmetrical.uncertainty chi.squared -> FSelector_chi.squared relief -> FSelector_relief oneR -> FSelector_oneR randomForestSRC.rfsrc -> randomForestSRC_importance randomForestSRC.var.select -> randomForestSRC_var.select randomForest.importance -> randomForest_importance fixed bug related loading namespaces required filter packages (@pat-s, #2483)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"learners---new-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"learners - new","title":"Version 2.14","text":"classif.liquidSVM (@PhilippPro, #2428) regr.liquidSVM (@PhilippPro, #2428)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"learners---general-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"learners - general","title":"Version 2.14","text":"regr.h2o.gbm: Various parameters added, \"h2o.use.data.table\" = TRUE now default (@j-hartshorn, #2508) h2o learners now support getting feature importance (@markusdumke, #2434)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"learners---fixes-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"learners - fixes","title":"Version 2.14","text":"cases optimized hyperparameters applied performance level nested CV (@berndbischl, #2479)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.14.html","id":"featsel---general-2-14-0","dir":"News","previous_headings":"mlr 2.14.0","what":"featSel - general","title":"Version 2.14","text":"FeatSelResult object now contains additional slot x.bit.names stores optimal bits slot x now always contains real feature names bit.names fixes bug makes makeFeatSelWrapper usable custom bit.names. Fixed bug due sffs crashed cases (@bmihaljevic, #2486)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"mlr-2150","dir":"News","previous_headings":"","what":"mlr 2.15.0","title":"Version 2.15","text":"CRAN release: 2019-08-06","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"breaking-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"Breaking","title":"Version 2.15","text":"Instead wide data.frame filter values now returned long (tidy) tibble. makes easier apply post-processing methods (like group_by(), etc) (@pat-s, #2456) benchmark() store tuning results ($extract slot) anymore default. want keep slot (e.g. post tuning analysis), set keep.extract = TRUE. change originated fact size BenchmarkResult objects extensive tuning got large (~ GB) can cause memory problems runtime multiple benchmark() calls executed HPCs. benchmark() store created models ($models slot) anymore default. reason $extract slot . Storing can enabled using models = TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"functions---general-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"functions - general","title":"Version 2.15","text":"generateFeatureImportanceData() gains argument show.info shows name current feature calculated, index queue elapsed time feature (@pat-s, #26222)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"learners---general-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"learners - general","title":"Version 2.15","text":"classif.liquidSVM regr.liquidSVM removed liquidSVM removed CRAN. fixed bug caused incorrect aggregation probabilities cases. bug existed since quite time exposed due change data.tables default rbindlist(). See #2578 information. (@mllg, #2579) se.method = \"jackknife\" se.method = \"bootstrap\" se.method = \"sd\" See ?regr.randomForest details. regr.ranger relies functions provided package (“jackknife” “infjackknife” (default)) (@jakob-r, #1784) regr.gbm now supports quantile distribution (@bthieurmel, #2603) classif.plsdaCaret now supports multiclass classification (@GegznaV, #2621)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"functions---general-2-15-0-1","dir":"News","previous_headings":"mlr 2.15.0","what":"functions - general","title":"Version 2.15","text":"getClassWeightParam() now also works Wrapper* Models ensemble models (@ja-thomas, #891) added getLearnerNote() query “Note” slot learner (@alona-sydorova, #2086) e1071::svm() now uses formula interface factors present. change supposed prevent “stack overflow” issues users encountered using large datasets. See #1738 information. (@mb706, #1740)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"learners---new-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"learners - new","title":"Version 2.15","text":"add learner cluster.MiniBatchKmeans package ClusterR (@Prasiddhi, #2554)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"function---general-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"function - general","title":"Version 2.15","text":"plotHyperParsEffect() now supports facet visualization hyperparam effects nested cv (@MasonGallo, #1653) fixed bug caused incorrect aggregation probabilities cases. bug existed since quite time exposed due change data.tables default rbindlist(). See #2578 information. (@mllg, #2579) fixed bug options(.learner.error) respected benchmark(). caused benchmark() stop even continued including FailureModels result (@dagola, #1984) getClassWeightParam() now also works Wrapper* Models ensemble models (@ja-thomas, #891) added getLearnerNote() query “Note” slot learner (@alona-sydorova, #2086)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"filters---general-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"filters - general","title":"Version 2.15","text":"Filter praznik_mrmr also supports regr surv tasks plotFilterValues() got bit “smarter” easier now regarding ordering multiple facets. (@pat-s, #2456) filterFeatures(), generateFilterValuesData() makeFilterWrapper() gained new examples. (@pat-s, #2456)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.15.html","id":"filters---new-2-15-0","dir":"News","previous_headings":"mlr 2.15.0","what":"filters - new","title":"Version 2.15","text":"Ensemble features now supported. filters combine multiple single filters create final ranking based certain statistical operations. new filters listed dedicated section “ensemble filters” tutorial. Tuning simple features supported yet missing feature ParamHelpers. (@pat-s, #2456)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.16.html","id":"mlr-2160","dir":"News","previous_headings":"","what":"mlr 2.16.0","title":"Version 2.16","text":"CRAN release: 2019-11-26","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.16.html","id":"package-infrastructure-2-16-0","dir":"News","previous_headings":"mlr 2.16.0","what":"package infrastructure","title":"Version 2.16","text":"now reference grouping functions pkgdown site (https://mlr.mlr-org.com/reference/index.html) CI testing now Circle CI (previously Travis CI)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.16.html","id":"learners---general-2-16-0","dir":"News","previous_headings":"mlr 2.16.0","what":"learners - general","title":"Version 2.16","text":"fixed bug classif.xgboost prevented passing watchlist binary tasks. caused suboptimal internal label inversion approach. Thanks @001ben reporting (#32) (@mllg) update fda.usc learners work package version >=2.0 update glmnet learners upstream package version 3.0.0 update xgboost learners upstream version 0.90.2 (@pat-s & @-marc, #2681) Updated ParamSet learners classif.gbm regr.gbm. Specifically, param shrinkage now defaults 0.1 instead 0.001. Also choices param distribution added. Internal parallelization package now suppressed (param n.cores). (@pat-s, #2651) Update parameters h2o.deeplearning learners (@albersonmiranda, #2668)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.16.html","id":"misc-2-16-0","dir":"News","previous_headings":"mlr 2.16.0","what":"misc","title":"Version 2.16","text":"Add configureMlr() .onLoad(), possibly fixing edge cases (#2585) (@pat-s, #2637)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.16.html","id":"learners---bugfixes-2-16-0","dir":"News","previous_headings":"mlr 2.16.0","what":"learners - bugfixes","title":"Version 2.16","text":"h2o.gbm learners running wcol passed somehow due internal bug. addition, bug caused another issue prediction prediction data.frame somehow formatted character rather numeric. Thanks @nagdevAmruthnath bringing #2630.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.16.html","id":"filters---general-2-16-0","dir":"News","previous_headings":"mlr 2.16.0","what":"filters - general","title":"Version 2.16","text":"Bugfix: Allow method = \"vh\" filter randomForestSRC_var.select return informative error message supported values. Also argument conservative can now passed. See #2646 #2639 information (@pat-s, #2649) Bugfix: Allow method = \"md\" filter randomForestSRC_var.select set value returned features threshold NA (Issue #2687) Bugfix: new praznik v7.0.0 release filter praznik_CMIM longer return result logical features. See https://gitlab.com/mbq/praznik/issues/19 information","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"mlr-2170","dir":"News","previous_headings":"","what":"mlr 2.17.0","title":"Version 2.17","text":"CRAN release: 2020-01-10","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"plotting-2-17-0","dir":"News","previous_headings":"mlr 2.17.0","what":"plotting","title":"Version 2.17","text":"n.show argument effect plotFilterValues(). Thanks @albersonmiranda. (#2689)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"functional-data-2-17-0","dir":"News","previous_headings":"mlr 2.17.0","what":"Functional Data","title":"Version 2.17","text":"PR: #2638 (@pfistl) Added several learners regression classification functional data classif.classiFunc.(kernel|knn) (knn/kernel using various semi-metrics) (classif|regr).fgam (Functional generalized additive models) (classif|regr).FDboost (Boosted functional generalized additive models) Added preprocessing steps feature extraction functional data extractFDAFourier (Fourier transform) extractFDAWavelets (Wavelet features) extractFDAFPCA (Principal components) extractFDATsfeatures (Time-Series features tsfeatures package) extractFDADTWKernel (Dynamic Time-Warping Kernel) extractFDAMultiResFeatures (Compute features multiple resolutions) Fixed bug multiclass binaryclass reduction techniques work functional data. Several minor bug fixes code improvements Extended clarified documentation several fda components.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"learners---general-2-17-0","dir":"News","previous_headings":"mlr 2.17.0","what":"learners - general","title":"Version 2.17","text":"xgboost: added options ‘auto’, ‘approx’ ‘gpu_hist’ param tree_method (@albersonmiranda, #2701) getFeatureImportance() now returns long data.frame columns variable importance. Beforehand, wide data.frame returned variable representing column (@pat-s, #1755).","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"filters---general-2-17-0","dir":"News","previous_headings":"mlr 2.17.0","what":"filters - general","title":"Version 2.17","text":"Allow custom threholding function passed filterFeatures makeFilterWrapper (@annette987, #2686) Allow ensemble filters include multiple base filters type (@annette987, #2688)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"filters---bugfixes-2-17-0","dir":"News","previous_headings":"mlr 2.17.0","what":"filters - bugfixes","title":"Version 2.17","text":"filterFeatures(): Arg thresh working correctly applied ensemble filters. (@annette987, #2699) Fixed incorrect ranking ensemble filters. Thanks @annette987 (#2698)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"mlr-2171","dir":"News","previous_headings":"","what":"mlr 2.17.1","title":"Version 2.17","text":"CRAN release: 2020-03-24","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"learners---bugfixes-2-17-1","dir":"News","previous_headings":"mlr 2.17.1","what":"Learners - bugfixes","title":"Version 2.17","text":"remove regr_slim learner due pkg (flare) orphaned CRAN","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"measures---bugixes-2-17-1","dir":"News","previous_headings":"mlr 2.17.1","what":"Measures - bugixes","title":"Version 2.17","text":"remove measure clValid::dunn tests (package orphaned) (#2742) Bugfix: tuneThreshold() now accounts direction measure. Beforehand, performance measure always minimized (#2732). Remove adjusted Rsq measure (arsq), fixes #2711","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"filters---bugfixes-2-17-1","dir":"News","previous_headings":"mlr 2.17.1","what":"Filters - bugfixes","title":"Version 2.17","text":"Fixed issue caused random forest minimal depth filter return NA values using thresholding. NAs returned features given threshold. (@annette987, #2710) Fixed problem prevented passing filter options via argument .args simple filters (@annette987, #2709)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"feature-selection---bugfixes-2-17-1","dir":"News","previous_headings":"mlr 2.17.1","what":"Feature selection - bugfixes","title":"Version 2.17","text":"Fix print.FeatSelResult() bits..features used selectFeatures() (#2721) Return long DF getFeatureImportance() (#2708)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.17.html","id":"misc-2-17-1","dir":"News","previous_headings":"mlr 2.17.1","what":"Misc","title":"Version 2.17","text":"pkgdown: Move changelog Appendix Account {checkmate} v2.0.0 update (#2734) Refactor function calls packages (<pkg::fun>) within ParamSets (#2730) avoid errors listLearners() pkgs installed listLearners() fail package installed (#2717)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.18.html","id":"mlr-2180","dir":"News","previous_headings":"","what":"mlr 2.18.0","title":"Version 2.18","text":"CRAN release: 2020-10-05 Many praznik filters now also able deal regression tasks (#2790, @bommert) praznik_MRMR: Remove handling survival tasks (#2790, @bommert) xgboost: update objective default reg:linear (deprecated) reg:squarederror issue warning blocking set Task blocking.cv set within `makeResampleDesc() (#2788) Fix order learners generateLearningCurveData() (#2768) getFeatureImportance(): Account feature importance weight linear xgboost models Fix learner note learner glmnet (default param s match learner note) (#2747) Remove dependency {hrbrthemes} used createSpatialResamplingPlots(). package caused issues R-devel. addition users set custom themes . Explicitly return value getNestedTuneResultsOptPathDf() (#2754)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.19.html","id":"mlr-2190","dir":"News","previous_headings":"","what":"mlr 2.19.0","title":"Version 2.19","text":"CRAN release: 2021-02-22 Add filter FSelectoRcpp::relief(). C++ based implementation RelieF filter algorithm way faster Java based one {FSelector} package (#2804) Fix S3 print method FilterWrapper objects Make ibrier measure work survival tasks (#2789) Switch testthat v3 (#2796) Enable parallel tests (#2796) Replace package PMCMR PMCMRplus (#2796) Remove CoxBoost learner due CRAN removal Warning fix.factors.prediction = TRUE causes generation NAs new factor levels prediction (@jakob-r, #2794) Clear error message prediction wrapped learner length newdata (@jakob-r, #2794)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.2.html","id":"mlr-22","dir":"News","previous_headings":"","what":"mlr 2.2:","title":"Version 2.2","text":"CRAN release: 2014-10-29 web tutorial MUCH improved! example tasks data sets Learners tasks now support ordered factors features. task description knows whether ordered factors present checked whether learner supports feature. set property ‘ordered’ conservatively, learners , sure ordered inputs handled correctly training. know models support , please inform us. basic R learners now new slots: name (descriptive name algorithm), short.name (abbreviation can used plots tables) note (notes regarding slight changes mlr integration learner ). makeLearner now supports options regarding learner error handling output set globally via configureMlr Additional arguments imputation functions allow fine-grain control dummy column creation imputeMin imputeMax now subtract add multiple range data minimum maximum, respectively. cluster methods now property ‘prob’ support fuzzy cluster membership probabilities, also support predict.type = ‘prob’. Everything basically works posterior probabilities classif.* methods. predict preserves rownames input output fixed bug createDummyFeatures caused error data contained missing values. plotLearnerPrediction works clustering allows greyscale plots (printing articles) whole object-oriented structure behind feature filtering much improved. Smaller changes signature makeFilterWrapper filterFeatures become necessary. fixed bug filter methods FSelector package caused error variable names contained accented letters filterFeatures can now also applied result getFilterValues dropped data.frame version preprocessing operations like mergeFactorLevelsBySize, joinClassLevels removeConstantFeatures consistency. now always require tasks input. support pretty generic framework stacking / super-learning now, see makeStackedLearner imbalancy correction + smote: ** fix bug “smote” factor features present ** change oversampling: sample new observations (replacement) ** extension smote algorithm (sampling): minority class observations binary classification either chosen via sampling alternatively, minority class observation used equal number times made getters BenchmarkResult consistent. now: getBMRTaskIds, getBMRLearnerIds, getBMRPredictions, getBMRPerformances, getBMRAggrPerformances getBMRTuneResults, getFeatSelResults, getBMRFilteredFeatures following methods work BenchmarkResult anymore: getTuneResult, getFeatSelResult Removed getFilterResult, getFilteredFeatures","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.2.html","id":"new-learners-2-2","dir":"News","previous_headings":"mlr 2.2:","what":"new learners:","title":"Version 2.2","text":"classif.bartMachine classif.lqa classif.randomForestSRC classif.sda regr.ctree regr.plsr regr.randomForestSRC cluster.cmeans cluster.DBScan cluster.kmeans cluster.FarthestFirst surv.cvglmnet surv.optimCoxBoostPenalty","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.2.html","id":"new-filters-2-2","dir":"News","previous_headings":"mlr 2.2:","what":"new filters:","title":"Version 2.2","text":"variance univariate carscore rf.importance, rf.min.depth anova.test, kruskal.test mrmr","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.2.html","id":"new-functions-2-2","dir":"News","previous_headings":"mlr 2.2:","what":"new functions","title":"Version 2.2","text":"makeMulticlassWrapper makeStackedLearner, getStackedBaseLearnerPredictions joinClassLevels summarizeColumns, summarizeLevels capLargeValues, mergeFactorLevelsBySize","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.3.html","id":"mlr-23","dir":"News","previous_headings":"","what":"mlr 2.3:","title":"Version 2.3","text":"CRAN release: 2015-02-04 resample now returns object class ResampleResult (downward compatible) allow print method. resampling features now supported arbitrary number factor features mlr supports ViperCharts plots now ROC plot via ROCR can now created automatically, call asROCRPrediction, construct plots via ROCR self. See plotROCRCurves mlr measures now slots “name” “note” exported simple “getters” tasks, see makeLearner probability predict.threshold can set classifiers, also see setPredictThreshold control objects tuning feature selection, user can now enable threshold tuning control objects tuning feature selection, user can now define logging function default console logging tuneParams selectFeatures informative, displays time memory info updated properties learners Default arguments classif.bartMachine, classif.randomForestSRC, regr.randomForestSRC sur.randomForestSRC changed allow missing data support default settings. externalized measure functions used vectors. minor bug fixes required basic learner packages loaded global namespace anymore, requireNamespace used internally instead. ensures less name clashes name shadowing resample passes dot arguments learner hyperpars new option “.par...bounds” disable --bound checks model parameters measures slightly internally changed. expose properties (check ?Measure) now unnecessary object slots removed classif.lda classif.qda now hyperpar “predict.method” filterFeatures makeFilterWrapper gain argument mandatory features plotLearnerPrediction new option “err.size” classif.plsDA cluster.DBscan now removed problems underlying learning algorithm new aggregation test.join following models now can handle factors ordereds extra dummy int encoding: classif.glmnet, regr.glmnet, surv.glmnet, surv.cvglmnet, surv.penalized, surv.optimCoxBoostPenalty, surv.glmboost, surv.CoxBoost","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.3.html","id":"new-functions-2-3","dir":"News","previous_headings":"mlr 2.3:","what":"new functions","title":"Version 2.3","text":"getTaskType, getTaskId, getTaskTargetNames plotROCRCurves plotViperCharts measureSSE, measureMSE, measureRMSE, measureMEDSE, … PreprocWrapperCaret setPredictThreshold","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.3.html","id":"new-learners-2-3","dir":"News","previous_headings":"mlr 2.3:","what":"new learners:","title":"Version 2.3","text":"classif.bdk classif.binomial classif.extraTrees classif.probit classif.xgboost classif.xyf regr.bartMachine regr.bcart regr.bdk regr.bgp regr.bgpllm regr.blm regr.brnn regr.btgp regr.btgpllm regr.btlm regr.cubist regr.elmNN regr.extraTrees regr.laGP regr.xgboost regr.xyf surv.rpart","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.4.html","id":"mlr-24","dir":"News","previous_headings":"","what":"mlr 2.4:","title":"Version 2.4","text":"CRAN release: 2015-06-13 WrappedModel printer slightly improved ReampleResult now stores runtime took resample slot getTaskFormula / getTaskFormulaAsString new argument ‘explicit.features’ getTaskData now recodeY = “drop.levels” drops empty factor levels option fix.factors makeLearner renamed fix.factors.prediction clarity showHyperPars removed. getParamSet exactly thing ‘resample’ ‘benchmark’ got argument keep.pred, setting FALSE allows discard prediction objects save memory slightly change mem usage reported tuning feature selection See TuneControl FeatSelControl documented done now. tuneIrace: allows set precision / digits within irace (using argument ‘digits’ makeTuneControlIrace); default maximum precision plotting general try introduce “data layer”, data can generated independently plotting first, well-defined objects; can plotted mlr custom code; naming scheme always generateData plot getFilterValues deprecated favor generateFilterValuesData plotFilterValues can now plot multiple filter methods using facetting plotROCRCurves rewritten use ggplot2 classif.ada: added “loss” hyperpar add missings properties ctree cforest methods: regr/classif ctree, regr/classif/surv cforest, regr/classif blackboost learner xgboost removed, package CRAN anymore, unfortunately reg.km: added param ‘iso’ classif.mda: added param ‘start.method’ changed default ‘lvq’, added params ‘sub.df’, ‘tot.df’ ‘criterion’ classif.randomForest: ‘sampsize’ can now int vector (instead scalar) plotThreshVsPerf plotLearningCurve now param ‘facet’","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.4.html","id":"new-functions-2-4","dir":"News","previous_headings":"mlr 2.4:","what":"new functions","title":"Version 2.4","text":"getTaskSize getNestedTuneResultsX, getNestedTuneResultsOptPathDf tuneDesign generateROCRCurvesData, generateFilterValuesData, generateLearningCurveData, plotLearningCurve, generateThreshVsPerfData, plotThreshVsPerf, generateThreshVsPerfData accepts Prediction, ResampleResult, lists ResampleResult, BenchmarkResult objects. experimental ggvis functions: plotROCRCurvesGGVIS, plotLearningCurveGGVIS, plotTuneMultiCritResultGGVIS, plotThreshVsPerfGGVIS, plotFilterValuesGGVIS","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.4.html","id":"new-learners-2-4","dir":"News","previous_headings":"mlr 2.4:","what":"new learners:","title":"Version 2.4","text":"classif.bst classif.hdrda classif.nodeHarvest classif.pamr classif.rFerns classif.sparseLDA regr.bst regr.frbs regr.nodeHarvest regr.slim","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.4.html","id":"new-measures-2-4","dir":"News","previous_headings":"mlr 2.4:","what":"new measures:","title":"Version 2.4","text":"brier","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.5.html","id":"mlr-25","dir":"News","previous_headings":"","what":"mlr 2.5:","title":"Version 2.5","text":"CRAN release: 2015-11-20 fixed bug caused performance() return incorrect values ResamplePredictions (somewhat experimental) support multilabel classification. now task, new baselearner (rFerns), generic reduction--binary algorithm (MultilabelWrapper) tuning: added ‘budget’ parameter makeTuneControl* (single-objective) makeTuneMultiCritControl* (multi-objective scenarios), allowing define maximum “number evaluations” budget tuning algorithms tuning: added ‘budget’ parameter makeTuneMultiCritControl*, allowing define maximum “number evaluations” budget tuning algorithms single-objective case makeTuneControlGenSA: optimized function considered non-smooth per default (change via … args) classif.svm, regr.svm: added ‘scale’ param ksvm: added ‘cache’ param plotFilterValuesGGVIS: sort n_show interactive, interactive flag removed renamed getProbabilities getPredictionProbabilities deprecated getProbabilities plots now use long names measures possible nasty bug measure “mcc”. fixed unit tested. apologies. removed getTaskFormulaAsString improved getTaskFormula former needed anymore aggregations now ‘name’ property, long name generateLearningCurveData generateThreshVsPerfData now append aggregation id output column name measure ids plotLearningCurve, plotLearningCurveGGVIS, plotThreshVsPerf, plotThreshVsPerfGGVIS now argument ‘pretty.names’ plots ‘name’ element measures instead ‘id’. makeCustomResampledMeasure now arguments ‘measure.id’ ‘aggregation.id’ instead ‘id’ corresponded measure. Also, ‘name’ note (corresponding measure) well ‘aggregation.name’ added. makeCostMeasure now arguments ‘name’ ‘id’. classification learner now can property ‘class.weights’, supported ‘class.weights.param’. latter indicates parameters provides class weights information learner. class weights integrated learner used default ‘wcw.param’ ‘makeWeightedClassesWrapper’ listLearners create = FALSE load packages anymore therefore faster reliable; also supports additional parameter check.packages now check whether required packages installed without loading many new functions statistical benchmark comparisons added, see rename hasProperties, getProperties hasLearnerProperties getLearnerProperties Learner properties now implemented object oriented state Learner. RLearners properties stored slot. class getter can overwritten. hill climbing algorithm stacking (Caruana 04) implemented method ‘hill.climb’ ‘makeStackedLearner’ select models base learners, equivalent weighted average. model compression algorithm stacking (Caruana 06) implemented method ‘compress’ ‘makeStackedLearner’ first select models base learners mimic behaviour super learner. default super learner neural network. relativeOverfitting provides way estimate much model overfits training data according measure. restructured LiblineaR learners convenient format. old ones removed: classif.LiblineaRBinary, classif.LiblineaRLogReg, classif.LiblineaRMultiClass. new ones, see . Added commonly used ResampleDesc description objects, save typing resample experiments: hout, cv2, cv3, cv5, cv10. regr.randomForest: changed default nodesize 5 (according randomForest defaults)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.5.html","id":"new-functions-2-5","dir":"News","previous_headings":"mlr 2.5:","what":"new functions","title":"Version 2.5","text":"getDefaultMeasure getTaskClassLevels getPredictionTruth, getPredictionResponse, getPredictionSE convertMLBenchObjToTask getBMRLearners, getBMRMeasures, getBMRMeasureIds makeMultilabelTask, makeMultilabelWrapper, getMultilabelBinaryPerformances generatePartialPredictionData, plotPartialPrediction, plotPartialPredictionGGVIS getClassWeightParam plotBenchmarkResult, convertBMRToRankMatrix, generateRankMatrixAsBarData, plotRankMatrixAsBar, generateBenchmarkSummaryData, plotBenchmarkSummary, friedmanTestBMR, friedmanPostHocTestBMR, generateCritDifferencesData, plotCritDifferences getCaretParamSet generateCalibrationData plotCalibration relativeOverfitting plotROCCurves","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.5.html","id":"new-measures-2-5","dir":"News","previous_headings":"mlr 2.5:","what":"new measures","title":"Version 2.5","text":"hamloss","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.5.html","id":"new-learners-2-5","dir":"News","previous_headings":"mlr 2.5:","what":"new learners","title":"Version 2.5","text":"multilabel.rFerns classif.avNNet classif.neuralnet regr.avNNet classif.clusterSVM classif.dcSVM classif.gaterSVM classif.mlp classif.saeDNN classif.dbnDNN classif.nnTrain classif.rknn regr.rknn classif.xgboost regr.xgboost classif.rotationForest classif.LiblineaRL1L2SVC classif.LiblineaRL1LogReg classif.LiblineaRL2L1SVC classif.LiblineaRL2LogReg classif.LiblineaRL1LMultiClassSVC regr.LiblineaRL2L1SVR regr.LiblineaRL2L2SVR classif.ranger regr.ranger surv.ranger","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.5.html","id":"new-filters-2-5","dir":"News","previous_headings":"mlr 2.5:","what":"new filters","title":"Version 2.5","text":"permutation.importance","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.5.html","id":"removed-functions-2-5","dir":"News","previous_headings":"mlr 2.5:","what":"removed functions","title":"Version 2.5","text":"setProperties, addProperties, removeProperties","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.6.html","id":"mlr-26","dir":"News","previous_headings":"","what":"mlr 2.6:","title":"Version 2.6","text":"CRAN release: 2015-11-26 cluster.kmeans: added support fuzzy clustering (property “prob”) regr.lm: removed erroneous param settings regr.glmnet: added ‘family’ param allowed ‘gaussian’, also ‘poisson’ disabled plotViperCharts unit tests VC seems offline currently multilabel: improve task getter functions, especially getTaskFormula now correct","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.6.html","id":"new-learners-2-6","dir":"News","previous_headings":"mlr 2.6:","what":"new learners","title":"Version 2.6","text":"regr.glmboost cluster.Cobweb","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.7.html","id":"mlr-27","dir":"News","previous_headings":"","what":"mlr 2.7:","title":"Version 2.7","text":"CRAN release: 2015-12-04 New argument “models” function benchmark fixed bug ‘keep.pred’ ignored benchmark function new functions benchmark plots refactored /renamed. names gone API: plotBenchmarkResult, generateRankMatrixAsBarData, plotRankMatrixAsBar, generateBenchmarkSummaryData, plotBenchmarkSummary, new API: plotBMRSummary, plotBMRBoxplots, plotBMRRanksAsBarChart","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.8.html","id":"mlr-28","dir":"News","previous_headings":"","what":"mlr 2.8:","title":"Version 2.8","text":"CRAN release: 2016-02-13 Feature filter “univariate” bad name, deprecated now called “univariate.model.score”. new one also better defaults. (generate/plot)PartialPrediction: added new arg “geom” tile plots small fix plotBMRSummary ModelMultiplexer inherits predict.type base learners now check learners ensemble predict.type new function getBMRModels extract stored models benchmark result Fixed bug several learners LiblineaR package (“classif.LiblineaRL2LogReg”, “classif.LiblineaRL2SVC”, “regr.LiblineaRL2L2SVR”) calling wrong value “type” (0) thus training wrong model. Fixed bug resampling objects hout, cv2, cv3, cv5, cv10 documented ResampleDesc help page regr.xgboost, classif.xgboost: add feval param fixed bug irace tuning interface unamed discrete values Fixed bugs “jackknife” “bootstrap” se estimators regr.randomForest. Added “sd” estimator regr.randomForest. Fixed mini bug ModelMultiplexer hyperpars needed predict passed correctly Fixed bug function capLargeValues wasn’t working passed task. capLargeValues now new argument “target”, prevent capping response values. classif.gbm, regr.gbm: Updated possible ‘distribution’ settings bit. oversample, undersample, makeOversampleWrapper, makeUndersampleWrapper, makeOverBaggingWrapper: Added arguments specifically select sampled class.","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.8.html","id":"api-changes-2-8","dir":"News","previous_headings":"mlr 2.8:","what":"API changes","title":"Version 2.8","text":"listLearners now returns data frame properties learners create false","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.8.html","id":"new-functions-2-8","dir":"News","previous_headings":"mlr 2.8:","what":"new functions","title":"Version 2.8","text":"getBMRModels","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.8.html","id":"removed-functions-2-8","dir":"News","previous_headings":"mlr 2.8:","what":"removed functions","title":"Version 2.8","text":"generateROCRCurvesData, plotROCRCurves, plotROCRCurvesGGVIS","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.8.html","id":"new-learners-2-8","dir":"News","previous_headings":"mlr 2.8:","what":"new learners","title":"Version 2.8","text":"classif.randomForestSRCSyn classif.cvglmnet regr.randomForestSRCSyn cluster.dbscan","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.8.html","id":"new-measures-2-8","dir":"News","previous_headings":"mlr 2.8:","what":"new measures","title":"Version 2.8","text":"rsq, arsq, expvar","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"mlr-29","dir":"News","previous_headings":"","what":"mlr 2.9:","title":"Version 2.9","text":"CRAN release: 2016-08-03","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"functions---general-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"functions - general","title":"Version 2.9","text":"various cleanups removed unused code subsetTask, getTaskData: arg “features” now also accepts logical integer removeConstantFeatures now also operates data.frames makeRemoveConstantFeaturesWrapper can used augment learner preprocessing step. normalizeFeatures, createDummyFeatures: arg ‘exclude’ replaced ‘cols’ normalizeFeatures now S3 can called also data.frames SMOTEWrapper: fix bug “sw.nn” correctly passed fixed bug caused hyperparameters passed correctly ModelMultiplexer cases fix bug NoFeaturesModel ModelMultiplexer fix small bug DownsampleWrapper trained weights getNestedTuneResultsOptPathDf: added new arg “trafo” improve documentation permutation.importance filter perform slight argument renaming fix potential name clashes plotPartialDependence can plot classification tasks one interacted features now generateFilterValuesData: added argument ‘.args’ add pretty.names arguments plots show learner short names instead IDs addition ‘data’ argument plotPartialDependence adds training data graph added new arguments “facet.wrap.nrow” “facet.wrap.ncol” enable arrangement facets rows columns plotting functions","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"functions---new-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"functions - new","title":"Version 2.9","text":"generateHyperParsEffectData, plotHyperParsEffect makeMultilabelClassifierChainsWrapper, makeMultilabelDBRWrapper makeMultilabelNestedStackingWrapper, makeMultilabelStackingWrapper makeConstantClassWrapper generateFunctionalANOVAData","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"functions---removed-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"functions - removed","title":"Version 2.9","text":"getParamSet generic (now ParamHelpers package)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"functions---renamed-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"functions - renamed","title":"Version 2.9","text":"generatePartialPrediction generatePartialDependence plotPartialPrediction plotPartialDependence plotPartialPredictionGGVIS plotPartialDependenceGGVIS","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"learners---general-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"learners - general","title":"Version 2.9","text":"fixed weight handling weight tag learners remove unnecessary linear.output parameter classif.neuralnet remove unsupported KSVM parameter value stringdot fix bartMachine compatibility issues classif.ranger, regr.ranger surv.ranger: now respect unordered factors default clean randomForestSRC randomForestSRCSyn learners “penalized” learner restructured improved (params added), also see . add stability.nugget parameter “regr.km” classif.blackboost, regr.blackboost: made sure arg “stump” passed correctly fixed parameter values WEKA learners IBk, J48, PART, EM, SimpleKMeans, XMeans classif.glmboost, regr.glmboost: add parameters stopintern trace","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"learners---new-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"learners - new","title":"Version 2.9","text":"classif.C50 classif.gausspr classif.penalized.fusedlasso classif.penalized.lasso classif.penalized.ridge classif.h2o.deeplearning classif.h2o.gbm classif.h2o.glm classif.h2o.randomForest classif.rrf regr.penalized.fusedlasso regr.gausspr regr.glm regr.GPfit regr.h2o.deeplearning regr.h2o.gbm regr.h2o.glm regr.h2o.randomForest regr.rrf surv.cv.CoxBoost surv.penalized.fusedlasso surv.penalized.lasso surv.penalized.ridge cluster.kkmeans multilabel.randomforestSRC","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"learners---removed-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"learners - removed","title":"Version 2.9","text":"surv.optimCoxBoostPenalty surv.penalized (split , see new learners )","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"measures---general-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"measures - general","title":"Version 2.9","text":"updated gmean measure unit test, added reference formula gmean makeCostMeasure: removed arg “task”, names cost matrix checked measure calculation","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"measures---new-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"measures - new","title":"Version 2.9","text":"multiclass.brier brier.scaled logloss multilabel.subset01, multilabel.f1, multilabel.acc, multilabel.ppv, multilabel.tpr multiclass.au1p, multiclass.au1u, multiclass.aunp, multiclass.aunu","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-2.9.html","id":"measures---renamed-2-9","dir":"News","previous_headings":"mlr 2.9:","what":"measures - renamed","title":"Version 2.9","text":"multiclass.auc multiclass.au1u hamloss multilabel.hamloss","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/news/news-dev.html","id":"filters---bugfixes-2-17-0-9000","dir":"News","previous_headings":"mlr 2.17.0.9000","what":"filters - bugfixes","title":"Version dev","text":"Fixed issue caused random forest minimal depth filter return NA values using thresholding. NAs returned features given threshold. (@annette987, #2710) Fixed problem prevented passing filter options via argument .args simple filters (@annette987, #2709)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-dev.html","id":"mlr-21709001","dir":"News","previous_headings":"","what":"mlr 2.17.0.9001","title":"Version dev","text":"Fix print.FeatSelResult() bits..features used selectFeatures() (#2721) Return long DF getFeatureImportance() (#2708) Remove adjusted Rsq measure (arsq), fixes #2711 listLearners() fail package installed (#2717)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-dev.html","id":"mlr-21709002","dir":"News","previous_headings":"","what":"mlr 2.17.0.9002","title":"Version dev","text":"Bugfix: tuneThreshold() now accounts direction measure. Beforehand, performance measure always minimized (#2732). pkgdown: Move changelog Appendix Account {checkmate} v2.0.0 update (#2734)","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-dev.html","id":"mlr-21709003","dir":"News","previous_headings":"","what":"mlr 2.17.0.9003","title":"Version dev","text":"remove measure clValid::dunn tests (package orphaned) (#2742) Refactor function calls packages (<pkg::fun>) within ParamSets (#2730) avoid errors listLearners() pkgs installed remove regr_slim learner due pkg (flare) orphaned CRAN","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-dev.html","id":"mlr-21909000","dir":"News","previous_headings":"","what":"mlr 2.19.0.9000","title":"Version dev","text":"Internal changes .","code":""},{"path":"https://mlr.mlr-org.com/dev/news/news-dev.html","id":"mlr-21909001","dir":"News","previous_headings":"","what":"mlr 2.19.0.9001","title":"Version dev","text":"Fixed information.gain filter calculation. , chi.squared calculated even though information.gain requested due glitch filter naming (#2816, @jokokojote)","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/Aggregation.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregation object. — Aggregation","title":"Aggregation object. — Aggregation","text":"aggregation method reduces performance values test (possibly training sets) single value. see possible implemented aggregations look aggregations. aggregation can access relevant information result resampling combine single value. Though usually something simple like taking mean test set performances done. Object members: id (character(1)) Name aggregation method. name (character(1)) Long name aggregation method. properties (character) Properties aggregation. fun (`function(task, perf.test, perf.train, measure, group, pred)]) Aggregation function.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/BenchmarkResult.html","id":null,"dir":"Reference","previous_headings":"","what":"BenchmarkResult object. — BenchmarkResult","title":"BenchmarkResult object. — BenchmarkResult","text":"Result benchmark experiment conducted benchmark following members: results (list ResampleResult): nested list resample results, first ordered task id, learner id. measures (list Measure): performance measures used benchmark experiment. learners (list Learner): learning algorithms compared benchmark experiment. print method object shows aggregated performance values tasks learners. recommended retrieve required information via getBMR* getter functions. can also convert object using .data.frame.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/ClassifTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a classification task. — makeClassifTask","title":"Create a classification task. — makeClassifTask","text":"Create classification task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/ClassifTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a classification task. — makeClassifTask","text":"","code":"makeClassifTask(   id = deparse(substitute(data)),   data,   target,   weights = NULL,   blocking = NULL,   coordinates = NULL,   positive = NA_character_,   fixup.data = \"warn\",   check.data = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/ClassifTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a classification task. — makeClassifTask","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). survival analysis names survival time event columns, length 2. multilabel classification contains names logical columns encode whether label present length corresponds number classes. weights (numeric) Optional, non-negative case weight vector used fitting. set cost-sensitive learning. Default NULL means (= equal) weights. blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions. positive (character(1)) Positive class binary classification (otherwise ignored set NA). Default first factor level target attribute. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/ClusterTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a cluster task. — makeClusterTask","title":"Create a cluster task. — makeClusterTask","text":"Create cluster task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/ClusterTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a cluster task. — makeClusterTask","text":"","code":"makeClusterTask(   id = deparse(substitute(data)),   data,   weights = NULL,   blocking = NULL,   coordinates = NULL,   fixup.data = \"warn\",   check.data = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/ClusterTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a cluster task. — makeClusterTask","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). weights (numeric) Optional, non-negative case weight vector used fitting. set cost-sensitive learning. Default NULL means (= equal) weights. blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/ConfusionMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Confusion matrix — ConfusionMatrix","title":"Confusion matrix — ConfusionMatrix","text":"result calculateConfusionMatrix. Object members: result (matrix) Confusion matrix absolute values marginals. Can also contain row column sums observations. task.desc (TaskDesc) Additional information task. sums (logical(1)) Flag marginal sums observations calculated. relative (logical(1)) Flag relative confusion matrices calculated. relative.row (matrix) Confusion matrix relative values marginals normalized row. relative.col (matrix) Confusion matrix relative values marginals normalized column. relative.error (numeric(1)) Relative error overall.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/CostSensTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a cost-sensitive classification task. — makeCostSensTask","title":"Create a cost-sensitive classification task. — makeCostSensTask","text":"Create cost-sensitive classification task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/CostSensTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a cost-sensitive classification task. — makeCostSensTask","text":"","code":"makeCostSensTask(   id = deparse(substitute(data)),   data,   costs,   blocking = NULL,   coordinates = NULL,   fixup.data = \"warn\",   check.data = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/CostSensTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a cost-sensitive classification task. — makeCostSensTask","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). costs (data.frame) numeric matrix data frame containing costs misclassification. assume general case observation specific costs. means n rows, corresponding observations, order data. columns correspond classes names class labels (unnamed use y1 yk labels). entry (,j) matrix specifies cost predicting class j observation . blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/FailureModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Failure model. — FailureModel","title":"Failure model. — FailureModel","text":"subclass WrappedModel. created set respective option configureMlr - model internally crashed training. model always predicts NAs. mlr option .error.dump TRUE, FailureModel contains debug trace error. can accessed getFailureModelDump inspected debugger. encapsulated learner.model simply string: error message generated model crashed. following code shows access message.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/FailureModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Failure model. — FailureModel","text":"","code":"configureMlr(on.learner.error = \"warn\") data = iris data$newfeat = 1 # will make LDA crash task = makeClassifTask(data = data, target = \"Species\") m = train(\"classif.lda\", task) # LDA crashed, but mlr catches this #> Warning: Could not train learner classif.lda: Error in lda.default(x, grouping, ...) :  #>   variable 5 appears to be constant within groups print(m) #> Model for learner.id=classif.lda; learner.class=classif.lda #> Trained on: task.id = data; obs = 150; features = 5 #> Hyperparameters:  #> Training failed: Error in lda.default(x, grouping, ...) :  #>   variable 5 appears to be constant within groups #>  #> Training failed: Error in lda.default(x, grouping, ...) :  #>   variable 5 appears to be constant within groups #>  print(m$learner.model) # the error message #> [1] \"Error in lda.default(x, grouping, ...) : \\n  variable 5 appears to be constant within groups\\n\" p = predict(m, task) # this will predict NAs print(p) #> Prediction: 150 observations #> predict.type: response #> threshold:  #> time: NA #>   id  truth response #> 1  1 setosa     <NA> #> 2  2 setosa     <NA> #> 3  3 setosa     <NA> #> 4  4 setosa     <NA> #> 5  5 setosa     <NA> #> 6  6 setosa     <NA> #> ... (#rows: 150, #cols: 3) print(performance(p)) #> mmce  #>   NA  configureMlr(on.learner.error = \"stop\")"},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelControl.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control structures for feature selection. — FeatSelControl","title":"Create control structures for feature selection. — FeatSelControl","text":"Feature selection method used selectFeatures. methods used follow wrapper approach, described Kohavi John (1997) (see references). following optimization algorithms available: FeatSelControlExhaustive Exhaustive search. feature sets (certain number features max.features) searched. FeatSelControlRandom Random search. Features vectors randomly drawn, certain number features max.features. feature included current set probability prob. basically drawing (0,1)-membership-vectors, element Bernoulli(prob) distributed. FeatSelControlSequential Deterministic forward backward search. means extending (forward) shrinking (backward) feature set. Depending given method different approaches taken.sfs Sequential Forward Search: Starting empty model, step feature increasing performance measure added model.sbs Sequential Backward Search: Starting model features, step feature decreasing performance measure least removed model.sffs Sequential Floating Forward Search: Starting empty model, step algorithm chooses best model models one additional feature models one feature less.sfbs Sequential Floating Backward Search: Similar sffs starting full model. FeatSelControlGA Search via genetic algorithm. GA simple (mu, lambda) (mu + lambda) algorithm, depending comma setting. comma strategy selects new population size mu lambda > mu offspring. plus strategy uses joint pool mu parents lambda offspring selecting mu new candidates. mu features, new lambda features generated randomly choosing pairs parents. crossed crossover.rate represents probability choosing feature first parent instead second parent. resulting offspring mutated, .e., bits flipped probability mutation.rate. max.features set, offspring repeatedly generated setting satisfied.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelControl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control structures for feature selection. — FeatSelControl","text":"","code":"makeFeatSelControlExhaustive(   same.resampling.instance = TRUE,   maxit = NA_integer_,   max.features = NA_integer_,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\" )  makeFeatSelControlGA(   same.resampling.instance = TRUE,   impute.val = NULL,   maxit = NA_integer_,   max.features = NA_integer_,   comma = FALSE,   mu = 10L,   lambda,   crossover.rate = 0.5,   mutation.rate = 0.05,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\" )  makeFeatSelControlRandom(   same.resampling.instance = TRUE,   maxit = 100L,   max.features = NA_integer_,   prob = 0.5,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\" )  makeFeatSelControlSequential(   same.resampling.instance = TRUE,   impute.val = NULL,   method,   alpha = 0.01,   beta = -0.001,   maxit = NA_integer_,   max.features = NA_integer_,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelControl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control structures for feature selection. — FeatSelControl","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. maxit (integer(1)) Maximal number iterations. Note, usually equal number function evaluations. max.features (integer(1)) Maximal number features. tune.threshold (logical(1)) threshold tuned measure hand, feature set evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. comma (logical(1)) Parameter GA feature selection, indicating whether use (mu, lambda) (mu + lambda) GA. default FALSE. mu (integer(1)) Parameter GA feature selection. Size parent population. lambda (integer(1)) Parameter GA feature selection. Size children population (smaller equal mu). crossover.rate (numeric(1)) Parameter GA feature selection. Probability choosing bit first parent within crossover mutation. mutation.rate (numeric(1)) Parameter GA feature selection. Probability flipping feature bit, .e. switch selecting / deselecting feature. prob (numeric(1)) Parameter random feature selection. Probability choosing feature. method (character(1)) Parameter sequential feature selection. character representing method. Possible values sfs (forward search), sbs (backward search), sffs (floating forward search) sfbs (floating backward search). alpha (numeric(1)) Parameter sequential feature selection. Minimal required value improvement difference forward / adding step. Default 0.01. beta (numeric(1)) Parameter sequential feature selection. Minimal required value improvement difference backward / removing step. Negative values imply allow slight decrease removal feature. Default -0.001.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelControl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control structures for feature selection. — FeatSelControl","text":"(FeatSelControl). specific subclass one FeatSelControlExhaustive, FeatSelControlRandom, FeatSelControlSequential, FeatSelControlGA.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelControl.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create control structures for feature selection. — FeatSelControl","text":"Ron Kohavi George H. John, Wrappers feature subset selection, Artificial Intelligence Volume 97, 1997, 273-324. http://ai.stanford.edu/~ronnyk/wrappersPrint.pdf.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Result of feature selection. — FeatSelResult","title":"Result of feature selection. — FeatSelResult","text":"Container results feature selection. Contains obtained features, performance values optimization path lead .   can visualize using analyzeFeatSelResult.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/FeatSelResult.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Result of feature selection. — FeatSelResult","text":"Object members: learner (Learner) Learner optimized. control (FeatSelControl) Control object feature selection. x (character) Vector feature names identified optimal. y (numeric) Performance values optimal x. threshold (numeric) Vector finally found used thresholds tune.threshold enabled FeatSelControl, otherwise present hence NULL. opt.path (ParamHelpers::OptPath) Optimization path lead x.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/LearnerProperties.html","id":null,"dir":"Reference","previous_headings":"","what":"Query properties of learners. — LearnerProperties","title":"Query properties of learners. — LearnerProperties","text":"Properties can accessed getLearnerProperties(learner), returns character vector. learner properties defined follows: numerics, factors, ordered Can numeric, factor ordered factor features handled? functionals Can arbitrary number functional features handled? single.functional Can exactly one functional feature handled? missings Can missing values features handled? weights Can observations weighted fitting? oneclas, twoclass, multiclass classif: Can one-class, two-class multi-class classification problems handled? class.weights classif: Can class weights handled? rcens, lcens, icens surv: Can right, left, interval censored data handled? prob classif, cluster, multilabel, surv: Can probabilites predicted? se regr: Can standard errors predicted? oobpreds classif, regr surv: Can bag predictions extracted trained model? featimp classif, regr, surv: model support extracting information feature importance?","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/LearnerProperties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query properties of learners. — LearnerProperties","text":"","code":"getLearnerProperties(learner)  hasLearnerProperties(learner, props)"},{"path":"https://mlr.mlr-org.com/dev/reference/LearnerProperties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query properties of learners. — LearnerProperties","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. props (character) Vector properties query.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/LearnerProperties.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query properties of learners. — LearnerProperties","text":"getLearnerProperties returns character vector learner properties. hasLearnerProperties returns logical vector length props.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/MeasureProperties.html","id":null,"dir":"Reference","previous_headings":"","what":"Query properties of measures. — MeasureProperties","title":"Query properties of measures. — MeasureProperties","text":"Properties can accessed getMeasureProperties(measure), returns character vector. measure properties defined Measure.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/MeasureProperties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query properties of measures. — MeasureProperties","text":"","code":"getMeasureProperties(measure)  hasMeasureProperties(measure, props)"},{"path":"https://mlr.mlr-org.com/dev/reference/MeasureProperties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query properties of measures. — MeasureProperties","text":"measure (Measure) Performance measure. Default first measure used benchmark experiment. props (character) Vector properties query.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/MeasureProperties.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query properties of measures. — MeasureProperties","text":"getMeasureProperties returns character vector measure properties. hasMeasureProperties returns logical vector length props.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/MultilabelTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a multilabel task. — makeMultilabelTask","title":"Create a multilabel task. — makeMultilabelTask","text":"Create multilabel task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/MultilabelTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a multilabel task. — makeMultilabelTask","text":"","code":"makeMultilabelTask(   id = deparse(substitute(data)),   data,   target,   weights = NULL,   blocking = NULL,   coordinates = NULL,   fixup.data = \"warn\",   check.data = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/MultilabelTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a multilabel task. — makeMultilabelTask","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). survival analysis names survival time event columns, length 2. multilabel classification contains names logical columns encode whether label present length corresponds number classes. weights (numeric) Optional, non-negative case weight vector used fitting. set cost-sensitive learning. Default NULL means (= equal) weights. blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/MultilabelTask.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a multilabel task. — makeMultilabelTask","text":"multilabel classification assume presence labels encoded via logical columns data. name column specifies name label. target char vector points columns.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/MultilabelTask.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Create a multilabel task. — makeMultilabelTask","text":"multilabel classification assume presence labels encoded via logical columns data. name column specifies name label. target char vector points columns.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/Prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction object. — Prediction","title":"Prediction object. — Prediction","text":"Result predict.WrappedModel. Use .data.frame access information convenient format. function getPredictionProbabilities useful access predicted probabilities. data member object contains always following columns: id, index numbers predicted cases task, response either numeric factor, predicted response values, truth, either numeric factor, true target values. probabilities predicted, many numeric columns classes named prob.classname. standard errors predicted, numeric column named se. constructor makePrediction mainly internal use. Object members: predict.type (character(1)) Type set setPredictType. data (data.frame) See details. threshold (numeric(1)) Threshold set predict function. task.desc (TaskDesc) Task description object. time (numeric(1)) Time learner needed generate predictions. error (character(1)) error messages generated learner (default NA_character_). Internal, use!","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/Prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction object. — Prediction","text":"","code":"makePrediction(   task.desc,   row.names,   id,   truth,   predict.type,   predict.threshold = NULL,   y,   time,   error = NA_character_,   dump = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/RLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal construction / wrapping of learner object. — RLearner","title":"Internal construction / wrapping of learner object. — RLearner","text":"Wraps already implemented learning method R make accessible mlr. Call method constructor. pass id (name), required package(s), description object changeable parameters (learner work, strongly recommended), use property tags define features learner. general overview integrate learning algorithm mlr's system, please read section online tutorial: https://mlr.mlr-org.com/articles/tutorial/create_learner.html see possible properties learner, go : LearnerProperties.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/RLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal construction / wrapping of learner object. — RLearner","text":"","code":"makeRLearner()  makeRLearnerClassif(   cl,   package,   par.set,   par.vals = list(),   properties = character(0L),   name = cl,   short.name = cl,   note = \"\",   class.weights.param = NULL,   callees = character(0L) )  makeRLearnerMultilabel(   cl,   package,   par.set,   par.vals = list(),   properties = character(0L),   name = cl,   short.name = cl,   note = \"\",   callees = character(0L) )  makeRLearnerRegr(   cl,   package,   par.set,   par.vals = list(),   properties = character(0L),   name = cl,   short.name = cl,   note = \"\",   callees = character(0L) )  makeRLearnerSurv(   cl,   package,   par.set,   par.vals = list(),   properties = character(0L),   name = cl,   short.name = cl,   note = \"\",   callees = character(0L) )  makeRLearnerCluster(   cl,   package,   par.set,   par.vals = list(),   properties = character(0L),   name = cl,   short.name = cl,   note = \"\",   callees = character(0L) )  makeRLearnerCostSens(   cl,   package,   par.set,   par.vals = list(),   properties = character(0L),   name = cl,   short.name = cl,   note = \"\",   callees = character(0L) )"},{"path":"https://mlr.mlr-org.com/dev/reference/RLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal construction / wrapping of learner object. — RLearner","text":"cl (character(1)) Class learner. convention, classification learners start “classif.” regression learners “regr.” survival learners start “surv.” clustering learners “cluster.” multilabel classification learners start “multilabel.”. list integrated learners available learners help page. package (character) Package(s) load implementation learner. par.set (ParamHelpers::ParamSet) Parameter set (hyper)parameters constraints. Dependent parameters requires field must use quote expression define . par.vals (list) Always set hyperparameters values object constructed. Useful default values missing underlying function. values can later overwritten user sets hyperparameters. Default empty list. properties (character) Set learner properties. See . Default character(0). name (character(1)) Meaningful name learner. Default id. short.name (character(1)) Short name learner. characters can used plots tables. Default id. note (character(1)) Additional notes regarding learner integration mlr. Default “”. class.weights.param (character(1)) Name parameter, can used providing class weights. callees (character) Character vector naming functions learner's package called relevant R help page. Default character(0).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/RLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal construction / wrapping of learner object. — RLearner","text":"(RLearner). specific subclass one RLearnerClassif, RLearnerCluster, RLearnerMultilabel, RLearnerRegr, RLearnerSurv.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/RegrTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a regression task. — makeRegrTask","title":"Create a regression task. — makeRegrTask","text":"Create regression task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/RegrTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a regression task. — makeRegrTask","text":"","code":"makeRegrTask(   id = deparse(substitute(data)),   data,   target,   weights = NULL,   blocking = NULL,   coordinates = NULL,   fixup.data = \"warn\",   check.data = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/RegrTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a regression task. — makeRegrTask","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). survival analysis names survival time event columns, length 2. multilabel classification contains names logical columns encode whether label present length corresponds number classes. weights (numeric) Optional, non-negative case weight vector used fitting. set cost-sensitive learning. Default NULL means (= equal) weights. blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/ResamplePrediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction from resampling. — ResamplePrediction","title":"Prediction from resampling. — ResamplePrediction","text":"Contains predictions resampling, returned (among stuff) function resample. Can basically used way Prediction, super class. main differences : () internal data.frame (member data) contains additional column iter, specifying iteration resampling strategy, additional columns set, specifying whether prediction observation “train” “test” set. (b) prediction time numeric vector, length equals number iterations.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/ResampleResult.html","id":null,"dir":"Reference","previous_headings":"","what":"ResampleResult object. — ResampleResult","title":"ResampleResult object. — ResampleResult","text":"container resample results.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/ResampleResult.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ResampleResult object. — ResampleResult","text":"Resample Result: resample result created resample contains following object members: task.id (character(1)): Name Task. learner.id (character(1)): Name Learner. measures.test (data.frame): Gives access performance measurements individual test sets. Rows correspond sets resampling iterations, columns performance measures. measures.train (data.frame): Gives access performance measurements individual training sets. Rows correspond sets resampling iterations, columns performance measures. Usually available, specifically requested, see general description . aggr (numeric): Named vector aggregated performance values. Names coded like <measure>.<aggregation>. err.msgs (data.frame): Number rows equals resampling iterations columns : iter, train, predict. Stores error messages generated train predict, caught via configureMlr. err.dumps (list list dump.frames): List length equal number resampling iterations. Contains lists dump.frames objects can fed debugger() inspect error dumps generated learner errors. One iteration can generate one error dump depending training, prediction training set, prediction test set, operations fail. Therefore lists named slots $train, $predict.train, $predict.test relevant. error dumps saved option .error.dump TRUE. pred (ResamplePrediction): Container predictions resampling. models [list WrappedModel): List fitted models NULL. extract (list): List extracted parts fitted models NULL. runtime (numeric(1)): Time seconds took execute resampling. print method object gives short overview, including task learner ids, aggregated measures runtime resampling.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/SurvTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a survival task. — makeSurvTask","title":"Create a survival task. — makeSurvTask","text":"Create survival task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/SurvTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a survival task. — makeSurvTask","text":"","code":"makeSurvTask(   id = deparse(substitute(data)),   data,   target,   weights = NULL,   blocking = NULL,   coordinates = NULL,   fixup.data = \"warn\",   check.data = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/SurvTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a survival task. — makeSurvTask","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). survival analysis names survival time event columns, length 2. multilabel classification contains names logical columns encode whether label present length corresponds number classes. weights (numeric) Optional, non-negative case weight vector used fitting. set cost-sensitive learning. Default NULL means (= equal) weights. blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/Task.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a classification, regression, survival, cluster, cost-sensitive classification or\nmultilabel task. — Task","title":"Create a classification, regression, survival, cluster, cost-sensitive classification or\nmultilabel task. — Task","text":"task encapsulates data specifies - subclasses - type task. also contains description object detailing aspects data. Useful operators : getTaskFormula, getTaskFeatureNames, getTaskData, getTaskTargets, subsetTask. Object members: env (environment) Environment data task stored. Use getTaskData order access . weights (numeric) See argument. NULL present. blocking (factor) See argument. NULL present. task.desc (TaskDesc) Encapsulates information task. Functional data can added task via matrix columns. information refer makeFunctionalData.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/Task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a classification, regression, survival, cluster, cost-sensitive classification or\nmultilabel task. — Task","text":"id (character(1)) Id string object. Default name R variable passed data. data (data.frame) data frame containing features target variable(s). target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). survival analysis names survival time event columns, length 2. multilabel classification contains names logical columns encode whether label present length corresponds number classes. costs (data.frame) numeric matrix data frame containing costs misclassification. assume general case observation specific costs. means n rows, corresponding observations, order data. columns correspond classes names class labels (unnamed use y1 yk labels). entry (,j) matrix specifies cost predicting class j observation . weights (numeric) Optional, non-negative case weight vector used fitting. set cost-sensitive learning. Default NULL means (= equal) weights. blocking (factor) optional factor length number observations. Observations blocking level “belong together”. Specifically, either put training test set resampling iteration. Default NULL means blocking. positive (character(1)) Positive class binary classification (otherwise ignored set NA). Default first factor level target attribute. fixup.data (character(1)) basic cleaning data performed? Currently means removing empty factor levels columns. Possible choices : “” = . “warn” = warn . “quiet” = keep silent. Default “warn”. check.data (logical(1)) sanity data checked initially task creation? good reasons turn (one might speed). Default TRUE. coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided data.frame needs number rows data consist least two dimensions.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/Task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a classification, regression, survival, cluster, cost-sensitive classification or\nmultilabel task. — Task","text":"Task.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/Task.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a classification, regression, survival, cluster, cost-sensitive classification or\nmultilabel task. — Task","text":"","code":"if (requireNamespace(\"mlbench\")) {   library(mlbench)   data(BostonHousing)   data(Ionosphere)    makeClassifTask(data = iris, target = \"Species\")   makeRegrTask(data = BostonHousing, target = \"medv\")   # an example of a classification task with more than those standard arguments:   blocking = factor(c(rep(1, 51), rep(2, 300)))   makeClassifTask(id = \"myIonosphere\", data = Ionosphere, target = \"Class\",     positive = \"good\", blocking = blocking)   makeClusterTask(data = iris[, -5L]) } #> Loading required namespace: mlbench #> Unsupervised task: iris[, -5L] #> Type: cluster #> Observations: 150 #> Features: #>    numerics     factors     ordered functionals  #>           4           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/reference/TaskDesc.html","id":null,"dir":"Reference","previous_headings":"","what":"Description object for task. — TaskDesc","title":"Description object for task. — TaskDesc","text":"Description object task, encapsulates basic properties task without store complete data set.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/TaskDesc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Description object for task. — TaskDesc","text":"Object members: id (character(1)) Id string task. type (character(1)) Type task, “classif” classification, “regr” regression, “surv” survival “cluster” cluster analysis, “costsens” cost-sensitive classification, “multilabel” multilabel classification. target (character(0) | character(1) | character(2) | character(n.classes)) Name(s) target variable(s). “surv” names survival time event columns, length 2. “costsens” length 0, target column, cost matrix instead. “multilabel” names logical columns indicate whether class label present number target variables corresponds number classes. size (integer(1)) Number cases data set. n.feat (integer(2)) Number features, named vector entries: “numerics”, “factors”, “ordered”, “functionals”. .missings (logical(1)) missing values present? .weights (logical(1)) weights specified observation? .blocking (logical(1)) blocking factor cases available task? class.levels (character) possible classes. present “classif”, “costsens”, “multilabel”. positive (character(1)) Positive class label binary classification. present “classif”, NA multiclass. negative (character(1)) Negative class label binary classification. present “classif”, NA multiclass.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/TuneControl.html","id":null,"dir":"Reference","previous_headings":"","what":"Control object for tuning — TuneControl","title":"Control object for tuning — TuneControl","text":"General tune control object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/TuneControl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Control object for tuning — TuneControl","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. start (list) Named list initial parameter values. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. ... () control parameters passed control arguments cmaes::cma_es GenSA::GenSA, well towards tunerConfig argument irace::irace.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/TuneMultiCritControl.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control structures for multi-criteria tuning. — TuneMultiCritControl","title":"Create control structures for multi-criteria tuning. — TuneMultiCritControl","text":"following tuners available: makeTuneMultiCritControlGrid Grid search. kinds parameter types can handled. can either use correct param type resolution, discretize always using ParamHelpers::makeDiscreteParam par.set passed tuneParams. makeTuneMultiCritControlRandom Random search. kinds parameter types can handled. makeTuneMultiCritControlNSGA2 Evolutionary method mco::nsga2. Can handle numeric(vector) integer(vector) hyperparameters, dependencies. integers internally proposed numeric values automatically rounded. makeTuneMultiCritControlMBO Model-based/ Bayesian optimization. kinds parameter types can handled.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/TuneMultiCritControl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control structures for multi-criteria tuning. — TuneMultiCritControl","text":"","code":"makeTuneMultiCritControlGrid(   same.resampling.instance = TRUE,   resolution = 10L,   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL )  makeTuneMultiCritControlMBO(   n.objectives = mbo.control$n.objectives,   same.resampling.instance = TRUE,   impute.val = NULL,   learner = NULL,   mbo.control = NULL,   tune.threshold = FALSE,   tune.threshold.args = list(),   continue = FALSE,   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL,   mbo.design = NULL )  makeTuneMultiCritControlNSGA2(   same.resampling.instance = TRUE,   impute.val = NULL,   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL,   ... )  makeTuneMultiCritControlRandom(   same.resampling.instance = TRUE,   maxit = 100L,   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/TuneMultiCritControl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control structures for multi-criteria tuning. — TuneMultiCritControl","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. resolution (integer) Resolution grid numeric/integer parameter par.set. vector parameters, resolution per dimension. Either pass one resolution parameters, named vector. See ParamHelpers::generateGridDesign. Default 10. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. case makeTuneMultiCritControlGrid number must identical size grid. makeTuneMultiCritControlRandom budget equals number iterations (maxit) performed random search algorithm. case makeTuneMultiCritControlNSGA2 budget corresponds product maximum number generations (max(generations)) + 1 (initial population) size population (popsize). makeTuneMultiCritControlMBO budget equals number objective function evaluations, .e. number MBO iterations + size initial design. NULL, overwrite existing stopping conditions mbo.control. n.objectives (integer(1)) Number objectives, .e. number Measures optimize. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. learner (Learner | NULL) surrogate learner: regression learner model performance landscape. default, NULL, mlrMBO automatically create suitable learner based rules described mlrMBO::makeMBOLearner. mbo.control (mlrMBO::MBOControl | NULL) Control object model-based optimization tuning. default, NULL, control object created defaults described mlrMBO::makeMBOControl. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. continue (logical(1)) Resume calculation previous run using mlrMBO::mboContinue? Requires “save.file.path” set. Note ParamHelpers::OptPath mlrMBO::OptResult include evaluations continuation. complete OptPath found slot $mbo.result$opt.path. mbo.design (data.frame | NULL) Initial design data frame. parameters corresponding trafo functions, design must transformed passed! default, NULL, default design created like described mlrMBO::mbo. ... () control parameters passed control arguments cmaes::cma_es GenSA::GenSA, well towards tunerConfig argument irace::irace. maxit (integer(1)) Number iterations random search. Default 100.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/TuneMultiCritControl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control structures for multi-criteria tuning. — TuneMultiCritControl","text":"(TuneMultiCritControl). specific subclass one TuneMultiCritControlGrid, TuneMultiCritControlRandom, TuneMultiCritControlNSGA2, TuneMultiCritControlMBO.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/TuneMultiCritResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Result of multi-criteria tuning. — TuneMultiCritResult","title":"Result of multi-criteria tuning. — TuneMultiCritResult","text":"Container results hyperparameter tuning. Contains obtained pareto set front optimization path lead . Object members: learner (Learner) Learner optimized. control (TuneControl) Control object tuning. x (list) List lists non-dominated hyperparameter settings pareto set. Note trafos params, x always TRANSFORMED scale directly use . y (matrix) Pareto front x. threshold Currently NULL. opt.path (ParamHelpers::OptPath) Optimization path lead x. Note trafos params, opt.path always contains UNTRANSFORMED values original scale. can simply call trafoOptPath(opt.path) transform , , .data.frame{trafoOptPath(opt.path)} ind (integer(n)) Indices Pareto optimal params opt.path. measures [(list ) Measure) Performance measures.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/TuneResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Result of tuning. — TuneResult","title":"Result of tuning. — TuneResult","text":"Container results hyperparameter tuning. Contains obtained point search space, performance values optimization path lead . Object members: learner (Learner) Learner optimized. control (TuneControl) Control object tuning. x (list) Named list hyperparameter values identified optimal. Note trafos params, x always TRANSFORMED scale directly use . y (numeric) Performance values optimal x. threshold (numeric) Vector finally found used thresholds tune.threshold enabled TuneControl, otherwise present hence NULL. opt.path (ParamHelpers::OptPath) Optimization path lead x. Note trafos params, opt.path always contains UNTRANSFORMED values original scale. can simply call trafoOptPath(opt.path) transform , , .data.frame{trafoOptPath(opt.path)}. mlr option .error.dump TRUE, OptPath .dump object extra column contains error dump traces failed optimization evaluations. can accessed getOptPathEl(opt.path)$extra$.dump.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/addRRMeasure.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute new measures for existing ResampleResult — addRRMeasure","title":"Compute new measures for existing ResampleResult — addRRMeasure","text":"Adds new measures existing ResampleResult.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/addRRMeasure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute new measures for existing ResampleResult — addRRMeasure","text":"","code":"addRRMeasure(res, measures)"},{"path":"https://mlr.mlr-org.com/dev/reference/addRRMeasure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute new measures for existing ResampleResult — addRRMeasure","text":"res (ResampleResult) result resample run keep.pred = TRUE. measures (Measure | list Measure) Performance measure(s) evaluate. Default default measure task, see getDefaultMeasure.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/addRRMeasure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute new measures for existing ResampleResult — addRRMeasure","text":"(ResampleResult).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/aggregations.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregation methods. — aggregations","title":"Aggregation methods. — aggregations","text":"test.mean Mean performance values test sets. test.sd Standard deviation performance values test sets. test.median Median performance values test sets. test.min Minimum performance values test sets. test.max Maximum performance values test sets. test.sum Sum performance values test sets. train.mean Mean performance values training sets. train.sd Standard deviation performance values training sets. train.median Median performance values training sets. train.min Minimum performance values training sets. train.max Maximum performance values training sets. train.sum Sum performance values training sets. b632 Aggregation B632 bootstrap. b632plus Aggregation B632+ bootstrap. testgroup.mean Performance values test sets grouped according resampling method. mean every group calculated, mean means. Mainly used repeated CV. testgroup.sd Similar testgroup.mean - mean every group calculated, standard deviation means obtained. Mainly used repeated CV. test.join Performance measure joined test sets. especially useful small sample sizes unbalanced group sizes significant impact aggregation, especially cross-validation test.join might make sense now. repeated CV, performance calculated repetition aggregated arithmetic mean.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/agri.task.html","id":null,"dir":"Reference","previous_headings":"","what":"European Union Agricultural Workforces clustering task. — agri.task","title":"European Union Agricultural Workforces clustering task. — agri.task","text":"Contains task (agri.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/agri.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"European Union Agricultural Workforces clustering task. — agri.task","text":"See cluster::agriculture.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/analyzeFeatSelResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Show and visualize the steps of feature selection. — analyzeFeatSelResult","title":"Show and visualize the steps of feature selection. — analyzeFeatSelResult","text":"function prints steps selectFeatures took find optimal set features reason stopped. can also print information calculations done intermediate step. Currently implemented sequential feature selection.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/analyzeFeatSelResult.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show and visualize the steps of feature selection. — analyzeFeatSelResult","text":"","code":"analyzeFeatSelResult(res, reduce = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/analyzeFeatSelResult.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show and visualize the steps of feature selection. — analyzeFeatSelResult","text":"res (FeatSelResult) result selectFeatures. reduce (logical(1)) Per iteration: Print selected feature (features evaluated)? Default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/analyzeFeatSelResult.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show and visualize the steps of feature selection. — analyzeFeatSelResult","text":"(invisible(NULL)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/asROCRPrediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts predictions to a format package ROCR can handle. — asROCRPrediction","title":"Converts predictions to a format package ROCR can handle. — asROCRPrediction","text":"Converts predictions format package ROCR can handle.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/asROCRPrediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts predictions to a format package ROCR can handle. — asROCRPrediction","text":"","code":"asROCRPrediction(pred)"},{"path":"https://mlr.mlr-org.com/dev/reference/asROCRPrediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts predictions to a format package ROCR can handle. — asROCRPrediction","text":"pred (Prediction) Prediction object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/batchmark.html","id":null,"dir":"Reference","previous_headings":"","what":"Run machine learning benchmarks as distributed experiments. — batchmark","title":"Run machine learning benchmarks as distributed experiments. — batchmark","text":"function parallel version benchmark using batchtools. Experiments created provided registry combination learners, tasks resamplings. experiments stored registry runs can started via batchtools::submitJobs. job one train/test split outer resampling. case nested resampling (e.g. makeTuneWrapper), job full run inner resampling, can parallelized second step ParallelMap. details usage support backends look batchtools tutorial page: https://github.com/mllg/batchtools. general workflow batchmark looks like : Create ExperimentRegistry using batchtools::makeExperimentRegistry. Call batchmark(...) defines jobs learners tasks base::expand.grid fashion. Submit jobs using batchtools::submitJobs. Babysit computation, wait jobs finish using batchtools::waitForJobs. Call reduceBatchmarkResult() reduce results BenchmarkResult. want use OpenML datasets can generate tasks vector dataset IDs easily tasks = lapply(data.ids, function(x) convertOMLDataSetToMlr(getOMLDataSet(x))).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/batchmark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run machine learning benchmarks as distributed experiments. — batchmark","text":"","code":"batchmark(   learners,   tasks,   resamplings,   measures,   keep.pred = TRUE,   keep.extract = FALSE,   models = FALSE,   reg = batchtools::getDefaultRegistry() )"},{"path":"https://mlr.mlr-org.com/dev/reference/batchmark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run machine learning benchmarks as distributed experiments. — batchmark","text":"learners (list Learner | character) Learning algorithms compared, can also single learner. pass strings learners created via makeLearner. tasks list Task Tasks learners run . resamplings [(list ) ResampleDesc) Resampling strategy tasks. one provided, replicated match number tasks. missing, 10-fold cross validation used. measures (list Measure) Performance measures tasks. missing, default measure first task used. keep.pred (logical(1)) Keep prediction data pred slot result object. many experiments (larger data sets) objects might unnecessarily increase object size / mem usage, really need . default set TRUE. keep.extract (logical(1)) Keep extract slot result object. creating lot benchmark results extensive tuning, resulting R objects can become large size. tuning results stored extract slot removed default (keep.extract = FALSE). Note keep.extract = FALSE able conduct analysis tuning results. models (logical(1)) fitted models stored ResampleResult? Default FALSE. reg (batchtools::Registry) Registry, created batchtools::makeExperimentRegistry. explicitly passed, uses last created registry.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/batchmark.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run machine learning benchmarks as distributed experiments. — batchmark","text":"(data.table). Generated job ids stored column “job.id”.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/bc.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Wisconsin Breast Cancer classification task. — bc.task","title":"Wisconsin Breast Cancer classification task. — bc.task","text":"Contains task (bc.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/bc.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wisconsin Breast Cancer classification task. — bc.task","text":"See mlbench::BreastCancer. column \"Id\" incomplete cases removed task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/benchmark.html","id":null,"dir":"Reference","previous_headings":"","what":"Benchmark experiment for multiple learners and tasks. — benchmark","title":"Benchmark experiment for multiple learners and tasks. — benchmark","text":"Complete benchmark experiment compare different learning algorithms across one tasks w.r.t. given resampling strategy. Experiments paired, meaning always training / test sets used different learners. Furthermore, can course pass “enhanced” learners via wrappers, e.g., learner can automatically tuned using makeTuneWrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/benchmark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Benchmark experiment for multiple learners and tasks. — benchmark","text":"","code":"benchmark(   learners,   tasks,   resamplings,   measures,   keep.pred = TRUE,   keep.extract = FALSE,   models = FALSE,   show.info = getMlrOption(\"show.info\") )"},{"path":"https://mlr.mlr-org.com/dev/reference/benchmark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Benchmark experiment for multiple learners and tasks. — benchmark","text":"learners (list Learner | character) Learning algorithms compared, can also single learner. pass strings learners created via makeLearner. tasks list Task Tasks learners run . resamplings (list ResampleDesc | ResampleInstance) Resampling strategy tasks. one provided, replicated match number tasks. missing, 10-fold cross validation used. measures (list Measure) Performance measures tasks. missing, default measure first task used. keep.pred (logical(1)) Keep prediction data pred slot result object. many experiments (larger data sets) objects might unnecessarily increase object size / mem usage, really need . default set TRUE. keep.extract (logical(1)) Keep extract slot result object. creating lot benchmark results extensive tuning, resulting R objects can become large size. tuning results stored extract slot removed default (keep.extract = FALSE). Note keep.extract = FALSE able conduct analysis tuning results. models (logical(1)) fitted models stored ResampleResult? Default FALSE. show.info (logical(1)) Print verbose output console? Default set via configureMlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/benchmark.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Benchmark experiment for multiple learners and tasks. — benchmark","text":"BenchmarkResult.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/benchmark.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Benchmark experiment for multiple learners and tasks. — benchmark","text":"","code":"lrns = list(makeLearner(\"classif.lda\"), makeLearner(\"classif.rpart\")) tasks = list(iris.task, sonar.task) rdesc = makeResampleDesc(\"CV\", iters = 2L) meas = list(acc, ber) bmr = benchmark(lrns, tasks, rdesc, measures = meas) #> Task: iris-example, Learner: classif.lda #> Resampling: cross-validation #> Measures:             acc       ber        #> [Resample] iter 1:    0.9866667 0.0138889  #> [Resample] iter 2:    0.9600000 0.0375458  #>  #> Aggregated Result: acc.test.mean=0.9733333,ber.test.mean=0.0257173 #>  #> Task: Sonar-example, Learner: classif.lda #> Resampling: cross-validation #> Measures:             acc       ber        #> [Resample] iter 1:    0.6634615 0.3245008  #> [Resample] iter 2:    0.7019231 0.2940631  #>  #> Aggregated Result: acc.test.mean=0.6826923,ber.test.mean=0.3092819 #>  #> Task: iris-example, Learner: classif.rpart #> Resampling: cross-validation #> Measures:             acc       ber        #> [Resample] iter 1:    0.9600000 0.0441919  #> [Resample] iter 2:    0.9333333 0.0604396  #>  #> Aggregated Result: acc.test.mean=0.9466667,ber.test.mean=0.0523157 #>  #> Task: Sonar-example, Learner: classif.rpart #> Resampling: cross-validation #> Measures:             acc       ber        #> [Resample] iter 1:    0.7596154 0.2592166  #> [Resample] iter 2:    0.7596154 0.2384045  #>  #> Aggregated Result: acc.test.mean=0.7596154,ber.test.mean=0.2488105 #>  rmat = convertBMRToRankMatrix(bmr) print(rmat) #>               Sonar-example iris-example #> classif.lda               2            1 #> classif.rpart             1            2 plotBMRSummary(bmr)  plotBMRBoxplots(bmr, ber, style = \"violin\") #> Warning: `fun.y` is deprecated. Use `fun` instead. #> Warning: `fun.ymin` is deprecated. Use `fun.min` instead. #> Warning: `fun.ymax` is deprecated. Use `fun.max` instead.  plotBMRRanksAsBarChart(bmr, pos = \"stack\")  friedmanTestBMR(bmr) #>  #> \tFriedman rank sum test #>  #> data:  acc.test.mean and learner.id and task.id #> Friedman chi-squared = 0, df = 1, p-value = 1 #>  friedmanPostHocTestBMR(bmr, p.value = 0.05) #> Loading required package: PMCMRplus #> Warning: Cannot reject null hypothesis of overall Friedman test, #>              returning overall Friedman test. #>  #> \tFriedman rank sum test #>  #> data:  acc.test.mean and learner.id and task.id #> Friedman chi-squared = 0, df = 1, p-value = 1 #>"},{"path":"https://mlr.mlr-org.com/dev/reference/bh.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Boston Housing regression task. — bh.task","title":"Boston Housing regression task. — bh.task","text":"Contains task (bh.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/bh.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Boston Housing regression task. — bh.task","text":"See mlbench::BostonHousing.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/cache_helpers.html","id":null,"dir":"Reference","previous_headings":"","what":"Get or delete mlr cache directory — cache_helpers","title":"Get or delete mlr cache directory — cache_helpers","text":"Helper functions deal mlr caching.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/cache_helpers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get or delete mlr cache directory — cache_helpers","text":"","code":"getCacheDir()  deleteCacheDir()"},{"path":"https://mlr.mlr-org.com/dev/reference/cache_helpers.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get or delete mlr cache directory — cache_helpers","text":"getCacheDir() returns default mlr cache directory deleteCacheDir() clears default mlr cache directory. Custom cache directories must deleted hand.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateConfusionMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Confusion matrix. — calculateConfusionMatrix","title":"Confusion matrix. — calculateConfusionMatrix","text":"Calculates confusion matrix (possibly resampled) prediction. Rows indicate true classes, columns predicted classes. marginal elements count number classification errors respective row column, .e., number errors condition corresponding true (rows) predicted (columns) class. last bottom right element displays total amount errors. list returned contains multiple matrices. relative = TRUE compute three matrices, one absolute values two relative. relative confusion matrices normalized based rows columns respectively, FALSE compute absolute value matrix. print function returns relative matrices compact way row column marginals can seen one matrix. details see ConfusionMatrix. Note resampling aggregation currently performed. predictions test sets joined vector yhat, labels joined vector y. yhat simply tabulated vs. y, computed single test set. probably mainly makes sense cross-validation used resampling.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateConfusionMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confusion matrix. — calculateConfusionMatrix","text":"","code":"calculateConfusionMatrix(pred, relative = FALSE, sums = FALSE, set = \"both\")  # S3 method for ConfusionMatrix print(x, both = TRUE, digits = 2, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/calculateConfusionMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confusion matrix. — calculateConfusionMatrix","text":"pred (Prediction) Prediction object. relative (logical(1)) TRUE two additional matrices calculated. One normalized rows one columns. sums (logical(1)) TRUE add absolute number observations group. set (character(1)) Specifies part(s) data used calculation. set equals train test, pred object must result resampling, otherwise error thrown. Defaults “”. Possible values “train”, “test”, “”. x (ConfusionMatrix) Object print. (logical(1)) TRUE absolute relative confusion matrices printed. digits (integer(1)) many numbers decimal point printed, relevant relative confusion matrices. ... () Currently used.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateConfusionMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confusion matrix. — calculateConfusionMatrix","text":"(ConfusionMatrix).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateConfusionMatrix.html","id":"methods-by-generic-","dir":"Reference","previous_headings":"","what":"Methods (by generic)","title":"Confusion matrix. — calculateConfusionMatrix","text":"print:","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/calculateConfusionMatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confusion matrix. — calculateConfusionMatrix","text":"","code":"# get confusion matrix after simple manual prediction allinds = 1:150 train = sample(allinds, 75) test = setdiff(allinds, train) mod = train(\"classif.lda\", iris.task, subset = train) pred = predict(mod, iris.task, subset = test) print(calculateConfusionMatrix(pred)) #>             predicted #> true         setosa versicolor virginica -err.- #>   setosa         26          0         0      0 #>   versicolor      0         24         2      2 #>   virginica       0          1        22      1 #>   -err.-          0          1         2      3 print(calculateConfusionMatrix(pred, sums = TRUE)) #>            setosa versicolor virginica -err.- -n- #> setosa         26          0         0      0  26 #> versicolor      0         24         2      2  26 #> virginica       0          1        22      1  23 #> -err.-          0          1         2      3  NA #> -n-            26         25        24     NA  75 print(calculateConfusionMatrix(pred, relative = TRUE)) #> Relative confusion matrix (normalized by row/column): #>             predicted #> true         setosa    versicolor virginica -err.-    #>   setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00      #>   versicolor 0.00/0.00 0.92/0.96  0.08/0.08 0.08      #>   virginica  0.00/0.00 0.04/0.04  0.96/0.92 0.04      #>   -err.-          0.00      0.04       0.08 0.04      #>  #>  #> Absolute confusion matrix: #>             predicted #> true         setosa versicolor virginica -err.- #>   setosa         26          0         0      0 #>   versicolor      0         24         2      2 #>   virginica       0          1        22      1 #>   -err.-          0          1         2      3  # now after cross-validation r = crossval(\"classif.lda\", iris.task, iters = 2L) #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.0533333  #> [Resample] iter 2:    0.0133333  #>  #> Aggregated Result: mmce.test.mean=0.0333333 #>  print(calculateConfusionMatrix(r$pred)) #>             predicted #> true         setosa versicolor virginica -err.- #>   setosa         50          0         0      0 #>   versicolor      0         48         2      2 #>   virginica       0          3        47      3 #>   -err.-          0          3         2      5"},{"path":"https://mlr.mlr-org.com/dev/reference/calculateROCMeasures.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate receiver operator measures. — calculateROCMeasures","title":"Calculate receiver operator measures. — calculateROCMeasures","text":"Calculate absolute number correct/incorrect classifications following evaluation measures: tpr True positive rate (Sensitivity, Recall) fpr False positive rate (Fall-) fnr False negative rate (Miss rate) tnr True negative rate (Specificity) ppv Positive predictive value (Precision) False omission rate lrp Positive likelihood ratio (LR+) fdr False discovery rate npv Negative predictive value acc Accuracy lrm Negative likelihood ratio (LR-) dor Diagnostic odds ratio details used measures see measures also https://en.wikipedia.org/wiki/Receiver_operating_characteristic. element false omission rate resulting object called fomr since never used variable name object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateROCMeasures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate receiver operator measures. — calculateROCMeasures","text":"","code":"calculateROCMeasures(pred)  # S3 method for ROCMeasures print(x, abbreviations = TRUE, digits = 2, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/calculateROCMeasures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate receiver operator measures. — calculateROCMeasures","text":"pred (Prediction) Prediction object. x (ROCMeasures) Created calculateROCMeasures. abbreviations (logical(1)) TRUE short paragraph explanations used measures printed additionally. digits (integer(1)) Number digits measures rounded . ... () Currently used.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateROCMeasures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate receiver operator measures. — calculateROCMeasures","text":"(ROCMeasures). list containing two elements confusion.matrix 2 times 2 confusion matrix absolute frequencies measures, list mentioned measures.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/calculateROCMeasures.html","id":"methods-by-generic-","dir":"Reference","previous_headings":"","what":"Methods (by generic)","title":"Calculate receiver operator measures. — calculateROCMeasures","text":"print:","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/calculateROCMeasures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate receiver operator measures. — calculateROCMeasures","text":"","code":"lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") fit = train(lrn, sonar.task) pred = predict(fit, task = sonar.task) calculateROCMeasures(pred) #>     predicted #> true M        R                              #>    M 95       16        tpr: 0.86 fnr: 0.14  #>    R 10       87        fpr: 0.1  tnr: 0.9   #>      ppv: 0.9 for: 0.16 lrp: 8.3  acc: 0.88  #>      fdr: 0.1 npv: 0.84 lrm: 0.16 dor: 51.66 #>  #>  #> Abbreviations: #> tpr - True positive rate (Sensitivity, Recall) #> fpr - False positive rate (Fall-out) #> fnr - False negative rate (Miss rate) #> tnr - True negative rate (Specificity) #> ppv - Positive predictive value (Precision) #> for - False omission rate #> lrp - Positive likelihood ratio (LR+) #> fdr - False discovery rate #> npv - Negative predictive value #> acc - Accuracy #> lrm - Negative likelihood ratio (LR-) #> dor - Diagnostic odds ratio"},{"path":"https://mlr.mlr-org.com/dev/reference/capLargeValues.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert large/infinite numeric values in a data.frame or task. — capLargeValues","title":"Convert large/infinite numeric values in a data.frame or task. — capLargeValues","text":"Convert numeric entries large/infinite (absolute) values data.frame task. numeric/integer columns affected.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/capLargeValues.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert large/infinite numeric values in a data.frame or task. — capLargeValues","text":"","code":"capLargeValues(   obj,   target = character(0L),   cols = NULL,   threshold = Inf,   impute = threshold,   what = \"abs\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/capLargeValues.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert large/infinite numeric values in a data.frame or task. — capLargeValues","text":"obj (data.frame | Task) Input data. target (character) Name column(s) specifying response. Target columns capped. Default character(0). cols (character) columns convert. Default numeric columns. threshold (numeric(1)) Threshold capping. Every entry whose absolute value equal larger converted. Default Inf. impute (numeric(1)) Replacement value large entries. Large negative entries converted -impute. Default threshold. (character(1)) kind entries affected? “abs” means abs(x) > threshold, “pos” means abs(x) > threshold && x > 0, “neg” means abs(x) > threshold && x < 0. Default “abs”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/capLargeValues.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert large/infinite numeric values in a data.frame or task. — capLargeValues","text":"(data.frame)","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/capLargeValues.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert large/infinite numeric values in a data.frame or task. — capLargeValues","text":"","code":"capLargeValues(iris, threshold = 5, impute = 5) #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species #> 1            5.0         3.5          1.4         0.2     setosa #> 2            4.9         3.0          1.4         0.2     setosa #> 3            4.7         3.2          1.3         0.2     setosa #> 4            4.6         3.1          1.5         0.2     setosa #> 5            5.0         3.6          1.4         0.2     setosa #> 6            5.0         3.9          1.7         0.4     setosa #> 7            4.6         3.4          1.4         0.3     setosa #> 8            5.0         3.4          1.5         0.2     setosa #> 9            4.4         2.9          1.4         0.2     setosa #> 10           4.9         3.1          1.5         0.1     setosa #> 11           5.0         3.7          1.5         0.2     setosa #> 12           4.8         3.4          1.6         0.2     setosa #> 13           4.8         3.0          1.4         0.1     setosa #> 14           4.3         3.0          1.1         0.1     setosa #> 15           5.0         4.0          1.2         0.2     setosa #> 16           5.0         4.4          1.5         0.4     setosa #> 17           5.0         3.9          1.3         0.4     setosa #> 18           5.0         3.5          1.4         0.3     setosa #> 19           5.0         3.8          1.7         0.3     setosa #> 20           5.0         3.8          1.5         0.3     setosa #> 21           5.0         3.4          1.7         0.2     setosa #> 22           5.0         3.7          1.5         0.4     setosa #> 23           4.6         3.6          1.0         0.2     setosa #> 24           5.0         3.3          1.7         0.5     setosa #> 25           4.8         3.4          1.9         0.2     setosa #> 26           5.0         3.0          1.6         0.2     setosa #> 27           5.0         3.4          1.6         0.4     setosa #> 28           5.0         3.5          1.5         0.2     setosa #> 29           5.0         3.4          1.4         0.2     setosa #> 30           4.7         3.2          1.6         0.2     setosa #> 31           4.8         3.1          1.6         0.2     setosa #> 32           5.0         3.4          1.5         0.4     setosa #> 33           5.0         4.1          1.5         0.1     setosa #> 34           5.0         4.2          1.4         0.2     setosa #> 35           4.9         3.1          1.5         0.2     setosa #> 36           5.0         3.2          1.2         0.2     setosa #> 37           5.0         3.5          1.3         0.2     setosa #> 38           4.9         3.6          1.4         0.1     setosa #> 39           4.4         3.0          1.3         0.2     setosa #> 40           5.0         3.4          1.5         0.2     setosa #> 41           5.0         3.5          1.3         0.3     setosa #> 42           4.5         2.3          1.3         0.3     setosa #> 43           4.4         3.2          1.3         0.2     setosa #> 44           5.0         3.5          1.6         0.6     setosa #> 45           5.0         3.8          1.9         0.4     setosa #> 46           4.8         3.0          1.4         0.3     setosa #> 47           5.0         3.8          1.6         0.2     setosa #> 48           4.6         3.2          1.4         0.2     setosa #> 49           5.0         3.7          1.5         0.2     setosa #> 50           5.0         3.3          1.4         0.2     setosa #> 51           5.0         3.2          4.7         1.4 versicolor #> 52           5.0         3.2          4.5         1.5 versicolor #> 53           5.0         3.1          4.9         1.5 versicolor #> 54           5.0         2.3          4.0         1.3 versicolor #> 55           5.0         2.8          4.6         1.5 versicolor #> 56           5.0         2.8          4.5         1.3 versicolor #> 57           5.0         3.3          4.7         1.6 versicolor #> 58           4.9         2.4          3.3         1.0 versicolor #> 59           5.0         2.9          4.6         1.3 versicolor #> 60           5.0         2.7          3.9         1.4 versicolor #> 61           5.0         2.0          3.5         1.0 versicolor #> 62           5.0         3.0          4.2         1.5 versicolor #> 63           5.0         2.2          4.0         1.0 versicolor #> 64           5.0         2.9          4.7         1.4 versicolor #> 65           5.0         2.9          3.6         1.3 versicolor #> 66           5.0         3.1          4.4         1.4 versicolor #> 67           5.0         3.0          4.5         1.5 versicolor #> 68           5.0         2.7          4.1         1.0 versicolor #> 69           5.0         2.2          4.5         1.5 versicolor #> 70           5.0         2.5          3.9         1.1 versicolor #> 71           5.0         3.2          4.8         1.8 versicolor #> 72           5.0         2.8          4.0         1.3 versicolor #> 73           5.0         2.5          4.9         1.5 versicolor #> 74           5.0         2.8          4.7         1.2 versicolor #> 75           5.0         2.9          4.3         1.3 versicolor #> 76           5.0         3.0          4.4         1.4 versicolor #> 77           5.0         2.8          4.8         1.4 versicolor #> 78           5.0         3.0          5.0         1.7 versicolor #> 79           5.0         2.9          4.5         1.5 versicolor #> 80           5.0         2.6          3.5         1.0 versicolor #> 81           5.0         2.4          3.8         1.1 versicolor #> 82           5.0         2.4          3.7         1.0 versicolor #> 83           5.0         2.7          3.9         1.2 versicolor #> 84           5.0         2.7          5.0         1.6 versicolor #> 85           5.0         3.0          4.5         1.5 versicolor #> 86           5.0         3.4          4.5         1.6 versicolor #> 87           5.0         3.1          4.7         1.5 versicolor #> 88           5.0         2.3          4.4         1.3 versicolor #> 89           5.0         3.0          4.1         1.3 versicolor #> 90           5.0         2.5          4.0         1.3 versicolor #> 91           5.0         2.6          4.4         1.2 versicolor #> 92           5.0         3.0          4.6         1.4 versicolor #> 93           5.0         2.6          4.0         1.2 versicolor #> 94           5.0         2.3          3.3         1.0 versicolor #> 95           5.0         2.7          4.2         1.3 versicolor #> 96           5.0         3.0          4.2         1.2 versicolor #> 97           5.0         2.9          4.2         1.3 versicolor #> 98           5.0         2.9          4.3         1.3 versicolor #> 99           5.0         2.5          3.0         1.1 versicolor #> 100          5.0         2.8          4.1         1.3 versicolor #> 101          5.0         3.3          5.0         2.5  virginica #> 102          5.0         2.7          5.0         1.9  virginica #> 103          5.0         3.0          5.0         2.1  virginica #> 104          5.0         2.9          5.0         1.8  virginica #> 105          5.0         3.0          5.0         2.2  virginica #> 106          5.0         3.0          5.0         2.1  virginica #> 107          4.9         2.5          4.5         1.7  virginica #> 108          5.0         2.9          5.0         1.8  virginica #> 109          5.0         2.5          5.0         1.8  virginica #> 110          5.0         3.6          5.0         2.5  virginica #> 111          5.0         3.2          5.0         2.0  virginica #> 112          5.0         2.7          5.0         1.9  virginica #> 113          5.0         3.0          5.0         2.1  virginica #> 114          5.0         2.5          5.0         2.0  virginica #> 115          5.0         2.8          5.0         2.4  virginica #> 116          5.0         3.2          5.0         2.3  virginica #> 117          5.0         3.0          5.0         1.8  virginica #> 118          5.0         3.8          5.0         2.2  virginica #> 119          5.0         2.6          5.0         2.3  virginica #> 120          5.0         2.2          5.0         1.5  virginica #> 121          5.0         3.2          5.0         2.3  virginica #> 122          5.0         2.8          4.9         2.0  virginica #> 123          5.0         2.8          5.0         2.0  virginica #> 124          5.0         2.7          4.9         1.8  virginica #> 125          5.0         3.3          5.0         2.1  virginica #> 126          5.0         3.2          5.0         1.8  virginica #> 127          5.0         2.8          4.8         1.8  virginica #> 128          5.0         3.0          4.9         1.8  virginica #> 129          5.0         2.8          5.0         2.1  virginica #> 130          5.0         3.0          5.0         1.6  virginica #> 131          5.0         2.8          5.0         1.9  virginica #> 132          5.0         3.8          5.0         2.0  virginica #> 133          5.0         2.8          5.0         2.2  virginica #> 134          5.0         2.8          5.0         1.5  virginica #> 135          5.0         2.6          5.0         1.4  virginica #> 136          5.0         3.0          5.0         2.3  virginica #> 137          5.0         3.4          5.0         2.4  virginica #> 138          5.0         3.1          5.0         1.8  virginica #> 139          5.0         3.0          4.8         1.8  virginica #> 140          5.0         3.1          5.0         2.1  virginica #> 141          5.0         3.1          5.0         2.4  virginica #> 142          5.0         3.1          5.0         2.3  virginica #> 143          5.0         2.7          5.0         1.9  virginica #> 144          5.0         3.2          5.0         2.3  virginica #> 145          5.0         3.3          5.0         2.5  virginica #> 146          5.0         3.0          5.0         2.3  virginica #> 147          5.0         2.5          5.0         1.9  virginica #> 148          5.0         3.0          5.0         2.0  virginica #> 149          5.0         3.4          5.0         2.3  virginica #> 150          5.0         3.0          5.0         1.8  virginica"},{"path":"https://mlr.mlr-org.com/dev/reference/changeData.html","id":null,"dir":"Reference","previous_headings":"","what":"Change Task Data — changeData","title":"Change Task Data — changeData","text":"Mainly internal use. Changes data associated task, without modifying task properties.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/changeData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change Task Data — changeData","text":"","code":"changeData(task, data, costs, weights, coordinates)"},{"path":"https://mlr.mlr-org.com/dev/reference/changeData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change Task Data — changeData","text":"task (Task) task. data (data.frame) new data associate task. names types feature columns must match old data. costs ([data.frame` Optional: cost matrix. weights (numeric) Optional: weight vector.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/checkLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Exported for internal use only. — checkLearner","title":"Exported for internal use only. — checkLearner","text":"Exported internal use .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/checkLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exported for internal use only. — checkLearner","text":"","code":"checkLearner(learner, type = NULL, props = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/checkLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exported for internal use only. — checkLearner","text":"learner (Learner | character(1)) learner check, name learner create type (character(1)) type learner require. props (character(1)) properties require.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/checkPredictLearnerOutput.html","id":null,"dir":"Reference","previous_headings":"","what":"Check output returned by predictLearner. — checkPredictLearnerOutput","title":"Check output returned by predictLearner. — checkPredictLearnerOutput","text":"Check output coming Learner's internal predictLearner function. function internal use.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/checkPredictLearnerOutput.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check output returned by predictLearner. — checkPredictLearnerOutput","text":"","code":"checkPredictLearnerOutput(learner, model, p)"},{"path":"https://mlr.mlr-org.com/dev/reference/checkPredictLearnerOutput.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check output returned by predictLearner. — checkPredictLearnerOutput","text":"learner (Learner) learner. model (WrappedModel)] Model produced training. p () prediction made learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/checkPredictLearnerOutput.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check output returned by predictLearner. — checkPredictLearnerOutput","text":"(). sanitized version p.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/configureMlr.html","id":null,"dir":"Reference","previous_headings":"","what":"Configures the behavior of the package. — configureMlr","title":"Configures the behavior of the package. — configureMlr","text":"Configuration done setting custom options. set option , current value kept. call function empty argument list, everything set defaults.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/configureMlr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configures the behavior of the package. — configureMlr","text":"","code":"configureMlr(   show.info,   on.learner.error,   on.learner.warning,   on.par.without.desc,   on.par.out.of.bounds,   on.measure.not.applicable,   show.learner.output,   on.error.dump )"},{"path":"https://mlr.mlr-org.com/dev/reference/configureMlr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configures the behavior of the package. — configureMlr","text":"show.info (logical(1)) methods mlr support show.info argument enable verbose output console. option sets default value arguments. Setting argument manually one functions overwrite default value specific function call. Default TRUE. .learner.error (character(1)) happen error underlying learning algorithm caught: “stop”: R exception generated. “warn”: FailureModel created, predicts NAs warning generated. “quiet”: “warn” without warning. Default “stop”. .learner.warning (character(1)) happen warning underlying learning algorithm generated: “warn”: warning generated usual. “quiet”: warning suppressed. Default “warn”. .par.without.desc (character(1)) happen parameter learner set value, parameter description object exists, indicating possibly wrong name: “stop”: R exception generated. “warn”: Warning, parameter still passed along learner. “quiet”: “warn” without warning. Default “stop”. .par...bounds (character(1)) happen parameter learner set bounds value. “stop”: R exception generated. “warn”: Warning, parameter still passed along learner. “quiet”: “warn” without warning. Default “stop”. .measure..applicable (logical(1)) happen measure applicable learner. “stop”: R exception generated. “warn”: Warning, value measure NA. “quiet”: “warn” without warning. Default “stop”. show.learner.output (logical(1)) output learning algorithm training prediction shown captured suppressed? Default TRUE. .error.dump (logical(1)) Specify whether FailureModel models failed predictions contain error dump can used debugger inspect error. option effective .learner.error “warn” “quiet”. TRUE, dump can accessed using getFailureModelDump FailureModel, getPredictionDump failed prediction, getRRDump resample predictions. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/configureMlr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configures the behavior of the package. — configureMlr","text":"(invisible(NULL)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/convertBMRToRankMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert BenchmarkResult to a rank-matrix. — convertBMRToRankMatrix","title":"Convert BenchmarkResult to a rank-matrix. — convertBMRToRankMatrix","text":"Computes matrix ranks different algorithms different datasets (tasks). Ranks computed aggregated measures. Smaller ranks imply better methods, measures minimized, small ranks imply small scores. measures maximized, small ranks imply large scores.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/convertBMRToRankMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert BenchmarkResult to a rank-matrix. — convertBMRToRankMatrix","text":"","code":"convertBMRToRankMatrix(   bmr,   measure = NULL,   ties.method = \"average\",   aggregation = \"default\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/convertBMRToRankMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert BenchmarkResult to a rank-matrix. — convertBMRToRankMatrix","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. ties.method (character(1)) See base::rank details. aggregation (character(1))  “mean” “default”. See getBMRAggrPerformances details “default”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/convertBMRToRankMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert BenchmarkResult to a rank-matrix. — convertBMRToRankMatrix","text":"(matrix) measure ranks entries. matrix one row learner, one column task.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/convertBMRToRankMatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert BenchmarkResult to a rank-matrix. — convertBMRToRankMatrix","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/convertMLBenchObjToTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a machine learning benchmark / demo object from package mlbench to a task. — convertMLBenchObjToTask","title":"Convert a machine learning benchmark / demo object from package mlbench to a task. — convertMLBenchObjToTask","text":"auto-set target column, drop column called “Id” convert logicals factors.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/convertMLBenchObjToTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a machine learning benchmark / demo object from package mlbench to a task. — convertMLBenchObjToTask","text":"","code":"convertMLBenchObjToTask(x, n = 100L, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/convertMLBenchObjToTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a machine learning benchmark / demo object from package mlbench to a task. — convertMLBenchObjToTask","text":"x (character(1)) Name mlbench function dataset. n (integer(1)) Number observations data simul functions. Note mlbench function setting exactly respected mlbench. Default 100. ... () Passed data simul functions.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/convertMLBenchObjToTask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a machine learning benchmark / demo object from package mlbench to a task. — convertMLBenchObjToTask","text":"","code":"print(convertMLBenchObjToTask(\"Ionosphere\")) #> Supervised task: Ionosphere #> Type: classif #> Target: Class #> Observations: 351 #> Features: #>    numerics     factors     ordered functionals  #>          32           2           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 2 #>  bad good  #>  126  225  #> Positive class: bad print(convertMLBenchObjToTask(\"mlbench.spirals\", n = 100, sd = 0.1)) #> Supervised task: mlbench.spirals #> Type: classif #> Target: classes #> Observations: 100 #> Features: #>    numerics     factors     ordered functionals  #>           2           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 2 #>  1  2  #> 50 50  #> Positive class: 1"},{"path":"https://mlr.mlr-org.com/dev/reference/costiris.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Iris cost-sensitive classification task. — costiris.task","title":"Iris cost-sensitive classification task. — costiris.task","text":"Contains task (costiris.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/costiris.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Iris cost-sensitive classification task. — costiris.task","text":"See datasets::iris. cost matrix generated artificially following Tu, H.-H. Lin, H.-T. (2010), One-sided support vector regression multiclass cost-sensitive classification. ICML, J. Fürnkranz T. Joachims, Eds., Omnipress, 1095--1102.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createDummyFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate dummy variables for factor features. — createDummyFeatures","title":"Generate dummy variables for factor features. — createDummyFeatures","text":"Replace factor features dummy variables. Internally model.matrix used. Non factor features left untouched passed result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createDummyFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate dummy variables for factor features. — createDummyFeatures","text":"","code":"createDummyFeatures(   obj,   target = character(0L),   method = \"1-of-n\",   cols = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/createDummyFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate dummy variables for factor features. — createDummyFeatures","text":"obj (data.frame | Task) Input data. target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). used obj data.frame, otherwise ignored. survival analysis applicable, names survival time event columns, length 2. multilabel classification names logical columns indicate whether class label present number target variables corresponds number classes. method (character(1)) Available : \"1--n\": n factor levels n dummy variables. \"reference\": n-1 dummy variables leaving first factor level variable. Default “1--n”. cols (character) Columns create dummy features . Default use columns.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createDummyFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate dummy variables for factor features. — createDummyFeatures","text":"data.frame | Task. type obj.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":null,"dir":"Reference","previous_headings":"","what":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"Visualize partitioning resample objects spatial information.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"","code":"createSpatialResamplingPlots(   task = NULL,   resample = NULL,   crs = NULL,   datum = 4326,   repetitions = 1,   color.train = \"#0072B5\",   color.test = \"#E18727\",   point.size = 0.5,   axis.text.size = 14,   x.axis.breaks = waiver(),   y.axis.breaks = waiver() )"},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"task Task  Task object. resample ResampleResult named list (multiple) ResampleResult returned resample. crs integer Coordinate reference system (EPSG code number) supplied coordinates Task. datum integer Coordinate reference system used resulting map. repetitions integer Number repetitions. color.train character Color train set. color.test character Color test set. point.size integer Point size. axis.text.size integer Font size axis labels. x.axis.breaks numeric Custom x axis breaks y.axis.breaks numeric Custom y axis breaks","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"(list 2L containing (1) multiple `gg`` objects (2) corresponding labels.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"named list given resample, names appear title fold. multiple inputs given resample, must named. function makes hard cut five columns resulting gridded plot. means resample object consists folds > 5, folds put new row. file saving, recommend use cowplot::save_plot. viewing resulting plot RStudio, margins may appear different really . Make sure save file disk inspect image. modifying axis breaks, negative values need used area located either western southern hemisphere. Use positive values northern eastern hemisphere.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"crs","dir":"Reference","previous_headings":"","what":"CRS","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"crs suitable coordinates stored Task. example, coordinates UTM, crs set UTM projection. Due limited axis space resulting grid (especially x-axis), data default projected lat/lon projection, specifically EPSG 4326. projections desired resulting map, please set argument datum accordingly. argument passed onto ggplot2::coord_sf.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"Patrick Schratz","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/createSpatialResamplingPlots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create (spatial) resampling plot objects. — createSpatialResamplingPlots","text":"","code":"# \\donttest{ rdesc = makeResampleDesc(\"SpRepCV\", folds = 5, reps = 4) r = resample(makeLearner(\"classif.qda\"), spatial.task, rdesc) #> Resampling: repeated spatial cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.5083333  #> [Resample] iter 2:    0.3354037  #> [Resample] iter 3:    0.3312102  #> [Resample] iter 4:    0.5773810  #> [Resample] iter 5:    0.2689655  #> [Resample] iter 6:    0.3333333  #> [Resample] iter 7:    0.3010204  #> [Resample] iter 8:    0.5106383  #> [Resample] iter 9:    0.3353293  #> [Resample] iter 10:   0.2719298  #> [Resample] iter 11:   0.3258427  #> [Resample] iter 12:   0.3010204  #> [Resample] iter 13:   0.2589286  #> [Resample] iter 14:   0.5368421  #> [Resample] iter 15:   0.3352941  #> [Resample] iter 16:   0.3308824  #> [Resample] iter 17:   0.5988701  #> [Resample] iter 18:   0.4375000  #> [Resample] iter 19:   0.2781955  #> [Resample] iter 20:   0.3354037  #>  #> Aggregated Result: mmce.test.mean=0.3756162 #>   ## ------------------------------------------------------------- ## single unnamed resample input with 5 folds and 2 repetitions ## -------------------------------------------------------------  plots = createSpatialResamplingPlots(spatial.task, r, crs = 32717,   repetitions = 2, x.axis.breaks = c(-79.065, -79.085),   y.axis.breaks = c(-3.970, -4)) cowplot::plot_grid(plotlist = plots[[\"Plots\"]], ncol = 5, nrow = 2,   labels = plots[[\"Labels\"]])   ## -------------------------------------------------------------------------- ## single named resample input with 5 folds and 1 repetition and 32717 datum ## --------------------------------------------------------------------------  plots = createSpatialResamplingPlots(spatial.task, list(\"Resamp\" = r),   crs = 32717, datum = 32717, repetitions = 1) cowplot::plot_grid(plotlist = plots[[\"Plots\"]], ncol = 5, nrow = 1,   labels = plots[[\"Labels\"]])   ## ------------------------------------------------------------- ## multiple named resample inputs with 5 folds and 1 repetition ## -------------------------------------------------------------  rdesc1 = makeResampleDesc(\"SpRepCV\", folds = 5, reps = 4) r1 = resample(makeLearner(\"classif.qda\"), spatial.task, rdesc1) #> Resampling: repeated spatial cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.5988701  #> [Resample] iter 2:    0.3396226  #> [Resample] iter 3:    0.2824427  #> [Resample] iter 4:    0.4366197  #> [Resample] iter 5:    0.3239437  #> [Resample] iter 6:    0.3312102  #> [Resample] iter 7:    0.2689655  #> [Resample] iter 8:    0.3354037  #> [Resample] iter 9:    0.5083333  #> [Resample] iter 10:   0.5773810  #> [Resample] iter 11:   0.2689655  #> [Resample] iter 12:   0.3312102  #> [Resample] iter 13:   0.3354037  #> [Resample] iter 14:   0.5083333  #> [Resample] iter 15:   0.5773810  #> [Resample] iter 16:   0.3333333  #> [Resample] iter 17:   0.3010204  #> [Resample] iter 18:   0.5106383  #> [Resample] iter 19:   0.3353293  #> [Resample] iter 20:   0.2719298  #>  #> Aggregated Result: mmce.test.mean=0.3888169 #>  rdesc2 = makeResampleDesc(\"RepCV\", folds = 5, reps = 4) r2 = resample(makeLearner(\"classif.qda\"), spatial.task, rdesc2) #> Resampling: repeated cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.3533333  #> [Resample] iter 2:    0.3533333  #> [Resample] iter 3:    0.3311258  #> [Resample] iter 4:    0.3333333  #> [Resample] iter 5:    0.3000000  #> [Resample] iter 6:    0.3866667  #> [Resample] iter 7:    0.2913907  #> [Resample] iter 8:    0.3400000  #> [Resample] iter 9:    0.2866667  #> [Resample] iter 10:   0.3800000  #> [Resample] iter 11:   0.2933333  #> [Resample] iter 12:   0.3000000  #> [Resample] iter 13:   0.3066667  #> [Resample] iter 14:   0.3133333  #> [Resample] iter 15:   0.4768212  #> [Resample] iter 16:   0.3000000  #> [Resample] iter 17:   0.4039735  #> [Resample] iter 18:   0.4000000  #> [Resample] iter 19:   0.2600000  #> [Resample] iter 20:   0.3066667  #>  #> Aggregated Result: mmce.test.mean=0.3358322 #>   plots = createSpatialResamplingPlots(spatial.task,   list(\"SpRepCV\" = r1, \"RepCV\" = r2), crs = 32717, repetitions = 1,   x.axis.breaks = c(-79.055, -79.085), y.axis.breaks = c(-3.975, -4)) cowplot::plot_grid(plotlist = plots[[\"Plots\"]], ncol = 5, nrow = 2,   labels = plots[[\"Labels\"]])   ## ------------------------------------------------------------------------------------- ## Complex arrangements of multiple named resample inputs with 5 folds and 1 repetition ## -------------------------------------------------------------------------------------  p1 = cowplot::plot_grid(plots[[\"Plots\"]][[1]], plots[[\"Plots\"]][[2]],   plots[[\"Plots\"]][[3]], ncol = 3, nrow = 1, labels = plots[[\"Labels\"]][1:3],   label_size = 18) p12 = cowplot::plot_grid(plots[[\"Plots\"]][[4]], plots[[\"Plots\"]][[5]],   ncol = 2, nrow = 1, labels = plots[[\"Labels\"]][4:5], label_size = 18)  p2 = cowplot::plot_grid(plots[[\"Plots\"]][[6]], plots[[\"Plots\"]][[7]],   plots[[\"Plots\"]][[8]], ncol = 3, nrow = 1, labels = plots[[\"Labels\"]][6:8],   label_size = 18) p22 = cowplot::plot_grid(plots[[\"Plots\"]][[9]], plots[[\"Plots\"]][[10]],   ncol = 2, nrow = 1, labels = plots[[\"Labels\"]][9:10], label_size = 18)  cowplot::plot_grid(p1, p12, p2, p22, ncol = 1)  # }"},{"path":"https://mlr.mlr-org.com/dev/reference/crossover.html","id":null,"dir":"Reference","previous_headings":"","what":"Crossover. — crossover","title":"Crossover. — crossover","text":"Takes two bit strings creates new one size selecting items first string second, based given rate (probability choosing element first string).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/crossover.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crossover. — crossover","text":"x (logical) First parent string. y (logical) Second parent string. rate (numeric(1)) number representing probability selecting element first string. Default 0.5.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/crossover.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crossover. — crossover","text":"(crossover).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/downsample.html","id":null,"dir":"Reference","previous_headings":"","what":"Downsample (subsample) a task or a data.frame. — downsample","title":"Downsample (subsample) a task or a data.frame. — downsample","text":"Decrease observations task ResampleInstance given percentage observations.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/downsample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Downsample (subsample) a task or a data.frame. — downsample","text":"","code":"downsample(obj, perc = 1, stratify = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/downsample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Downsample (subsample) a task or a data.frame. — downsample","text":"obj (Task | ResampleInstance) Input data ResampleInstance. perc (numeric(1)) Percentage (0, 1). Default 1. stratify (logical(1)) classification: downsampled data stratified according target classes? Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/downsample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Downsample (subsample) a task or a data.frame. — downsample","text":"([data.frame| [Task] | [ResampleInstance]). type asobj`.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/dropFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Drop some features of task. — dropFeatures","title":"Drop some features of task. — dropFeatures","text":"Drop features task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/dropFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Drop some features of task. — dropFeatures","text":"","code":"dropFeatures(task, features)"},{"path":"https://mlr.mlr-org.com/dev/reference/dropFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Drop some features of task. — dropFeatures","text":"task (Task) task. features (character) Features drop.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/dropFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Drop some features of task. — dropFeatures","text":"Task.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate relative overfitting. — estimateRelativeOverfitting","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"Estimates relative overfitting model ratio difference test train performance difference test performance -information case train performance. -information case features carry information respect prediction. simulated permuting features predictions.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"","code":"estimateRelativeOverfitting(   predish,   measures,   task,   learner = NULL,   pred.train = NULL,   iter = 1 )"},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"predish (ResampleDesc | ResamplePrediction | Prediction) Resampling strategy resampling prediction test predictions. measures (Measure | list Measure) Performance measure(s) evaluate. Default default measure task, see getDefaultMeasure. task (Task) task. learner (Learner | character(1)) learner. pass string learner created via makeLearner. pred.train (Prediction) Training predictions. needed test predictions passed. iter (integer) Iteration number. Default 1, usually need specify . needed test predictions passed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"(data.frame). Relative overfitting estimate(s), named measure(s), resampling iteration.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"Currently support classification regression tasks implemented.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"Bradley Efron Robert Tibshirani; Improvements Cross-Validation: .632+ Bootstrap Method, Journal American Statistical Association, Vol. 92, . 438. (Jun., 1997), pp. 548-560.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/estimateRelativeOverfitting.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate relative overfitting. — estimateRelativeOverfitting","text":"","code":"task = makeClassifTask(data = iris, target = \"Species\") rdesc = makeResampleDesc(\"CV\", iters = 2) estimateRelativeOverfitting(rdesc, acc, task, makeLearner(\"classif.knn\")) #>    iter relative.overfit.acc #> 1:    1          -0.06521739 #> 2:    2           0.06122449 estimateRelativeOverfitting(rdesc, acc, task, makeLearner(\"classif.lda\")) #>    iter relative.overfit.acc #> 1:    1           0.02040816 #> 2:    2          -0.02083333 rpred = resample(\"classif.knn\", task, rdesc)$pred #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.0666667  #> [Resample] iter 2:    0.0266667  #>  #> Aggregated Result: mmce.test.mean=0.0466667 #>  estimateRelativeOverfitting(rpred, acc, task) #>    iter relative.overfit.acc #> 1:    1           0.06250000 #> 2:    2          -0.06666667"},{"path":"https://mlr.mlr-org.com/dev/reference/estimateResidualVariance.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate the residual variance. — estimateResidualVariance","title":"Estimate the residual variance. — estimateResidualVariance","text":"Estimate residual variance regression model given task. regression learner provided instead model, model trained (see train) first.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/estimateResidualVariance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate the residual variance. — estimateResidualVariance","text":"","code":"estimateResidualVariance(x, task, data, target)"},{"path":"https://mlr.mlr-org.com/dev/reference/estimateResidualVariance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate the residual variance. — estimateResidualVariance","text":"x (Learner WrappedModel) Learner wrapped model. task (RegrTask) Regression task. missing, data target must supplied. data (data.frame) data frame containing features target variable. missing, task must supplied. target (character(1)) Name target variable. missing, task must supplied.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDABsignal.html","id":null,"dir":"Reference","previous_headings":"","what":"Bspline mlq features — extractFDABsignal","title":"Bspline mlq features — extractFDABsignal","text":"function extracts features functional data based Bspline fit. details refer FDboost::bsignal().","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDABsignal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bspline mlq features — extractFDABsignal","text":"","code":"extractFDABsignal(bsignal.knots = 10L, bsignal.df = 3)"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDABsignal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bspline mlq features — extractFDABsignal","text":"bsignal.knots (integer(1)) number knots bspline. bsignal.df (numeric(1)) effective degree freedom penalized bspline.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDABsignal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bspline mlq features — extractFDABsignal","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDADTWKernel.html","id":null,"dir":"Reference","previous_headings":"","what":"DTW kernel features — extractFDADTWKernel","title":"DTW kernel features — extractFDADTWKernel","text":"function extracts features functional data based DTW distance reference dataframe.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDADTWKernel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DTW kernel features — extractFDADTWKernel","text":"","code":"extractFDADTWKernel(   ref.method = \"random\",   n.refs = 0.05,   refs = NULL,   dtwwindow = 0.05 )"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDADTWKernel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DTW kernel features — extractFDADTWKernel","text":"ref.method (character(1)) reference curves obtained? Method random draws n.refs random reference curves, uses curves references. order use user-provided reference curves, parameter set fixed. n.refs (numeric(1)) Number reference curves drawn (fraction number observations training data). refs (matrix|integer(n)) Integer vector training set row indices matrix reference curves length functionals training data. Overwrites ref.method n.refs. dtwwindow (numeric(1)) Size warping window size (proportion query length).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDADTWKernel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DTW kernel features — extractFDADTWKernel","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFPCA.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract functional principal component analysis features. — extractFDAFPCA","title":"Extract functional principal component analysis features. — extractFDAFPCA","text":"function extracts functional principal components data.frame containing functional features. Uses stats::prcomp.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFPCA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract functional principal component analysis features. — extractFDAFPCA","text":"","code":"extractFDAFPCA(rank. = NULL, center = TRUE, scale. = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFPCA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract functional principal component analysis features. — extractFDAFPCA","text":"rank. (integer(1)) Number principal components extract. Default NULL center (logical(1))  data centered applying PCA? scale. (logical(1))  data scaled applying PCA?","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFPCA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract functional principal component analysis features. — extractFDAFPCA","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract features from functional data. — extractFDAFeatures","title":"Extract features from functional data. — extractFDAFeatures","text":"Extract non-functional features functional features using various methods. function extractFDAFeatures performs extraction functional features via methods specified feat.methods transforms mentioned functional (matrix) features regular data.frame columns. Additionally, “extractFDAFeatDesc” object contains learned coefficients helpful data re-extraction predict-phase returned. can used reextractFDAFeatures order extract features prediction phase.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract features from functional data. — extractFDAFeatures","text":"","code":"extractFDAFeatures(obj, target = character(0L), feat.methods = list(), ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract features from functional data. — extractFDAFeatures","text":"obj (Task | data.frame) Task data.frame extract functional features . Must contain functional features matrix columns. target (character(1)) Task target column. neccessary data.frames Default character(0). feat.methods (named list) List functional features along desired methods functional feature. “” applies extractFDAFeatures method functional feature. Names feat.methods must match column names functional features. Available feature extraction methods available family fda_featextractor. Specifying functional feature multiple times different extraction methods allows extraction different features functional. Default list() nothing. ... () hyperparameters passed feat.methods specified .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract features from functional data. — extractFDAFeatures","text":"(list) data|task (data.frame | Task) Extracted features, type obj. desc (extracFDAFeatDesc) Description object. See description details.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract features from functional data. — extractFDAFeatures","text":"description object contains slots: target character: See argument. coln character: Colum names data. fd.cols character: Functional feature names. extractFDAFeat list: Contains feature.methods relevant parameters reextraction.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract features from functional data. — extractFDAFeatures","text":"","code":"df = data.frame(x = matrix(rnorm(24), ncol = 8), y = factor(c(\"a\", \"a\", \"b\"))) fdf = makeFunctionalData(df, fd.features = list(x1 = 1:4, x2 = 5:8), exclude.cols = \"y\") task = makeClassifTask(data = fdf, target = \"y\") extracted = extractFDAFeatures(task,   feat.methods = list(\"x1\" = extractFDAFourier(), \"x2\" = extractFDAWavelets(filter = \"haar\"))) print(extracted$task) #> Supervised task: fdf #> Type: classif #> Target: y #> Observations: 3 #> Features: #>    numerics     factors     ordered functionals  #>           8           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 2 #> a b  #> 2 1  #> Positive class: a reextractFDAFeatures(task, extracted$desc) #> Supervised task: fdf #> Type: classif #> Target: y #> Observations: 3 #> Features: #>    numerics     factors     ordered functionals  #>           8           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 2 #> a b  #> 2 1  #> Positive class: a"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFourier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Fourier transform features. — extractFDAFourier","title":"Fast Fourier transform features. — extractFDAFourier","text":"function extracts features functional data based fast fourier transform. details refer stats::fft.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFourier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Fourier transform features. — extractFDAFourier","text":"","code":"extractFDAFourier(trafo.coeff = \"phase\")"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFourier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Fourier transform features. — extractFDAFourier","text":"trafo.coeff (character(1)) Specifies transformation complex frequency domain representation calculated feature representation. Must one “amplitude” “phase”. Default “phase”. phase shift returned Rad, .e. values lie [-180, 180].","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAFourier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Fourier transform features. — extractFDAFourier","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAMultiResFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiresolution feature extraction. — extractFDAMultiResFeatures","title":"Multiresolution feature extraction. — extractFDAMultiResFeatures","text":"function extracts currently mean multiple segments curve stacks features. segments length set hierachy way features cover different resolution levels.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAMultiResFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiresolution feature extraction. — extractFDAMultiResFeatures","text":"","code":"extractFDAMultiResFeatures(res.level = 3L, shift = 0.5, seg.lens = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAMultiResFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiresolution feature extraction. — extractFDAMultiResFeatures","text":"res.level (integer(1)) number resolution hierachy, length divided factor 2. shift (numeric(1)) overlapping proportion slide window one step. seg.lens (integer(1)) Curve subsequence lengths. Needs sum length functional.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAMultiResFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiresolution feature extraction. — extractFDAMultiResFeatures","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDATsfeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Time-Series Feature Heuristics — extractFDATsfeatures","title":"Time-Series Feature Heuristics — extractFDATsfeatures","text":"function extracts features functional data based known Heuristics. details refer tsfeatures::tsfeatures(). hood function uses package tsfeatures::tsfeatures(). information see Hyndman, Wang Laptev, Large-Scale Unusual Time Series Detection, ICDM 2015. Note: Currently computes following features: \"frequency\", \"stl_features\", \"entropy\", \"acf_features\", \"arch_stat\", \"crossing_points\", \"flat_spots\", \"hurst\",  \"holt_parameters\", \"lumpiness\", \"max_kl_shift\", \"max_var_shift\", \"max_level_shift\", \"stability\", \"nonlinearity\"","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDATsfeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time-Series Feature Heuristics — extractFDATsfeatures","text":"","code":"extractFDATsfeatures(   scale = TRUE,   trim = FALSE,   trim_amount = 0.1,   parallel = FALSE,   na.action = na.pass,   feats = NULL,   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDATsfeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Time-Series Feature Heuristics — extractFDATsfeatures","text":"scale (logical(1)) TRUE, time series scaled mean 0 sd 1 features computed. trim (logical(1)) TRUE, time series trimmed trim_amount features computed. Values larger trim_amount absolute value set NA. trim_amount (numeric(1)) Default level trimming trim==TRUE. parallel (logical(1)) TRUE, multiple cores (multiple sessions) used. speeds things large number time series. na.action (logical(1)) function handle missing values. Use na.interp estimate missing values feats (character) character vector function names apply time-series order extract features. Default: feats = c(\"frequency\", \"stl_features\", \"entropy\", \"acf_features\", \"arch_stat\", \"crossing_points\", \"flat_spots\", \"hurst\",  \"holt_parameters\", \"lumpiness\", \"max_kl_shift\", \"max_var_shift\", \"max_level_shift\", \"stability\", \"nonlinearity\") ... () arguments passed respective tsfeatures functions.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDATsfeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Time-Series Feature Heuristics — extractFDATsfeatures","text":"(data.frame)","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDATsfeatures.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Time-Series Feature Heuristics — extractFDATsfeatures","text":"Hyndman, Wang Laptev, Large-Scale Unusual Time Series Detection, ICDM 2015.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAWavelets.html","id":null,"dir":"Reference","previous_headings":"","what":"Discrete Wavelet transform features. — extractFDAWavelets","title":"Discrete Wavelet transform features. — extractFDAWavelets","text":"function extracts discrete wavelet transform coefficients raw functional data. See wavelets::dwt information.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAWavelets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discrete Wavelet transform features. — extractFDAWavelets","text":"","code":"extractFDAWavelets(filter = \"la8\", boundary = \"periodic\")"},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAWavelets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discrete Wavelet transform features. — extractFDAWavelets","text":"filter (character(1)) Specifies filter used. Must one d|la|bl|c followed even number level filter. level filter needs smaller equal time-series length. information acceptable filters see help(wt.filter). Defaults la8. boundary (character(1)) Boundary used. “periodic” assumes circular time series, “reflection” series extended twice length. Default “periodic”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/extractFDAWavelets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discrete Wavelet transform features. — extractFDAWavelets","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter features by thresholding filter values. — filterFeatures","title":"Filter features by thresholding filter values. — filterFeatures","text":"First, calls generateFilterValuesData. Features selected via select val.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter features by thresholding filter values. — filterFeatures","text":"","code":"filterFeatures(   task,   method = \"randomForestSRC_importance\",   fval = NULL,   perc = NULL,   abs = NULL,   threshold = NULL,   fun = NULL,   fun.args = NULL,   mandatory.feat = NULL,   select.method = NULL,   base.methods = NULL,   cache = FALSE,   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter features by thresholding filter values. — filterFeatures","text":"task (Task) task. method (character(1)) See listFilterMethods. Default “randomForestSRC_importance”. fval (FilterValues) Result generateFilterValuesData. pass , filter values object used feature filtering. method ... ignored . Default NULL used. perc (numeric(1)) set, select perc*100 top scoring features. perc = 1 means select features.Mutually exclusive argumentsabs, thresholdandfun`. abs (numeric(1)) set, select abs top scoring features. Mutually exclusive arguments perc, threshold fun. threshold (numeric(1)) set, select features whose score exceeds threshold. Mutually exclusive arguments perc, abs fun. fun (function) set, select features via custom thresholding function, must return number top scoring features select. Mutually exclusive arguments perc, abs threshold. fun.args () Arguments passed custom thresholding function. mandatory.feat (character) Mandatory features always included regardless scores select.method multiple methods supplied argument method, specify method used final subsetting. base.methods method ensemble filter, specify base filter methods ensemble method use. cache (character(1) | logical) Whether use caching filter value creation. See details. ... () Passed selected filter method.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter features by thresholding filter values. — filterFeatures","text":"Task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":"caching","dir":"Reference","previous_headings":"","what":"Caching","title":"Filter features by thresholding filter values. — filterFeatures","text":"cache = TRUE, default mlr cache directory used cache filter values. directory operating system dependent can checked getCacheDir(). default cache can cleared deleteCacheDir(). Alternatively, custom directory can passed store cache. Note caching thread safe. work parallel computation many systems, guarantee.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":"simple-and-ensemble-filters","dir":"Reference","previous_headings":"","what":"Simple and ensemble filters","title":"Filter features by thresholding filter values. — filterFeatures","text":"Besides passing (multiple) simple filter methods can also pass ensemble filter method (list). ensemble method use simple methods calculate ranking. See listFilterEnsembleMethods() available ensemble methods.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/filterFeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter features by thresholding filter values. — filterFeatures","text":"","code":"# simple filter filterFeatures(iris.task, method = \"FSelectorRcpp_gain.ratio\", abs = 2) #> Supervised task: iris-example #> Type: classif #> Target: Species #> Observations: 150 #> Features: #>    numerics     factors     ordered functionals  #>           2           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 3 #>     setosa versicolor  virginica  #>         50         50         50  #> Positive class: NA # ensemble filter filterFeatures(iris.task, method = \"E-min\",   base.methods = c(\"FSelectorRcpp_gain.ratio\",     \"FSelectorRcpp_information.gain\"), abs = 2) #> Supervised task: iris-example #> Type: classif #> Target: Species #> Observations: 150 #> Features: #>    numerics     factors     ordered functionals  #>           2           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 3 #>     setosa versicolor  virginica  #>         50         50         50  #> Positive class: NA"},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanPostHocTestBMR.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform a posthoc Friedman-Nemenyi test. — friedmanPostHocTestBMR","title":"Perform a posthoc Friedman-Nemenyi test. — friedmanPostHocTestBMR","text":"Performs PMCMRplus::frdAllPairsNemenyiTest BenchmarkResult selected measure. means pairwise comparisons learners performed. null hypothesis post hoc test pair learners equal. null hypothesis included ad hoc stats::friedman.test can rejected object class pairwise.htest returned. , function returns corresponding friedman.test. Note benchmark results least two learners least two tasks required.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanPostHocTestBMR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform a posthoc Friedman-Nemenyi test. — friedmanPostHocTestBMR","text":"","code":"friedmanPostHocTestBMR(   bmr,   measure = NULL,   p.value = 0.05,   aggregation = \"default\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanPostHocTestBMR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform a posthoc Friedman-Nemenyi test. — friedmanPostHocTestBMR","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. p.value (numeric(1)) p-value tests. Default: 0.05 aggregation (character(1))  “mean” “default”. See getBMRAggrPerformances details “default”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanPostHocTestBMR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform a posthoc Friedman-Nemenyi test. — friedmanPostHocTestBMR","text":"(pairwise.htest): See PMCMRplus::frdAllPairsNemenyiTest details. Additionally two components added list: f.rejnull (logical(1)): Whether according friedman.test rejects Null hypothesis selected p.value crit.difference (list(2)): Minimal difference mean ranks two learners need order significantly different","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanPostHocTestBMR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform a posthoc Friedman-Nemenyi test. — friedmanPostHocTestBMR","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanTestBMR.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform overall Friedman test for a BenchmarkResult. — friedmanTestBMR","title":"Perform overall Friedman test for a BenchmarkResult. — friedmanTestBMR","text":"Performs stats::friedman.test selected measure. null hypothesis apart effect different (Task), location parameter (aggregated performance measure) Learner. Note benchmark results least two learners least two tasks required.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanTestBMR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform overall Friedman test for a BenchmarkResult. — friedmanTestBMR","text":"","code":"friedmanTestBMR(bmr, measure = NULL, aggregation = \"default\")"},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanTestBMR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform overall Friedman test for a BenchmarkResult. — friedmanTestBMR","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. aggregation (character(1))  “mean” “default”. See getBMRAggrPerformances details “default”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanTestBMR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform overall Friedman test for a BenchmarkResult. — friedmanTestBMR","text":"(htest): See stats::friedman.test details.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/friedmanTestBMR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform overall Friedman test for a BenchmarkResult. — friedmanTestBMR","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/fuelsubset.task.html","id":null,"dir":"Reference","previous_headings":"","what":"FuelSubset functional data regression task. — fuelsubset.task","title":"FuelSubset functional data regression task. — fuelsubset.task","text":"Contains task (fuelsubset.task). 2 functional covariates 1 scalar covariate. predict heat value fuel based ultraviolet radiation spectrum infrared ray radiation one scalar column called h2o.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/fuelsubset.task.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"FuelSubset functional data regression task. — fuelsubset.task","text":"features grids scaled way FDboost::FDboost.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/fuelsubset.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"FuelSubset functional data regression task. — fuelsubset.task","text":"See Brockhaus, S., Scheipl, F., Hothorn, T., & Greven, S. (2015). functional linear array model. Statistical Modelling, 15(3), 279–300.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateCalibrationData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate classifier calibration data. — generateCalibrationData","title":"Generate classifier calibration data. — generateCalibrationData","text":"calibrated classifier one predicted probability class closely matches rate class occurs, e.g. data points assigned predicted probability class .8, approximately 80 percent points belong class classifier well calibrated. estimated empirically grouping data points similar predicted probabilities class, plotting rate class within bin predicted probability bins.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateCalibrationData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate classifier calibration data. — generateCalibrationData","text":"","code":"generateCalibrationData(obj, breaks = \"Sturges\", groups = NULL, task.id = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/generateCalibrationData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate classifier calibration data. — generateCalibrationData","text":"obj (list Prediction | list ResampleResult | BenchmarkResult) Single prediction object, list , single resample result, list , benchmark result. case list probably produced different learners want compare, name list names want see plots, probably learner shortnames ids. breaks (character(1) | numeric) character(1), algorithm use generating probability bins. See hist details. numeric, cut points bins. Default “Sturges”. groups (integer(1)) number bins construct. specified, breaks ignored. Default NULL. task.id (character(1)) Selected task BenchmarkResult plots , ignored otherwise. Default first task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateCalibrationData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate classifier calibration data. — generateCalibrationData","text":"CalibrationData. list containing: proportion data.frame columns: Learner Name learner. bin Bins calculated according breaks groups argument. Class Class labels (binary classification positive class). Proportion Proportion observations class Class among observations posterior probabilities class Class within interval given bin. data data.frame columns: Learner Name learner. truth True class label. Class Class labels (binary classification positive class). Probability Predicted posterior probability Class. bin Bin corresponding Probability. task (TaskDesc) Task description.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateCalibrationData.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate classifier calibration data. — generateCalibrationData","text":"Vuk, Miha, Curk, Tomaz. “ROC Curve, Lift Chart, Calibration Plot.” Metodoloski zvezki. Vol. 3. . 1 (2006): 89-108.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/generateCritDifferencesData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for critical-differences plot. — generateCritDifferencesData","title":"Generate data for critical-differences plot. — generateCritDifferencesData","text":"Generates data can used plot critical differences plot. Computes critical differences according either \"Bonferroni-Dunn\" test \"Nemenyi\" test.\"Bonferroni-Dunn\" usually yields higher power compare algorithms , algorithms baseline instead.  Learners drawn y-axis according average rank.  test = \"nemenyi\" bar drawn, connecting groups significantly different learners. test = \"bd\" interval drawn arround algorithm selected baseline. learners within interval signifcantly different baseline.  Calculation: $$CD = q_{\\alpha} \\sqrt{\\left(\\frac{k(k+1)}{6N}\\right)}$$  \\(q_\\alpha\\) based studentized range statistic. See references details.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateCritDifferencesData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for critical-differences plot. — generateCritDifferencesData","text":"","code":"generateCritDifferencesData(   bmr,   measure = NULL,   p.value = 0.05,   baseline = NULL,   test = \"bd\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/generateCritDifferencesData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for critical-differences plot. — generateCritDifferencesData","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. p.value (numeric(1)) P-value critical difference. Default: 0.05 baseline (character(1)): (learner.id)  Select learner.id baseline test = \"bd\" (\"Bonferroni-Dunn\") critical differences diagram. critical difference interval positioned arround learner. Defaults best performing algorithm.  test = \"nemenyi\", baseline needed performs pairwise comparisons. test (character(1))  Test critical differences computed.  “bd” Bonferroni-Dunn Test, comparing classifiers baseline, thus performing comparison one classifier others.  Algorithms connected single line statistically different baseline.  “nemenyi” PMCMRplus::frdAllPairsNemenyiTest comparing classifiers . null hypothesis difference classifiers can rejected classifiers single grey bar connecting .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateCritDifferencesData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for critical-differences plot. — generateCritDifferencesData","text":"(critDifferencesData). List containing: data (data.frame) containing info descriptive part plot friedman.nemenyi.test (list) class pairwise.htest  contains calculated PMCMRplus::frdAllPairsNemenyiTest cd.info (list) containing info critical difference positioning baseline baseline chosen plotting p.value p.value used PMCMRplus::frdAllPairsNemenyiTest computation critical difference","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/generateFeatureImportanceData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate feature importance. — generateFeatureImportanceData","title":"Generate feature importance. — generateFeatureImportanceData","text":"Estimate important individual features groups features contrasting prediction performances. method “permutation.importance” compute change performance permuting values feature (group features) compare predictions made unmcuted data.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateFeatureImportanceData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate feature importance. — generateFeatureImportanceData","text":"","code":"generateFeatureImportanceData(   task,   method = \"permutation.importance\",   learner,   features = getTaskFeatureNames(task),   interaction = FALSE,   measure,   contrast = function(x, y) x - y,   aggregation = mean,   nmc = 50L,   replace = TRUE,   local = FALSE,   show.info = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/generateFeatureImportanceData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate feature importance. — generateFeatureImportanceData","text":"task (Task) task. method (character(1)) method used compute feature importance. method available “permutation.importance”. Default “permutation.importance”. learner (Learner | character(1)) learner. pass string learner created via makeLearner. features (character) features compute importance . default features contained Task. interaction (logical(1)) Whether compute importance features argument jointly. method = \"permutation.importance\" entails permuting values features together contrasting performance performance without features permuted. default FALSE. measure (Measure) Performance measure. Default first measure used benchmark experiment. contrast (function) difference function takes numeric vector returns numeric vector length. default element-wise difference vectors. aggregation (function) function aggregates differences. function must take numeric vector return numeric vector length 1. default mean. nmc (integer(1)) number Monte-Carlo iterations use computing feature importance. nmc == -1 method = \"permutation.importance\" permutations features used. default 50. replace (logical(1)) Whether sample feature values without replacement. default TRUE. local (logical(1)) Whether compute per-observation importance. default FALSE. show.info (logical(1)) Whether progress output (feature name, time elapsed) displayed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateFeatureImportanceData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate feature importance. — generateFeatureImportanceData","text":"(FeatureImportance). named list contains computed feature importance input arguments. Object members: res (data.frame) columns feature combination features (colon separated) importance computed. row coresponds importance feature specified column target. interaction (logical(1)) Whether importance features computed jointly rather individually. measure (Measure) measure used compute performance. contrast (function) function used compare performance predictions. aggregation (function) function used aggregate contrast performance predictions across Monte-Carlo iterations. replace (logical(1)) Whether , method = \"permutation.importance\", feature values sampled replacement. nmc (integer(1)) number Monte-Carlo iterations used compute feature importance. nmc == -1 method = \"permutation.importance\" permutations used. local (logical(1)) Whether observation-specific importance computed features.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateFeatureImportanceData.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate feature importance. — generateFeatureImportanceData","text":"Jerome Friedman; Greedy Function Approximation: Gradient Boosting Machine, Annals Statistics, Vol. 29, . 5 (Oct., 2001), pp. 1189-1232.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/generateFeatureImportanceData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate feature importance. — generateFeatureImportanceData","text":"","code":"lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") fit = train(lrn, iris.task) imp = generateFeatureImportanceData(iris.task, \"permutation.importance\",   lrn, \"Petal.Width\", nmc = 10L, local = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/generateFilterValuesData.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates feature filter values. — generateFilterValuesData","title":"Calculates feature filter values. — generateFilterValuesData","text":"Calculates numerical filter values features. list features, use listFilterMethods.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateFilterValuesData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates feature filter values. — generateFilterValuesData","text":"","code":"generateFilterValuesData(   task,   method = \"randomForestSRC_importance\",   nselect = getTaskNFeats(task),   ...,   more.args = list() )"},{"path":"https://mlr.mlr-org.com/dev/reference/generateFilterValuesData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates feature filter values. — generateFilterValuesData","text":"task (Task) task. method (character | list) Filter method(s). case ensemble filters list notation needs used. See examples information. Default “randomForestSRC_importance”. nselect (integer(1)) Number scores request. Scores getting calculated features per default. ... () Passed selected method. Can use method contains one element. .args (named list) Extra args passed filter methods. List elements named filter method name args passed . general flexible option .... Default empty list.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateFilterValuesData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates feature filter values. — generateFilterValuesData","text":"(FilterValues). list containing: task.desc [TaskDesc) Task description. data (data.frame) columns: name(character) Name feature. type(character) Feature column type. method(numeric) One column method feature importance values.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateFilterValuesData.html","id":"simple-and-ensemble-filters","dir":"Reference","previous_headings":"","what":"Simple and ensemble filters","title":"Calculates feature filter values. — generateFilterValuesData","text":"Besides passing (multiple) simple filter methods can also pass ensemble filter method (list). ensemble method use simple methods calculate ranking. See listFilterEnsembleMethods() available ensemble methods.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/generateFilterValuesData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates feature filter values. — generateFilterValuesData","text":"","code":"# two simple filter methods fval = generateFilterValuesData(iris.task,   method = c(\"FSelectorRcpp_gain.ratio\", \"FSelectorRcpp_information.gain\")) # using ensemble method \"E-mean\" fval = generateFilterValuesData(iris.task,   method = list(\"E-mean\", c(\"FSelectorRcpp_gain.ratio\",     \"FSelectorRcpp_information.gain\")))"},{"path":"https://mlr.mlr-org.com/dev/reference/generateHyperParsEffectData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate hyperparameter effect data. — generateHyperParsEffectData","title":"Generate hyperparameter effect data. — generateHyperParsEffectData","text":"Generate cleaned hyperparameter effect data tuning result nested cross-validation tuning result. object returned can used custom visualization passed downstream box mlr method, plotHyperParsEffect.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateHyperParsEffectData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate hyperparameter effect data. — generateHyperParsEffectData","text":"","code":"generateHyperParsEffectData(   tune.result,   include.diagnostics = FALSE,   trafo = FALSE,   partial.dep = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/generateHyperParsEffectData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate hyperparameter effect data. — generateHyperParsEffectData","text":"tune.result (TuneResult | ResampleResult) Result tuneParams (resample used nested cross-validation). tuning result (results output nested cross-validation), also containing optimizer results. nested CV output passed, element list considered separate run, data run included dataframe within returned HyperParsEffectData. include.diagnostics (logical(1)) diagnostic info (eol error msg) included? Default FALSE. trafo (logical(1)) units hyperparameter path converted transformed scale? useful trafo used create path. Default FALSE. partial.dep (logical(1)) partial dependence requested based converting reg task? sets flag know use partial dependence downstream. likely set TRUE 2 hyperparameters tuned simultaneously. Partial dependence always requested 2 hyperparameters tuned simultaneously. Setting TRUE cause plotHyperParsEffect automatically plot partial dependence called downstream. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateHyperParsEffectData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate hyperparameter effect data. — generateHyperParsEffectData","text":"(HyperParsEffectData) Object containing hyperparameter effects dataframe, tuning performance measures used, hyperparameters used, flag including diagnostic info, flag whether nested cv used, flag whether partial dependence generated, optimization algorithm used.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateHyperParsEffectData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate hyperparameter effect data. — generateHyperParsEffectData","text":"","code":"if (FALSE) { # 3-fold cross validation ps = makeParamSet(makeDiscreteParam(\"C\", values = 2^(-4:4))) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"CV\", iters = 3L) res = tuneParams(\"classif.ksvm\", task = pid.task, resampling = rdesc,   par.set = ps, control = ctrl) data = generateHyperParsEffectData(res) plt = plotHyperParsEffect(data, x = \"C\", y = \"mmce.test.mean\") plt + ylab(\"Misclassification Error\")  # nested cross validation ps = makeParamSet(makeDiscreteParam(\"C\", values = 2^(-4:4))) ctrl = makeTuneControlGrid() rdesc = makeResampleDesc(\"CV\", iters = 3L) lrn = makeTuneWrapper(\"classif.ksvm\", control = ctrl,   resampling = rdesc, par.set = ps) res = resample(lrn, task = pid.task, resampling = cv2,   extract = getTuneResult) data = generateHyperParsEffectData(res) plotHyperParsEffect(data, x = \"C\", y = \"mmce.test.mean\", plot.type = \"line\") }"},{"path":"https://mlr.mlr-org.com/dev/reference/generateLearningCurveData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a learning curve. — generateLearningCurveData","title":"Generates a learning curve. — generateLearningCurveData","text":"Observe performance changes increasing number observations.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateLearningCurveData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a learning curve. — generateLearningCurveData","text":"","code":"generateLearningCurveData(   learners,   task,   resampling = NULL,   percs = seq(0.1, 1, by = 0.1),   measures,   stratify = FALSE,   show.info = getMlrOption(\"show.info\") )"},{"path":"https://mlr.mlr-org.com/dev/reference/generateLearningCurveData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a learning curve. — generateLearningCurveData","text":"learners [(list ) Learner) Learning algorithms compared. task (Task) task. resampling (ResampleDesc | ResampleInstance) Resampling strategy evaluate performance measure. strategy given default \"Holdout\" performed. percs (numeric) Vector percentages drawn training split. values represent x-axis. Internally makeDownsampleWrapper used combination benchmark. Thus percentage different set observations drawn resulting noisy performance measures quality sample can differ. measures [(list ) Measure) Performance measures generate learning curves , representing y-axis. stratify (logical(1)) classification: downsampled data stratified according target classes? show.info (logical(1)) Print verbose output console? Default set via configureMlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateLearningCurveData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a learning curve. — generateLearningCurveData","text":"(LearningCurveData). list containing: Task List Measure) Performance measures data (data.frame) columns: learner Names learners. percentage Percentages drawn training split. One column Measure passed generateLearningCurveData.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/generateLearningCurveData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generates a learning curve. — generateLearningCurveData","text":"","code":"r = generateLearningCurveData(list(\"classif.rpart\", \"classif.knn\"),   task = sonar.task, percs = seq(0.2, 1, by = 0.2),   measures = list(tp, fp, tn, fn),   resampling = makeResampleDesc(method = \"Subsample\", iters = 5),   show.info = FALSE) plotLearningCurve(r)"},{"path":"https://mlr.mlr-org.com/dev/reference/generatePartialDependenceData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate partial dependence. — generatePartialDependenceData","title":"Generate partial dependence. — generatePartialDependenceData","text":"Estimate learned prediction function affected one features. learned function f(x) x partitioned x_s x_c, partial dependence f x_s can summarized averaging x_c setting x_s range values interest, estimating E_(x_c)(f(x_s, x_c)). conditional expectation f observation estimated similarly. Additionally, partial derivatives marginalized function w.r.t. features can computed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generatePartialDependenceData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate partial dependence. — generatePartialDependenceData","text":"","code":"generatePartialDependenceData(   obj,   input,   features = NULL,   interaction = FALSE,   derivative = FALSE,   individual = FALSE,   fun = mean,   bounds = c(qnorm(0.025), qnorm(0.975)),   uniform = TRUE,   n = c(10, NA),   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/generatePartialDependenceData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate partial dependence. — generatePartialDependenceData","text":"obj (WrappedModel) Result train. input (data.frame | Task) Input data. features character vector feature names contained training data. specified features input used. interaction (logical(1)) Whether features interacted . TRUE Cartesian product prediction grid feature taken, partial dependence unique combination values features estimated. Note length features greater two, plotPartialDependence used. FALSE feature considered separately. case features can much longer two. Default FALSE. derivative (logical(1)) Whether partial derivative learned function respect features estimated. TRUE interaction must FALSE. partial derivative individual observations may estimated. Note computation time increases learned prediction function evaluated gridsize points * number points required estimate partial derivative. Additional arguments may passed numDeriv::grad (regression survival tasks) numDeriv::jacobian (classification tasks). Note functions smooth may result estimated derivatives 0 (points function change within +/- epsilon) estimates trending towards +/- infinity (discontinuities). Default FALSE. individual (logical(1)) Whether plot individual conditional expectation curves rather aggregated curve, .e., rather aggregating (using fun) partial dependences features, plot partial dependences observations data across values features. algorithm developed Goldstein, Kapelner, Bleich, Pitkin (2015). Default FALSE. fun function function operates output predictions made input data. regression means numeric vector, , e.g., multiclass classification problem, migh instead probabilities returned numeric matrix. argument can return vectors arbitrary length, however, length greater one, must named, e.g., fun = mean fun = function(x) c(\"mean\" = mean(x), \"variance\" = var(x)). default mean, unless obj classification predict.type = \"response\" case default proportion observations predicted class. bounds (numeric(2)) value (lower, upper) estimated standard error multiplied estimate bound confidence region partial dependence. Ignored predict.type != \"se\" learner. Default 2.5 97.5 quantiles (-1.96, 1.96) Gaussian distribution. uniform (logical(1)) Whether prediction grid features uniform grid size n[1] sampled replacement input. Default TRUE. n (integer21) first element n gives size prediction grid created feature. second element n gives size sample drawn without replacement input data. Setting n[2] less number rows input decrease computation time. default n[1] 10, default n[2] number rows input. ... additional arguments passed mmpf::marginalPrediction.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generatePartialDependenceData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate partial dependence. — generatePartialDependenceData","text":"PartialDependenceData. named list, contains partial dependence, input data, target, features, task description, arguments controlling type partial dependences made. Object members: data data.frame columns prediction: one column regression survival analysis, column class predicted probability classification well column element features. individual = TRUE additional column idx gives index data prediction corresponds . task.desc TaskDesc Task description. target Target feature regression, target feature levels classification, survival event indicator survival. features character Features argument input. interaction (logical(1)) Whether features interacted (.e. conditioning). derivative (logical(1)) Whether partial derivative estimated. individual (logical(1)) Whether partial dependences aggregated individual curves retained.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generatePartialDependenceData.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate partial dependence. — generatePartialDependenceData","text":"Goldstein, Alex, Adam Kapelner, Justin Bleich, Emil Pitkin. “Peeking inside black box: Visualizing statistical learning plots individual conditional expectation.” Journal Computational Graphical Statistics. Vol. 24, . 1 (2015): 44-65. Friedman, Jerome. “Greedy Function Approximation: Gradient Boosting Machine.” Annals Statistics. Vol. 29. . 5 (2001): 1189-1232.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/generatePartialDependenceData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate partial dependence. — generatePartialDependenceData","text":"","code":"lrn = makeLearner(\"regr.svm\") fit = train(lrn, bh.task) pd = generatePartialDependenceData(fit, bh.task, \"lstat\") #> Loading required package: mmpf plotPartialDependence(pd, data = getTaskData(bh.task))   lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") fit = train(lrn, iris.task) pd = generatePartialDependenceData(fit, iris.task, \"Petal.Width\") plotPartialDependence(pd, data = getTaskData(iris.task))"},{"path":"https://mlr.mlr-org.com/dev/reference/generateThreshVsPerfData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate threshold vs. performance(s) for 2-class classification. — generateThreshVsPerfData","title":"Generate threshold vs. performance(s) for 2-class classification. — generateThreshVsPerfData","text":"Generates data threshold vs. performance(s) 2-class classification can used plotting.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateThreshVsPerfData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate threshold vs. performance(s) for 2-class classification. — generateThreshVsPerfData","text":"","code":"generateThreshVsPerfData(   obj,   measures,   gridsize = 100L,   aggregate = TRUE,   task.id = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/generateThreshVsPerfData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate threshold vs. performance(s) for 2-class classification. — generateThreshVsPerfData","text":"obj (list Prediction | list ResampleResult | BenchmarkResult) Single prediction object, list , single resample result, list , benchmark result. case list probably produced different learners want compare, name list names want see plots, probably learner shortnames ids. measures (Measure | list Measure) Performance measure(s) evaluate. Default default measure task, see getDefaultMeasure. gridsize (integer(1)) Grid resolution x-axis (threshold). Default 100. aggregate (logical(1)) Whether aggregate ResamplePredictions plot performance iteration separately. Default TRUE. task.id (character(1)) Selected task BenchmarkResult plots , ignored otherwise. Default first task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/generateThreshVsPerfData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate threshold vs. performance(s) for 2-class classification. — generateThreshVsPerfData","text":"(ThreshVsPerfData). named list containing measured performance across threshold grid, measures, whether performance estimates aggregated (applicable (list ) ResampleResults).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRAggrPerformances.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the aggregated performance values from a benchmark result. — getBMRAggrPerformances","title":"Extract the aggregated performance values from a benchmark result. — getBMRAggrPerformances","text":"Either list lists “aggr” numeric vectors, returned resample, objects rbind-ed extra columns “task.id” “learner.id”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRAggrPerformances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the aggregated performance values from a benchmark result. — getBMRAggrPerformances","text":"","code":"getBMRAggrPerformances(   bmr,   task.ids = NULL,   learner.ids = NULL,   as.df = FALSE,   drop = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRAggrPerformances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the aggregated performance values from a benchmark result. — getBMRAggrPerformances","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . .df (character(1)) Return one data.frame result - list lists objects?. Default FALSE. drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRAggrPerformances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the aggregated performance values from a benchmark result. — getBMRAggrPerformances","text":"(list | data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFeatSelResults.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the feature selection results from a benchmark result. — getBMRFeatSelResults","title":"Extract the feature selection results from a benchmark result. — getBMRFeatSelResults","text":"Returns nested list FeatSelResults. first level nesting data set, second learner, third benchmark resampling iterations. .df TRUE, data frame “task.id”, “learner.id”, resample iteration selected features returned. Note one feature selected data frame requested, multiple rows dataset-learner-iteration; one selected feature.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFeatSelResults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the feature selection results from a benchmark result. — getBMRFeatSelResults","text":"","code":"getBMRFeatSelResults(   bmr,   task.ids = NULL,   learner.ids = NULL,   as.df = FALSE,   drop = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFeatSelResults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the feature selection results from a benchmark result. — getBMRFeatSelResults","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . .df (character(1)) Return one data.frame result - list lists objects?. Default FALSE. drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFeatSelResults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the feature selection results from a benchmark result. — getBMRFeatSelResults","text":"(list | data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFilteredFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the feature selection results from a benchmark result. — getBMRFilteredFeatures","title":"Extract the feature selection results from a benchmark result. — getBMRFilteredFeatures","text":"Returns nested list characters first level nesting data set, second learner, third benchmark resampling iterations. list lowest level list selected features. .df TRUE, data frame “task.id”, “learner.id”, resample iteration selected features returned. Note one feature selected data frame requested, multiple rows dataset-learner-iteration; one selected feature.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFilteredFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the feature selection results from a benchmark result. — getBMRFilteredFeatures","text":"","code":"getBMRFilteredFeatures(   bmr,   task.ids = NULL,   learner.ids = NULL,   as.df = FALSE,   drop = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFilteredFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the feature selection results from a benchmark result. — getBMRFilteredFeatures","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . .df (character(1)) Return one data.frame result - list lists objects?. Default FALSE. drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRFilteredFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the feature selection results from a benchmark result. — getBMRFilteredFeatures","text":"(list | data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerIds.html","id":null,"dir":"Reference","previous_headings":"","what":"Return learner ids used in benchmark. — getBMRLearnerIds","title":"Return learner ids used in benchmark. — getBMRLearnerIds","text":"Gets IDs learners used benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerIds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return learner ids used in benchmark. — getBMRLearnerIds","text":"","code":"getBMRLearnerIds(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerIds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return learner ids used in benchmark. — getBMRLearnerIds","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerIds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return learner ids used in benchmark. — getBMRLearnerIds","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerShortNames.html","id":null,"dir":"Reference","previous_headings":"","what":"Return learner short.names used in benchmark. — getBMRLearnerShortNames","title":"Return learner short.names used in benchmark. — getBMRLearnerShortNames","text":"Gets learner short.names learners used benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerShortNames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return learner short.names used in benchmark. — getBMRLearnerShortNames","text":"","code":"getBMRLearnerShortNames(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerShortNames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return learner short.names used in benchmark. — getBMRLearnerShortNames","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearnerShortNames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return learner short.names used in benchmark. — getBMRLearnerShortNames","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearners.html","id":null,"dir":"Reference","previous_headings":"","what":"Return learners used in benchmark. — getBMRLearners","title":"Return learners used in benchmark. — getBMRLearners","text":"Gets learners used benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return learners used in benchmark. — getBMRLearners","text":"","code":"getBMRLearners(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return learners used in benchmark. — getBMRLearners","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRLearners.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return learners used in benchmark. — getBMRLearners","text":"(list).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasureIds.html","id":null,"dir":"Reference","previous_headings":"","what":"Return measures IDs used in benchmark. — getBMRMeasureIds","title":"Return measures IDs used in benchmark. — getBMRMeasureIds","text":"Gets IDs measures used benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasureIds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return measures IDs used in benchmark. — getBMRMeasureIds","text":"","code":"getBMRMeasureIds(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasureIds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return measures IDs used in benchmark. — getBMRMeasureIds","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasureIds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return measures IDs used in benchmark. — getBMRMeasureIds","text":"(list). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasures.html","id":null,"dir":"Reference","previous_headings":"","what":"Return measures used in benchmark. — getBMRMeasures","title":"Return measures used in benchmark. — getBMRMeasures","text":"Gets measures used benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return measures used in benchmark. — getBMRMeasures","text":"","code":"getBMRMeasures(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return measures used in benchmark. — getBMRMeasures","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRMeasures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return measures used in benchmark. — getBMRMeasures","text":"(list). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRModels.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract all models from benchmark result. — getBMRModels","title":"Extract all models from benchmark result. — getBMRModels","text":"list lists containing WrappedModels trained benchmark experiment. models FALSE call benchmark, function return NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRModels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract all models from benchmark result. — getBMRModels","text":"","code":"getBMRModels(bmr, task.ids = NULL, learner.ids = NULL, drop = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRModels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract all models from benchmark result. — getBMRModels","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRModels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract all models from benchmark result. — getBMRModels","text":"(list).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPerformances.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the test performance values from a benchmark result. — getBMRPerformances","title":"Extract the test performance values from a benchmark result. — getBMRPerformances","text":"Either list lists “measure.test” data.frames, returned resample, objects rbind-ed extra columns “task.id” “learner.id”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPerformances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the test performance values from a benchmark result. — getBMRPerformances","text":"","code":"getBMRPerformances(   bmr,   task.ids = NULL,   learner.ids = NULL,   as.df = FALSE,   drop = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPerformances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the test performance values from a benchmark result. — getBMRPerformances","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . .df (character(1)) Return one data.frame result - list lists objects?. Default FALSE. drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPerformances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the test performance values from a benchmark result. — getBMRPerformances","text":"(list | data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPredictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the predictions from a benchmark result. — getBMRPredictions","title":"Extract the predictions from a benchmark result. — getBMRPredictions","text":"Either list lists ResamplePrediction objects, returned resample, objects rbind-ed extra columns “task.id” “learner.id”. predict.type “prob”, probabilities class returned addition response. keep.pred FALSE call benchmark, function return NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPredictions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the predictions from a benchmark result. — getBMRPredictions","text":"","code":"getBMRPredictions(   bmr,   task.ids = NULL,   learner.ids = NULL,   as.df = FALSE,   drop = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPredictions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the predictions from a benchmark result. — getBMRPredictions","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . .df (character(1)) Return one data.frame result - list lists objects?. Default FALSE. drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRPredictions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the predictions from a benchmark result. — getBMRPredictions","text":"(list | data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescriptions.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract all task descriptions from benchmark result (DEPRECATED). — getBMRTaskDescriptions","title":"Extract all task descriptions from benchmark result (DEPRECATED). — getBMRTaskDescriptions","text":"list containing TaskDescs task contained benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescriptions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract all task descriptions from benchmark result (DEPRECATED). — getBMRTaskDescriptions","text":"","code":"getBMRTaskDescriptions(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescriptions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract all task descriptions from benchmark result (DEPRECATED). — getBMRTaskDescriptions","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescriptions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract all task descriptions from benchmark result (DEPRECATED). — getBMRTaskDescriptions","text":"(list).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescs.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract all task descriptions from benchmark result. — getBMRTaskDescs","title":"Extract all task descriptions from benchmark result. — getBMRTaskDescs","text":"list containing TaskDescs task contained benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract all task descriptions from benchmark result. — getBMRTaskDescs","text":"","code":"getBMRTaskDescs(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract all task descriptions from benchmark result. — getBMRTaskDescs","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskDescs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract all task descriptions from benchmark result. — getBMRTaskDescs","text":"(list).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskIds.html","id":null,"dir":"Reference","previous_headings":"","what":"Return task ids used in benchmark. — getBMRTaskIds","title":"Return task ids used in benchmark. — getBMRTaskIds","text":"Gets task IDs used benchmark experiment.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskIds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return task ids used in benchmark. — getBMRTaskIds","text":"","code":"getBMRTaskIds(bmr)"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskIds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return task ids used in benchmark. — getBMRTaskIds","text":"bmr (BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTaskIds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return task ids used in benchmark. — getBMRTaskIds","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTuneResults.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the tuning results from a benchmark result. — getBMRTuneResults","title":"Extract the tuning results from a benchmark result. — getBMRTuneResults","text":"Returns nested list TuneResults. first level nesting data set, second learner, third benchmark resampling iterations. .df TRUE, data frame “task.id”, “learner.id”, resample iteration, parameter values performances returned.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTuneResults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the tuning results from a benchmark result. — getBMRTuneResults","text":"","code":"getBMRTuneResults(   bmr,   task.ids = NULL,   learner.ids = NULL,   as.df = FALSE,   drop = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTuneResults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the tuning results from a benchmark result. — getBMRTuneResults","text":"bmr (BenchmarkResult) Benchmark result. task.ids (character(1)) Restrict result certain tasks. Default . learner.ids (character(1)) Restrict result certain learners. Default . .df (character(1)) Return one data.frame result - list lists objects?. Default FALSE. drop (logical(1)) drop FALSE (default), nested list following structure returned:res[task.ids][learner.ids]. drop set TRUE checked list structure can simplified. one learner passed, list entries task returned. one task passed, entries named corresponding learner. experiment one task learner, whole list structure removed. Note name task/learner dropped return object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getBMRTuneResults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the tuning results from a benchmark result. — getBMRTuneResults","text":"(list | data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getCaretParamSet.html","id":null,"dir":"Reference","previous_headings":"","what":"Get tuning parameters from a learner of the caret R-package. — getCaretParamSet","title":"Get tuning parameters from a learner of the caret R-package. — getCaretParamSet","text":"Constructs grid tuning parameters learner caret R-package. values converted list non-tunable parameters (par.vals) tunable ParamHelpers::ParamSet (par.set), can used tuneParams tuning learner. Numerical parameters either specified lower upper bounds discretized specific values.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getCaretParamSet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get tuning parameters from a learner of the caret R-package. — getCaretParamSet","text":"","code":"getCaretParamSet(learner, length = 3L, task, discretize = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getCaretParamSet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get tuning parameters from a learner of the caret R-package. — getCaretParamSet","text":"learner (character(1)) name learner caret (cf. https://topepo.github.io/caret/available-models.html). Note names caret often differ ones mlr. length (integer(1)) length / precision parameter used caret generating grid tuning parameters. caret generates either many values per tuning parameter / dimension defined length single value (case non-tunable par.vals). task (Task) Learning task, might requested creating tuning grid. discretize (logical(1)) numerical parameters discretized? Alternatively, defined lower upper bounds. default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getCaretParamSet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get tuning parameters from a learner of the caret R-package. — getCaretParamSet","text":"(list(2)). list parameters: par.vals contains list constant tuning parameters par.set ParamHelpers::ParamSet, containing configurable tuning parameters","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getCaretParamSet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get tuning parameters from a learner of the caret R-package. — getCaretParamSet","text":"","code":"if (requireNamespace(\"caret\") && requireNamespace(\"mlbench\")) {   library(caret)   classifTask = makeClassifTask(data = iris, target = \"Species\")    # (1) classification (random forest) with discretized parameters   getCaretParamSet(\"rf\", length = 9L, task = classifTask, discretize = TRUE)    # (2) regression (gradient boosting machine) without discretized parameters   library(mlbench)   data(BostonHousing)   regrTask = makeRegrTask(data = BostonHousing, target = \"medv\")   getCaretParamSet(\"gbm\", length = 9L, task = regrTask, discretize = FALSE) } #> Loading required namespace: caret #> Loading required package: ggplot2 #> Loading required package: lattice #>  #> Attaching package: ‘caret’ #> The following object is masked from ‘package:mlr’: #>  #>     train #> note: only 3 unique complexity parameters in default grid. Truncating the grid to 3 . #>  #> $par.vals #> $par.vals$shrinkage #> [1] 0.1 #>  #> $par.vals$n.minobsinnode #> [1] 10 #>  #>  #> $par.set #>                      Type len Def    Constr Req Tunable Trafo #> interaction.depth integer   -   -    1 to 9   -    TRUE     - #> n.trees           numeric   -   - 50 to 450   -    TRUE     - #>"},{"path":"https://mlr.mlr-org.com/dev/reference/getClassWeightParam.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the class weight parameter of a learner. — getClassWeightParam","title":"Get the class weight parameter of a learner. — getClassWeightParam","text":"Gets class weight parameter learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getClassWeightParam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the class weight parameter of a learner. — getClassWeightParam","text":"","code":"getClassWeightParam(learner, lrn.id = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/getClassWeightParam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the class weight parameter of a learner. — getClassWeightParam","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. lrn.id (character) used BaseEnsembles. possible multiple learners base ensemble class weight param. Specify learner class weight extracted.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getClassWeightParam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the class weight parameter of a learner. — getClassWeightParam","text":"numericLearnerParam: numeric parameter object, containing class weight parameter given learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getConfMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Confusion matrix. — getConfMatrix","title":"Confusion matrix. — getConfMatrix","text":"getConfMatrix deprecated. Please use calculateConfusionMatrix. Calculates confusion matrix (possibly resampled) prediction. Rows indicate true classes, columns predicted classes. marginal elements count number classification errors respective row column, .e., number errors condition corresponding true (rows) predicted (columns) class. last element margin diagonal displays total amount errors. Note resampling aggregation currently performed. predictions test sets joined vector yhat, labels joined vector y. yhat simply tabulated vs y, computed single test set. probably mainly makes sense cross-validation used resampling.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getConfMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confusion matrix. — getConfMatrix","text":"","code":"getConfMatrix(pred, relative = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getConfMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confusion matrix. — getConfMatrix","text":"pred (Prediction) Prediction object. relative (logical(1)) TRUE rows normalized show relative frequencies. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getConfMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confusion matrix. — getConfMatrix","text":"(matrix). confusion matrix.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getDefaultMeasure.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default measure. — getDefaultMeasure","title":"Get default measure. — getDefaultMeasure","text":"Get default measure task type, task, task description learner. Currently : classif: mmce regr: mse cluster: db surv: cindex costsen: mcp multilabel: multilabel.hamloss","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getDefaultMeasure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default measure. — getDefaultMeasure","text":"","code":"getDefaultMeasure(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getDefaultMeasure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default measure. — getDefaultMeasure","text":"x ([character(1)` | Task | TaskDesc | Learner) Task type, task, task description, learner name, learner, type learner (e.g. \"classif\").","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getDefaultMeasure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default measure. — getDefaultMeasure","text":"(Measure).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelDump.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the error dump of FailureModel. — getFailureModelDump","title":"Return the error dump of FailureModel. — getFailureModelDump","text":"Returns error dump can used debugger() evaluate errors. configureMlr configuration .error.dump FALSE, returns NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelDump.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the error dump of FailureModel. — getFailureModelDump","text":"","code":"getFailureModelDump(model)"},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelDump.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the error dump of FailureModel. — getFailureModelDump","text":"model (WrappedModel) model.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelDump.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the error dump of FailureModel. — getFailureModelDump","text":"(last.dump).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelMsg.html","id":null,"dir":"Reference","previous_headings":"","what":"Return error message of FailureModel. — getFailureModelMsg","title":"Return error message of FailureModel. — getFailureModelMsg","text":"model created one sets corresponding option configureMlr. failure occurred, NA returned. complex wrappers getter returns first error message encountered model failed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelMsg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return error message of FailureModel. — getFailureModelMsg","text":"","code":"getFailureModelMsg(model)"},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelMsg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return error message of FailureModel. — getFailureModelMsg","text":"model (WrappedModel) model.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFailureModelMsg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return error message of FailureModel. — getFailureModelMsg","text":"(character(1)).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatSelResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the selected feature set and optimization path after training. — getFeatSelResult","title":"Returns the selected feature set and optimization path after training. — getFeatSelResult","text":"Returns selected feature set optimization path training.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatSelResult.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the selected feature set and optimization path after training. — getFeatSelResult","text":"","code":"getFeatSelResult(object)"},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatSelResult.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the selected feature set and optimization path after training. — getFeatSelResult","text":"object (WrappedModel) Trained Model created makeFeatSelWrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatSelResult.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the selected feature set and optimization path after training. — getFeatSelResult","text":"(FeatSelResult).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates feature importance values for trained models. — getFeatureImportance","title":"Calculates feature importance values for trained models. — getFeatureImportance","text":"learners possible calculate feature importance measure. getFeatureImportance extracts values trained models. See list supported learners.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates feature importance values for trained models. — getFeatureImportance","text":"","code":"getFeatureImportance(object, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates feature importance values for trained models. — getFeatureImportance","text":"object (WrappedModel) Wrapped model, result train(). ... () Additional parameters, passed underlying importance value generating function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates feature importance values for trained models. — getFeatureImportance","text":"(FeatureImportance) object containing data.frame variable importances information.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates feature importance values for trained models. — getFeatureImportance","text":"boosting  Measure accounts gain Gini index given feature tree weight tree. cforest  Permutation principle 'mean decrease accuracy' principle randomForest. auc=TRUE (binary classification), area curve used measure.  algorithm used survival learner 'extremely slow experimental; use risk'. See party::varimp() details parameters. gbm  Estimation relative influence feature. See gbm::relative.influence() details parameters. h2o  Relative feature importances returned h2o::h2o.varimp(). randomForest  type = 2 (default) 'MeanDecreaseGini' measured, based Gini impurity index used calculation nodes. Alternatively, can set type 1, measure mean decrease accuracy calculated OOB data. Note, case learner's parameter importance needs set able compute feature importance values. See randomForest::importance() details. RRF  identical randomForest. randomForestSRC  method can calculate feature importance various measures. default Breiman-Cutler permutation method used. See randomForestSRC::vimp() details. ranger  Supports measures mentioned randomForest learner. Note, need specifically set learners parameter importance, able compute feature importance measures. See ranger::importance() ranger::ranger() details. rpart  Sum decrease impurity surrogate variables node xgboost  value implies relative contribution corresponding feature model calculated taking feature's contribution tree model. exact computation importance xgboost undocumented.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportanceLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates feature importance values for a given learner. — getFeatureImportanceLearner.regr.randomForestSRC","title":"Calculates feature importance values for a given learner. — getFeatureImportanceLearner.regr.randomForestSRC","text":"function mostly internal usage. calculate feature importance use getFeatureImportance. return value named numeric vector. need one value feature dataset. getFeatureImportance missing features get importance zero vector contains NA also replaced zero.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportanceLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates feature importance values for a given learner. — getFeatureImportanceLearner.regr.randomForestSRC","text":"","code":"# S3 method for regr.randomForestSRC getFeatureImportanceLearner(.learner, .model, ...)  getFeatureImportanceLearner(.learner, .model, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportanceLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates feature importance values for a given learner. — getFeatureImportanceLearner.regr.randomForestSRC","text":".learner (Learner | character(1)) learner. .model (WrappedModel) model. ... () Additional parameters, passed underlying importance value generating function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFeatureImportanceLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates feature importance values for a given learner. — getFeatureImportanceLearner.regr.randomForestSRC","text":"(numeric) named vector variable importance.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFilteredFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the filtered features. — getFilteredFeatures","title":"Returns the filtered features. — getFilteredFeatures","text":"Returns filtered features.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFilteredFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the filtered features. — getFilteredFeatures","text":"","code":"getFilteredFeatures(model)"},{"path":"https://mlr.mlr-org.com/dev/reference/getFilteredFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the filtered features. — getFilteredFeatures","text":"model (WrappedModel) Trained Model created makeFilterWrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFilteredFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the filtered features. — getFilteredFeatures","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getFunctionalFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Get only functional features from a task or a data.frame. — getFunctionalFeatures","title":"Get only functional features from a task or a data.frame. — getFunctionalFeatures","text":"parameters “subset”, “features”, “recode.target” ignored data.frame method.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFunctionalFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get only functional features from a task or a data.frame. — getFunctionalFeatures","text":"","code":"getFunctionalFeatures(object, subset = NULL, features, recode.target = \"no\")  # S3 method for Task getFunctionalFeatures(object, subset = NULL, features, recode.target = \"no\")  # S3 method for data.frame getFunctionalFeatures(object, subset = NULL, features, recode.target = \"no\")"},{"path":"https://mlr.mlr-org.com/dev/reference/getFunctionalFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get only functional features from a task or a data.frame. — getFunctionalFeatures","text":"object (Task/data.frame) Object check . subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used. features (character | integer | logical) Vector selected inputs. can either pass character vector feature names, vector indices, logical vector. case index vector element denotes position feature name returned getTaskFeatureNames. Note target feature always included resulting task, pass . Default use features. recode.target (character(1)) target classes recoded? Supported binary multilabel classification survival. Possible values binary classification “01”, “-1+1” “drop.levels”. two latter cases target vector converted numeric vector. positive class coded “+1” negative class either “0” “-1”. “drop.levels” remove empty factor levels target column. multilabel case logical targets can converted factors “multilabel.factor”. survival, may choose recode survival times “left”, “right” “interval2” censored times using “lcens”, “rcens” “icens”, respectively. See survival::Surv format specification. Default binary classification survival “” (nothing).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getFunctionalFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get only functional features from a task or a data.frame. — getFunctionalFeatures","text":"Returns data.frame containing functional features.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getHomogeneousEnsembleModels.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated, use getLearnerModel instead. — getHomogeneousEnsembleModels","title":"Deprecated, use getLearnerModel instead. — getHomogeneousEnsembleModels","text":"Deprecated, use getLearnerModel instead.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getHomogeneousEnsembleModels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated, use getLearnerModel instead. — getHomogeneousEnsembleModels","text":"","code":"getHomogeneousEnsembleModels(model, learner.models = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getHomogeneousEnsembleModels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated, use getLearnerModel instead. — getHomogeneousEnsembleModels","text":"model Deprecated. learner.models Deprecated.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getHyperPars.html","id":null,"dir":"Reference","previous_headings":"","what":"Get current parameter settings for a learner. — getHyperPars","title":"Get current parameter settings for a learner. — getHyperPars","text":"Retrieves current hyperparameter settings learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getHyperPars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get current parameter settings for a learner. — getHyperPars","text":"","code":"getHyperPars(learner, for.fun = c(\"train\", \"predict\", \"both\"))"},{"path":"https://mlr.mlr-org.com/dev/reference/getHyperPars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get current parameter settings for a learner. — getHyperPars","text":"learner (Learner) learner. .fun (character(1)) Restrict returned settings hyperparameters corresponding used (see ParamHelpers::LearnerParam). Must subset : “train”, “predict” “”. Default c(\"train\", \"predict\", \"\").","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getHyperPars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get current parameter settings for a learner. — getHyperPars","text":"(list). named list values.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getHyperPars.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get current parameter settings for a learner. — getHyperPars","text":"function shows hyperparameters differ learner default (mlr changed default) user set hyperparameters manually learner creation. want overview available hyperparameters use getParamSet().","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getHyperPars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get current parameter settings for a learner. — getHyperPars","text":"","code":"getHyperPars(makeLearner(\"classif.ranger\")) #> $num.threads #> [1] 1 #>  #> $verbose #> [1] FALSE #>  #> $respect.unordered.factors #> [1] \"order\" #>   ## set learner hyperparameter `mtry` manually getHyperPars(makeLearner(\"classif.ranger\", mtry = 100)) #> $num.threads #> [1] 1 #>  #> $verbose #> [1] FALSE #>  #> $respect.unordered.factors #> [1] \"order\" #>  #> $mtry #> [1] 100 #>"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerId.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the ID of the learner. — getLearnerId","title":"Get the ID of the learner. — getLearnerId","text":"Get ID learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerId.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the ID of the learner. — getLearnerId","text":"","code":"getLearnerId(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerId.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the ID of the learner. — getLearnerId","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerId.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the ID of the learner. — getLearnerId","text":"(character(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Get underlying R model of learner integrated into mlr. — getLearnerModel","title":"Get underlying R model of learner integrated into mlr. — getLearnerModel","text":"Get underlying R model learner integrated mlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get underlying R model of learner integrated into mlr. — getLearnerModel","text":"","code":"getLearnerModel(model, more.unwrap = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get underlying R model of learner integrated into mlr. — getLearnerModel","text":"model (WrappedModel) model, returned e.g., train. .unwrap (logical(1)) learners basic learners R, implemented mlr meta-techniques. Examples everything inherits HomogeneousEnsemble. cases, learner.model often list mlr WrappedModels. option allows strip basic R models. option simply ignored basic learner models. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get underlying R model of learner integrated into mlr. — getLearnerModel","text":"(). fitted model, depending learner / wrapped package. E.g., model class rpart::rpart learner “classif.rpart”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerNote.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the note for the learner. — getLearnerNote","title":"Get the note for the learner. — getLearnerNote","text":"Get note learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerNote.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the note for the learner. — getLearnerNote","text":"","code":"getLearnerNote(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerNote.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the note for the learner. — getLearnerNote","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerNote.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the note for the learner. — getLearnerNote","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPackages.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the required R packages of the learner. — getLearnerPackages","title":"Get the required R packages of the learner. — getLearnerPackages","text":"Get R packages learner requires.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPackages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the required R packages of the learner. — getLearnerPackages","text":"","code":"getLearnerPackages(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPackages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the required R packages of the learner. — getLearnerPackages","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPackages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the required R packages of the learner. — getLearnerPackages","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParVals.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the parameter values of the learner. — getLearnerParVals","title":"Get the parameter values of the learner. — getLearnerParVals","text":"Alias getHyperPars.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParVals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the parameter values of the learner. — getLearnerParVals","text":"","code":"getLearnerParVals(learner, for.fun = c(\"train\", \"predict\", \"both\"))"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParVals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the parameter values of the learner. — getLearnerParVals","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. .fun (character(1)) Restrict returned settings hyperparameters corresponding used (see ParamHelpers::LearnerParam). Must subset : “train”, “predict” “”. Default c(\"train\", \"predict\", \"\").","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParVals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the parameter values of the learner. — getLearnerParVals","text":"(list). named list values.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParamSet.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the parameter set of the learner. — getLearnerParamSet","title":"Get the parameter set of the learner. — getLearnerParamSet","text":"Alias getParamSet.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParamSet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the parameter set of the learner. — getLearnerParamSet","text":"","code":"getLearnerParamSet(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParamSet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the parameter set of the learner. — getLearnerParamSet","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerParamSet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the parameter set of the learner. — getLearnerParamSet","text":"ParamSet.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPredictType.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the predict type of the learner. — getLearnerPredictType","title":"Get the predict type of the learner. — getLearnerPredictType","text":"Get predict type learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPredictType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the predict type of the learner. — getLearnerPredictType","text":"","code":"getLearnerPredictType(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPredictType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the predict type of the learner. — getLearnerPredictType","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerPredictType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the predict type of the learner. — getLearnerPredictType","text":"(character(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerShortName.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the short name of the learner. — getLearnerShortName","title":"Get the short name of the learner. — getLearnerShortName","text":"ordinary learner simply short name returned. wrapped learners, wrapper id successively attached short name base learner. E.g: “rf.bagged.imputed”","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerShortName.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the short name of the learner. — getLearnerShortName","text":"","code":"getLearnerShortName(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerShortName.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the short name of the learner. — getLearnerShortName","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerShortName.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the short name of the learner. — getLearnerShortName","text":"(character(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerType.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the type of the learner. — getLearnerType","title":"Get the type of the learner. — getLearnerType","text":"Get type learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the type of the learner. — getLearnerType","text":"","code":"getLearnerType(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the type of the learner. — getLearnerType","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getLearnerType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the type of the learner. — getLearnerType","text":"(character(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getMlrOptions.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns a list of mlr's options. — getMlrOptions","title":"Returns a list of mlr's options. — getMlrOptions","text":"Gets options mlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getMlrOptions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns a list of mlr's options. — getMlrOptions","text":"","code":"getMlrOptions()"},{"path":"https://mlr.mlr-org.com/dev/reference/getMlrOptions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns a list of mlr's options. — getMlrOptions","text":"(list).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getMultilabelBinaryPerformances.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve binary classification measures for multilabel classification predictions. — getMultilabelBinaryPerformances","title":"Retrieve binary classification measures for multilabel classification predictions. — getMultilabelBinaryPerformances","text":"Measures quality binary label prediction w.r.t. binary classification performance measure.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getMultilabelBinaryPerformances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve binary classification measures for multilabel classification predictions. — getMultilabelBinaryPerformances","text":"","code":"getMultilabelBinaryPerformances(pred, measures)"},{"path":"https://mlr.mlr-org.com/dev/reference/getMultilabelBinaryPerformances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve binary classification measures for multilabel classification predictions. — getMultilabelBinaryPerformances","text":"pred (Prediction) Multilabel Prediction object. measures (Measure | list Measure) Performance measure(s) evaluate, must applicable binary classification performance. Default mmce.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getMultilabelBinaryPerformances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve binary classification measures for multilabel classification predictions. — getMultilabelBinaryPerformances","text":"(named matrix). Performance value(s), column names measure(s), row names labels.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getMultilabelBinaryPerformances.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve binary classification measures for multilabel classification predictions. — getMultilabelBinaryPerformances","text":"","code":"# see makeMultilabelBinaryRelevanceWrapper"},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsOptPathDf.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the opt.paths from each tuning step from the outer resampling. — getNestedTuneResultsOptPathDf","title":"Get the opt.paths from each tuning step from the outer resampling. — getNestedTuneResultsOptPathDf","text":"resampled tuning wrapper (see makeTuneWrapper) resample(..., extract = getTuneResult) helper returns data.frame opt.paths combined rbind. additional column iter indicates resampling iteration row belongs.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsOptPathDf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the opt.paths from each tuning step from the outer resampling. — getNestedTuneResultsOptPathDf","text":"","code":"getNestedTuneResultsOptPathDf(r, trafo = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsOptPathDf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the opt.paths from each tuning step from the outer resampling. — getNestedTuneResultsOptPathDf","text":"r (ResampleResult)  result resampling tuning wrapper. trafo (logical(1)) units hyperparameter path converted transformed scale? necessary trafo used create opt.paths. Note opt.paths always stored untransformed scale. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsOptPathDf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the opt.paths from each tuning step from the outer resampling. — getNestedTuneResultsOptPathDf","text":"(data.frame). See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsOptPathDf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the opt.paths from each tuning step from the outer resampling. — getNestedTuneResultsOptPathDf","text":"","code":"# see example of makeTuneWrapper"},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsX.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the tuned hyperparameter settings from a nested tuning. — getNestedTuneResultsX","title":"Get the tuned hyperparameter settings from a nested tuning. — getNestedTuneResultsX","text":"resampled tuning wrapper (see makeTuneWrapper) resample(..., extract = getTuneResult) helper returns data.frame best found hyperparameter settings resampling iteration.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsX.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the tuned hyperparameter settings from a nested tuning. — getNestedTuneResultsX","text":"","code":"getNestedTuneResultsX(r)"},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsX.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the tuned hyperparameter settings from a nested tuning. — getNestedTuneResultsX","text":"r (ResampleResult)  result resampling tuning wrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsX.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the tuned hyperparameter settings from a nested tuning. — getNestedTuneResultsX","text":"(data.frame). One column tuned hyperparameter one row outer resampling iteration.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getNestedTuneResultsX.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the tuned hyperparameter settings from a nested tuning. — getNestedTuneResultsX","text":"","code":"# see example of makeTuneWrapper"},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPreds.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts out-of-bag predictions from trained models. — getOOBPreds","title":"Extracts out-of-bag predictions from trained models. — getOOBPreds","text":"Learners like randomForest produce --bag predictions. getOOBPreds extracts information trained models builds prediction object provided predict (prediction time set NA). classification case: stored exactly (Prediction) object depends predict.type setting Learner. can call listLearners(properties = \"oobpreds\") get list learners provide .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPreds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts out-of-bag predictions from trained models. — getOOBPreds","text":"","code":"getOOBPreds(model, task)"},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPreds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts out-of-bag predictions from trained models. — getOOBPreds","text":"model (WrappedModel) model. task (Task) task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPreds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts out-of-bag predictions from trained models. — getOOBPreds","text":"(Prediction).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPreds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extracts out-of-bag predictions from trained models. — getOOBPreds","text":"","code":"training.set = sample(1:150, 50) lrn = makeLearner(\"classif.ranger\", predict.type = \"prob\", predict.threshold = 0.6) mod = train(lrn, sonar.task, subset = training.set) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions oob = getOOBPreds(mod, sonar.task) #> Error in checkClass(x, classes, ordered, null.ok): object 'mod' not found oob #> Error in eval(expr, envir, enclos): object 'oob' not found performance(oob, measures = list(auc, mmce)) #> Error in performance(oob, measures = list(auc, mmce)): object 'oob' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPredsLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Provides out-of-bag predictions for a given model and the corresponding learner. — getOOBPredsLearner","title":"Provides out-of-bag predictions for a given model and the corresponding learner. — getOOBPredsLearner","text":"function mostly internal usage. get --bag predictions use getOOBPreds.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPredsLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provides out-of-bag predictions for a given model and the corresponding learner. — getOOBPredsLearner","text":"","code":"getOOBPredsLearner(.learner, .model)"},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPredsLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provides out-of-bag predictions for a given model and the corresponding learner. — getOOBPredsLearner","text":".learner (Learner) learner. .model (WrappedModel) Wrapped model.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getOOBPredsLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Provides out-of-bag predictions for a given model and the corresponding learner. — getOOBPredsLearner","text":"output structure (predictLearner).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getParamSet.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a description of all possible parameter settings for a learner. — getParamSet","title":"Get a description of all possible parameter settings for a learner. — getParamSet","text":"Returns ParamHelpers::ParamSet Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getParamSet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a description of all possible parameter settings for a learner. — getParamSet","text":"ParamSet.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionDump.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the error dump of a failed Prediction. — getPredictionDump","title":"Return the error dump of a failed Prediction. — getPredictionDump","text":"Returns error dump can used debugger() evaluate errors. configureMlr configuration .error.dump FALSE prediction fail, returns NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionDump.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the error dump of a failed Prediction. — getPredictionDump","text":"","code":"getPredictionDump(pred)"},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionDump.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the error dump of a failed Prediction. — getPredictionDump","text":"pred (Prediction) Prediction object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionDump.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the error dump of a failed Prediction. — getPredictionDump","text":"(last.dump).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionProbabilities.html","id":null,"dir":"Reference","previous_headings":"","what":"Get probabilities for some classes. — getPredictionProbabilities","title":"Get probabilities for some classes. — getPredictionProbabilities","text":"Get probabilities classes.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionProbabilities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get probabilities for some classes. — getPredictionProbabilities","text":"","code":"getPredictionProbabilities(pred, cl)"},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionProbabilities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get probabilities for some classes. — getPredictionProbabilities","text":"pred (Prediction) Prediction object. cl (character) Names classes. Default either classes multi-class / multilabel problems positive class binary classification.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionProbabilities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get probabilities for some classes. — getPredictionProbabilities","text":"(data.frame) numerical columns numerical vector length cl 1. Order columns defined cl.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionProbabilities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get probabilities for some classes. — getPredictionProbabilities","text":"","code":"task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.lda\", predict.type = \"prob\") mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions # predict probabilities pred = predict(mod, newdata = iris) #> Error in predict(mod, newdata = iris): object 'mod' not found  # Get probabilities for all classes head(getPredictionProbabilities(pred)) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found  # Get probabilities for a subset of classes head(getPredictionProbabilities(pred, c(\"setosa\", \"virginica\"))) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionResponse.html","id":null,"dir":"Reference","previous_headings":"","what":"Get response / truth from prediction object. — getPredictionResponse","title":"Get response / truth from prediction object. — getPredictionResponse","text":"following types returned, depending task type:","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionResponse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get response / truth from prediction object. — getPredictionResponse","text":"","code":"getPredictionResponse(pred)  getPredictionSE(pred)  getPredictionTruth(pred)"},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionResponse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get response / truth from prediction object. — getPredictionResponse","text":"pred (Prediction) Prediction object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionResponse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get response / truth from prediction object. — getPredictionResponse","text":"See .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionTaskDesc.html","id":null,"dir":"Reference","previous_headings":"","what":"Get summarizing task description from prediction. — getPredictionTaskDesc","title":"Get summarizing task description from prediction. — getPredictionTaskDesc","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionTaskDesc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get summarizing task description from prediction. — getPredictionTaskDesc","text":"","code":"getPredictionTaskDesc(pred)"},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionTaskDesc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get summarizing task description from prediction. — getPredictionTaskDesc","text":"pred (Prediction) Prediction object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getPredictionTaskDesc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get summarizing task description from prediction. — getPredictionTaskDesc","text":"ret_taskdesc","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getProbabilities.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated, use getPredictionProbabilities instead. — getProbabilities","title":"Deprecated, use getPredictionProbabilities instead. — getProbabilities","text":"Deprecated, use getPredictionProbabilities instead.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getProbabilities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated, use getPredictionProbabilities instead. — getProbabilities","text":"","code":"getProbabilities(pred, cl)"},{"path":"https://mlr.mlr-org.com/dev/reference/getProbabilities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated, use getPredictionProbabilities instead. — getProbabilities","text":"pred Deprecated. cl Deprecated.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRDump.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the error dump of ResampleResult. — getRRDump","title":"Return the error dump of ResampleResult. — getRRDump","text":"Returns error dumps generated resampling, can used debugger() debug errors. dumps saved configureMlr configuration .error.dump, corresponding learner config, TRUE. returned object list many entries resampling used folds. entries can subset following slots, depending step resampling iteration failed: “train” (error training step), “predict.train” (prediction training subset), “predict.test” (prediction test subset).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRDump.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the error dump of ResampleResult. — getRRDump","text":"","code":"getRRDump(res)"},{"path":"https://mlr.mlr-org.com/dev/reference/getRRDump.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the error dump of ResampleResult. — getRRDump","text":"res (ResampleResult) result resample.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRDump.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the error dump of ResampleResult. — getRRDump","text":"list.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictionList.html","id":null,"dir":"Reference","previous_headings":"","what":"Get list of predictions for train and test set of each single resample iteration. — getRRPredictionList","title":"Get list of predictions for train and test set of each single resample iteration. — getRRPredictionList","text":"function creates list two slots train test slot list Prediction objects single resample iteration. case predict = \"train\" used resample description (see makeResampleDesc), slot test NULL case predict = \"test\" used, slot train NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictionList.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get list of predictions for train and test set of each single resample iteration. — getRRPredictionList","text":"","code":"getRRPredictionList(res, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictionList.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get list of predictions for train and test set of each single resample iteration. — getRRPredictionList","text":"res (ResampleResult) result resample run keep.pred = TRUE. ... () options passed makePrediction.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictionList.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get list of predictions for train and test set of each single resample iteration. — getRRPredictionList","text":"list.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get predictions from resample results. — getRRPredictions","title":"Get predictions from resample results. — getRRPredictions","text":"simple getter.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get predictions from resample results. — getRRPredictions","text":"","code":"getRRPredictions(res)"},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get predictions from resample results. — getRRPredictions","text":"res (ResampleResult) result resample run keep.pred = TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRPredictions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get predictions from resample results. — getRRPredictions","text":"(ResamplePrediction).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDesc.html","id":null,"dir":"Reference","previous_headings":"","what":"Get task description from resample results (DEPRECATED). — getRRTaskDesc","title":"Get task description from resample results (DEPRECATED). — getRRTaskDesc","text":"Get summarizing task description.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDesc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get task description from resample results (DEPRECATED). — getRRTaskDesc","text":"","code":"getRRTaskDesc(res)"},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDesc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get task description from resample results (DEPRECATED). — getRRTaskDesc","text":"res (ResampleResult) result resample.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDesc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get task description from resample results (DEPRECATED). — getRRTaskDesc","text":"(TaskDesc).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDescription.html","id":null,"dir":"Reference","previous_headings":"","what":"Get task description from resample results (DEPRECATED). — getRRTaskDescription","title":"Get task description from resample results (DEPRECATED). — getRRTaskDescription","text":"Get summarizing task description.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDescription.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get task description from resample results (DEPRECATED). — getRRTaskDescription","text":"","code":"getRRTaskDescription(res)"},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDescription.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get task description from resample results (DEPRECATED). — getRRTaskDescription","text":"res (ResampleResult) result resample.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getRRTaskDescription.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get task description from resample results (DEPRECATED). — getRRTaskDescription","text":"(TaskDesc).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getResamplingIndices.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the resampling indices from a tuning or feature selection wrapper.. — getResamplingIndices","title":"Get the resampling indices from a tuning or feature selection wrapper.. — getResamplingIndices","text":"resampled tuning feature selection wrapper (see makeTuneWrapper) resample(..., extract = getTuneResult) resample(..., extract = getFeatSelResult) helper returns list resampling indices used respective method.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getResamplingIndices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the resampling indices from a tuning or feature selection wrapper.. — getResamplingIndices","text":"","code":"getResamplingIndices(object, inner = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getResamplingIndices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the resampling indices from a tuning or feature selection wrapper.. — getResamplingIndices","text":"object (ResampleResult)  result resampling tuning feature selection wrapper. inner (logical)  TRUE, returns inner indices nested resampling setting.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getResamplingIndices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the resampling indices from a tuning or feature selection wrapper.. — getResamplingIndices","text":"(list). One list outer resampling fold.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getResamplingIndices.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the resampling indices from a tuning or feature selection wrapper.. — getResamplingIndices","text":"","code":"task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.rpart\") # stupid mini grid ps = makeParamSet(   makeDiscreteParam(\"cp\", values = c(0.05, 0.1)),   makeDiscreteParam(\"minsplit\", values = c(10, 20)) ) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"Holdout\") outer = makeResampleDesc(\"CV\", iters = 2) lrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl) # nested resampling for evaluation # we also extract tuned hyper pars in each iteration and by that the resampling indices r = resample(lrn, task, outer, extract = getTuneResult) #> Resampling: cross-validation #> Measures:             mmce       #> [Tune] Started tuning learner classif.rpart for parameter set: #>              Type len Def   Constr Req Tunable Trafo #> cp       discrete   -   - 0.05,0.1   -    TRUE     - #> minsplit discrete   -   -    10,20   -    TRUE     - #> With control class: TuneControlGrid #> Imputation value: 1 #> [Tune-x] 1: cp=0.05; minsplit=10 #> [Tune-y] 1: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune-x] 2: cp=0.1; minsplit=10 #> [Tune-y] 2: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune-x] 3: cp=0.05; minsplit=20 #> [Tune-y] 3: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune-x] 4: cp=0.1; minsplit=20 #> [Tune-y] 4: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune] Result: cp=0.05; minsplit=20 : mmce.test.mean=0.0800000 #> [Resample] iter 1:    0.0533333  #> [Tune] Started tuning learner classif.rpart for parameter set: #>              Type len Def   Constr Req Tunable Trafo #> cp       discrete   -   - 0.05,0.1   -    TRUE     - #> minsplit discrete   -   -    10,20   -    TRUE     - #> With control class: TuneControlGrid #> Imputation value: 1 #> [Tune-x] 1: cp=0.05; minsplit=10 #> [Tune-y] 1: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune-x] 2: cp=0.1; minsplit=10 #> [Tune-y] 2: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune-x] 3: cp=0.05; minsplit=20 #> [Tune-y] 3: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune-x] 4: cp=0.1; minsplit=20 #> [Tune-y] 4: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune] Result: cp=0.1; minsplit=20 : mmce.test.mean=0.0400000 #> [Resample] iter 2:    0.0400000  #>  #> Aggregated Result: mmce.test.mean=0.0466667 #>  # get tuning indices getResamplingIndices(r, inner = TRUE) #> [[1]] #> [[1]]$train.inds #> [[1]]$train.inds[[1]] #>  [1]  79 125 106  92  58  54  85   4   1   7  82  93 103  53  91  90  29  66  77 #> [20]  22  49  65 130  94 116  61  87  80  95 111   8  48  16 149  26  76 137  33 #> [39]  11   6 135  60 124 118 105  10   9  39  62 119 #>  #>  #> [[1]]$test.inds #> [[1]]$test.inds[[1]] #>  [1]  69  59  99  40 122 102 114  19  96 148 127 123  43  31  45  78  13 121  28 #> [20] 132 133  37 100 112 117 #>  #>  #>  #> [[2]] #> [[2]]$train.inds #> [[2]]$train.inds[[1]] #>  [1]  47 147 139  36  34  75 109  55  23 150  83  27  41  84 142 144  24 145  64 #> [20]  14  42  81  51  97   2  57  32 120  30  88 115  12  71  52 110 107   3  18 #> [39]   5  21 126  46  44 140  17  50  35 128  38 138 #>  #>  #> [[2]]$test.inds #> [[2]]$test.inds[[1]] #>  [1]  20 131 134 146  56  63  86 141  15 143  74  89  25  72  98  70  67  68 108 #> [20] 136 101 104 129  73 113 #>  #>  #>"},{"path":"https://mlr.mlr-org.com/dev/reference/getStackedBaseLearnerPredictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the predictions for each base learner. — getStackedBaseLearnerPredictions","title":"Returns the predictions for each base learner. — getStackedBaseLearnerPredictions","text":"Returns predictions base learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getStackedBaseLearnerPredictions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the predictions for each base learner. — getStackedBaseLearnerPredictions","text":"","code":"getStackedBaseLearnerPredictions(model, newdata = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/getStackedBaseLearnerPredictions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the predictions for each base learner. — getStackedBaseLearnerPredictions","text":"model (WrappedModel) Wrapped model, result train. newdata (data.frame) New observations, predictions using specified base learners returned. Default NULL extracts base learner predictions made training.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getStackedBaseLearnerPredictions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Returns the predictions for each base learner. — getStackedBaseLearnerPredictions","text":"None.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskClassLevels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the class levels for classification and multilabel tasks. — getTaskClassLevels","title":"Get the class levels for classification and multilabel tasks. — getTaskClassLevels","text":"NB: multilabel, getTaskTargetNames getTaskClassLevels actually return thing.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskClassLevels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the class levels for classification and multilabel tasks. — getTaskClassLevels","text":"","code":"getTaskClassLevels(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskClassLevels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the class levels for classification and multilabel tasks. — getTaskClassLevels","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskClassLevels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the class levels for classification and multilabel tasks. — getTaskClassLevels","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskCosts.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract costs in task. — getTaskCosts","title":"Extract costs in task. — getTaskCosts","text":"Returns “NULL” task type “costsens”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskCosts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract costs in task. — getTaskCosts","text":"","code":"getTaskCosts(task, subset = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskCosts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract costs in task. — getTaskCosts","text":"task (CostSensTask) task. subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskCosts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract costs in task. — getTaskCosts","text":"(matrix | NULL).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskData.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract data in task. — getTaskData","title":"Extract data in task. — getTaskData","text":"Useful trainLearner add learning machine package.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract data in task. — getTaskData","text":"","code":"getTaskData(   task,   subset = NULL,   features,   target.extra = FALSE,   recode.target = \"no\",   functionals.as = \"dfcols\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract data in task. — getTaskData","text":"task (Task) task. subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used. features (character | integer | logical) Vector selected inputs. can either pass character vector feature names, vector indices, logical vector. case index vector element denotes position feature name returned getTaskFeatureNames. Note target feature always included resulting task, pass . Default use features. target.extra (logical(1)) target vector returned separately? , single data.frame including target columns returned, otherwise list input data.frame extra vector data.frame targets. Default FALSE. recode.target (character(1)) target classes recoded? Supported binary multilabel classification survival. Possible values binary classification “01”, “-1+1” “drop.levels”. two latter cases target vector converted numeric vector. positive class coded “+1” negative class either “0” “-1”. “drop.levels” remove empty factor levels target column. multilabel case logical targets can converted factors “multilabel.factor”. survival, may choose recode survival times “left”, “right” “interval2” censored times using “lcens”, “rcens” “icens”, respectively. See survival::Surv format specification. Default binary classification survival “” (nothing). functionals.(character(1)) represents functional features? Option “matrix”: Keep matrix columns data.frame. Option “dfcols”: Convert individual numeric data.frame columns. Default “dfcols”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract data in task. — getTaskData","text":"Either data.frame list data.frame data vector target.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract data in task. — getTaskData","text":"","code":"library(\"mlbench\") data(BreastCancer)  df = BreastCancer df$Id = NULL task = makeClassifTask(id = \"BreastCancer\", data = df, target = \"Class\", positive = \"malignant\") head(getTaskData) #>                                                                   #> 1 function (task, subset = NULL, features, target.extra = FALSE,  #> 2     recode.target = \"no\", functionals.as = \"dfcols\")            #> 3 {                                                               #> 4     checkTask(task, \"Task\")                                     #> 5     checkTaskSubset(subset, size = task$task.desc$size)         #> 6     assertLogical(target.extra)                                 head(getTaskData(task, features = c(\"Cell.size\", \"Cell.shape\"), recode.target = \"-1+1\")) #>   Cell.size Cell.shape Class #> 1         1          1    -1 #> 2         4          4    -1 #> 3         1          1    -1 #> 4         8          8    -1 #> 5         1          1    -1 #> 6        10         10     1 head(getTaskData(task, subset = 1:100, recode.target = \"01\")) #>   Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size Bare.nuclei #> 1            5         1          1             1            2           1 #> 2            5         4          4             5            7          10 #> 3            3         1          1             1            2           2 #> 4            6         8          8             1            3           4 #> 5            4         1          1             3            2           1 #> 6            8        10         10             8            7          10 #>   Bl.cromatin Normal.nucleoli Mitoses Class #> 1           3               1       1     0 #> 2           3               2       1     0 #> 3           3               1       1     0 #> 4           3               7       1     0 #> 5           3               1       1     0 #> 6           9               7       1     1"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDesc.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a summarizing task description. — getTaskDesc","title":"Get a summarizing task description. — getTaskDesc","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDesc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a summarizing task description. — getTaskDesc","text":"","code":"getTaskDesc(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDesc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a summarizing task description. — getTaskDesc","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDesc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a summarizing task description. — getTaskDesc","text":"ret_taskdesc","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDescription.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated, use getTaskDesc instead. — getTaskDescription","title":"Deprecated, use getTaskDesc instead. — getTaskDescription","text":"Deprecated, use getTaskDesc instead.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDescription.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated, use getTaskDesc instead. — getTaskDescription","text":"","code":"getTaskDescription(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskDescription.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated, use getTaskDesc instead. — getTaskDescription","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFeatureNames.html","id":null,"dir":"Reference","previous_headings":"","what":"Get feature names of task. — getTaskFeatureNames","title":"Get feature names of task. — getTaskFeatureNames","text":"Target column name included.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFeatureNames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get feature names of task. — getTaskFeatureNames","text":"","code":"getTaskFeatureNames(task)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFeatureNames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get feature names of task. — getTaskFeatureNames","text":"task (Task) task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFeatureNames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get feature names of task. — getTaskFeatureNames","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFormula.html","id":null,"dir":"Reference","previous_headings":"","what":"Get formula of a task. — getTaskFormula","title":"Get formula of a task. — getTaskFormula","text":"usually simply <target> ~ . multilabel <target_1> + ... + <target_k> ~.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFormula.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get formula of a task. — getTaskFormula","text":"","code":"getTaskFormula(   x,   target = getTaskTargetNames(x),   explicit.features = FALSE,   env = parent.frame() )"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFormula.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get formula of a task. — getTaskFormula","text":"x (Task | TaskDesc) Task description object. target (character(1)) Left hand side formula. Default defined task x. explicit.features (logical(1)) features (right hand side formula) explicitly listed? Default FALSE, .e., represented \".\". env (environment) Environment formula. Default parent.frame().","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskFormula.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get formula of a task. — getTaskFormula","text":"(formula).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskId.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the id of the task. — getTaskId","title":"Get the id of the task. — getTaskId","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskId.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the id of the task. — getTaskId","text":"","code":"getTaskId(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskId.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the id of the task. — getTaskId","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskId.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the id of the task. — getTaskId","text":"(character(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskNFeats.html","id":null,"dir":"Reference","previous_headings":"","what":"Get number of features in task. — getTaskNFeats","title":"Get number of features in task. — getTaskNFeats","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskNFeats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get number of features in task. — getTaskNFeats","text":"","code":"getTaskNFeats(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskNFeats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get number of features in task. — getTaskNFeats","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskNFeats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get number of features in task. — getTaskNFeats","text":"(integer(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskSize.html","id":null,"dir":"Reference","previous_headings":"","what":"Get number of observations in task. — getTaskSize","title":"Get number of observations in task. — getTaskSize","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskSize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get number of observations in task. — getTaskSize","text":"","code":"getTaskSize(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskSize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get number of observations in task. — getTaskSize","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskSize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get number of observations in task. — getTaskSize","text":"(integer(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargetNames.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the name(s) of the target column(s). — getTaskTargetNames","title":"Get the name(s) of the target column(s). — getTaskTargetNames","text":"NB: multilabel, getTaskTargetNames getTaskClassLevels actually return thing.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargetNames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the name(s) of the target column(s). — getTaskTargetNames","text":"","code":"getTaskTargetNames(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargetNames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the name(s) of the target column(s). — getTaskTargetNames","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargetNames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the name(s) of the target column(s). — getTaskTargetNames","text":"(character).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargets.html","id":null,"dir":"Reference","previous_headings":"","what":"Get target data of task. — getTaskTargets","title":"Get target data of task. — getTaskTargets","text":"Get target data task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get target data of task. — getTaskTargets","text":"","code":"getTaskTargets(task, recode.target = \"no\")"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get target data of task. — getTaskTargets","text":"task (Task) task. recode.target (character(1)) target classes recoded? Supported binary multilabel classification survival. Possible values binary classification “01”, “-1+1” “drop.levels”. two latter cases target vector converted numeric vector. positive class coded “+1” negative class either “0” “-1”. “drop.levels” remove empty factor levels target column. multilabel case logical targets can converted factors “multilabel.factor”. survival, may choose recode survival times “left”, “right” “interval2” censored times using “lcens”, “rcens” “icens”, respectively. See survival::Surv format specification. Default binary classification survival “” (nothing).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get target data of task. — getTaskTargets","text":"factor classification numeric regression, data.frame logical columns multilabel.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskTargets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get target data of task. — getTaskTargets","text":"","code":"task = makeClassifTask(data = iris, target = \"Species\") getTaskTargets(task) #>   [1] setosa     setosa     setosa     setosa     setosa     setosa     #>   [7] setosa     setosa     setosa     setosa     setosa     setosa     #>  [13] setosa     setosa     setosa     setosa     setosa     setosa     #>  [19] setosa     setosa     setosa     setosa     setosa     setosa     #>  [25] setosa     setosa     setosa     setosa     setosa     setosa     #>  [31] setosa     setosa     setosa     setosa     setosa     setosa     #>  [37] setosa     setosa     setosa     setosa     setosa     setosa     #>  [43] setosa     setosa     setosa     setosa     setosa     setosa     #>  [49] setosa     setosa     versicolor versicolor versicolor versicolor #>  [55] versicolor versicolor versicolor versicolor versicolor versicolor #>  [61] versicolor versicolor versicolor versicolor versicolor versicolor #>  [67] versicolor versicolor versicolor versicolor versicolor versicolor #>  [73] versicolor versicolor versicolor versicolor versicolor versicolor #>  [79] versicolor versicolor versicolor versicolor versicolor versicolor #>  [85] versicolor versicolor versicolor versicolor versicolor versicolor #>  [91] versicolor versicolor versicolor versicolor versicolor versicolor #>  [97] versicolor versicolor versicolor versicolor virginica  virginica  #> [103] virginica  virginica  virginica  virginica  virginica  virginica  #> [109] virginica  virginica  virginica  virginica  virginica  virginica  #> [115] virginica  virginica  virginica  virginica  virginica  virginica  #> [121] virginica  virginica  virginica  virginica  virginica  virginica  #> [127] virginica  virginica  virginica  virginica  virginica  virginica  #> [133] virginica  virginica  virginica  virginica  virginica  virginica  #> [139] virginica  virginica  virginica  virginica  virginica  virginica  #> [145] virginica  virginica  virginica  virginica  virginica  virginica  #> Levels: setosa versicolor virginica"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskType.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the type of the task. — getTaskType","title":"Get the type of the task. — getTaskType","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the type of the task. — getTaskType","text":"","code":"getTaskType(x)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the type of the task. — getTaskType","text":"x (Task | TaskDesc) Task description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTaskType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the type of the task. — getTaskType","text":"(character(1)).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the optimal hyperparameters and optimization path after training. — getTuneResult","title":"Returns the optimal hyperparameters and optimization path after training. — getTuneResult","text":"Returns optimal hyperparameters optimization path training.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResult.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the optimal hyperparameters and optimization path after training. — getTuneResult","text":"","code":"getTuneResult(object)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResult.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the optimal hyperparameters and optimization path after training. — getTuneResult","text":"object (WrappedModel) Trained Model created makeTuneWrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResult.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the optimal hyperparameters and optimization path after training. — getTuneResult","text":"(TuneResult).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResultOptPath.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the optimization path of a tuning result. — getTuneResultOptPath","title":"Get the optimization path of a tuning result. — getTuneResultOptPath","text":"Returns opt.path (TuneResult) object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResultOptPath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the optimization path of a tuning result. — getTuneResultOptPath","text":"","code":"getTuneResultOptPath(tune.result, as.df = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResultOptPath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the optimization path of a tuning result. — getTuneResultOptPath","text":"tune.result (TuneResult)  tuning result (tuneParams) function. .df (logical(1)) optimization path returned data frame? Default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/getTuneResultOptPath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the optimization path of a tuning result. — getTuneResultOptPath","text":"(ParamHelpers::OptPath) (data.frame).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/gunpoint.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Gunpoint functional data classification task. — gunpoint.task","title":"Gunpoint functional data classification task. — gunpoint.task","text":"Contains task (gunpoint.task). classify whether person raises gun just empty hand.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/gunpoint.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gunpoint functional data classification task. — gunpoint.task","text":"See Ratanamahatana, C. . & Keogh. E. (2004). Everything know Dynamic Time Warping Wrong. Proceedings SIAM International Conference Data Mining (SDM05), 506-510.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/hasFunctionalFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether the object contains functional features. — hasFunctionalFeatures","title":"Check whether the object contains functional features. — hasFunctionalFeatures","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/hasFunctionalFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether the object contains functional features. — hasFunctionalFeatures","text":"","code":"hasFunctionalFeatures(obj)"},{"path":"https://mlr.mlr-org.com/dev/reference/hasFunctionalFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether the object contains functional features. — hasFunctionalFeatures","text":"obj (Task | TaskDesc | data.frame) Object check.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/hasFunctionalFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether the object contains functional features. — hasFunctionalFeatures","text":"(logical(1))","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/hasProperties.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated, use hasLearnerProperties instead. — hasProperties","title":"Deprecated, use hasLearnerProperties instead. — hasProperties","text":"Deprecated, use hasLearnerProperties instead.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/hasProperties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated, use hasLearnerProperties instead. — hasProperties","text":"","code":"hasProperties(learner, props)"},{"path":"https://mlr.mlr-org.com/dev/reference/hasProperties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated, use hasLearnerProperties instead. — hasProperties","text":"learner Deprecated. props Deprecated.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/helpLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Access help page of learner functions. — helpLearner","title":"Access help page of learner functions. — helpLearner","text":"Interactive function gives user quick access help pages associated various functions involved given learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/helpLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access help page of learner functions. — helpLearner","text":"","code":"helpLearner(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/helpLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Access help page of learner functions. — helpLearner","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/helpLearnerParam.html","id":null,"dir":"Reference","previous_headings":"","what":"Get specific help for a learner's parameters. — helpLearnerParam","title":"Get specific help for a learner's parameters. — helpLearnerParam","text":"Print description parameters given learner. description automatically extracted help pages learner, may incomplete.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/helpLearnerParam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get specific help for a learner's parameters. — helpLearnerParam","text":"","code":"helpLearnerParam(learner, param = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/helpLearnerParam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get specific help for a learner's parameters. — helpLearnerParam","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. param (character | NULL) Parameter(s) describe. Defaults NULL, prints information documentation status parameters.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/imputations.html","id":null,"dir":"Reference","previous_headings":"","what":"Built-in imputation methods. — imputations","title":"Built-in imputation methods. — imputations","text":"built-ins : imputeConstant(const) imputation using constant value, imputeMedian() imputation using median, imputeMode() imputation using mode, imputeMin(multiplier) imputing constant values shifted minimum using min(x) - multiplier * diff(range(x)), imputeMax(multiplier) imputing constant values shifted maximum using max(x) + multiplier * diff(range(x)), imputeNormal(mean, sd) imputation using normally distributed random values. Mean standard deviation calculated data provided. imputeHist(breaks, use.mids) imputation using random values probabilities calculated using table hist. imputeLearner(learner, features = NULL) imputations using response classification regression learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/imputations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Built-in imputation methods. — imputations","text":"","code":"imputeConstant(const)  imputeMedian()  imputeMean()  imputeMode()  imputeMin(multiplier = 1)  imputeMax(multiplier = 1)  imputeUniform(min = NA_real_, max = NA_real_)  imputeNormal(mu = NA_real_, sd = NA_real_)  imputeHist(breaks, use.mids = TRUE)  imputeLearner(learner, features = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/imputations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Built-in imputation methods. — imputations","text":"const () Constant valued use imputation. multiplier (numeric(1)) Value stored minimum maximum multiplied imputation done. min (numeric(1)) Lower bound uniform distribution. NA (default), estimated data. max (numeric(1)) Upper bound uniform distribution. NA (default), estimated data. mu (numeric(1)) Mean normal distribution. missing estimated data. sd (numeric(1)) Standard deviation normal distribution. missing estimated data. breaks (numeric(1)) Number breaks use graphics::hist. missing, defaults auto-detection via “Sturges”. use.mids (logical(1)) x numeric histogram used, impute bin mids (default) instead draw uniformly distributed samples within bin range. learner (Learner | character(1)) Supervised learner. predictions used imputations. pass string learner created via makeLearner. Note target column available operation. features (character) Features use learner prediction. Default NULL uses available features except target column original task.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/impute.html","id":null,"dir":"Reference","previous_headings":"","what":"Impute and re-impute data — impute","title":"Impute and re-impute data — impute","text":"Allows imputation missing feature values various techniques. Note possibility re-impute data set way imputation performed training. especially comes handy resampling one wants perform imputation test set training set. function impute performs imputation data set returns, alongside imputed data set, “ImputationDesc” object can contain “learned” coefficients helpful data. can passed together new data set reimpute. imputation techniques can specified certain features feature classes, see function arguments. can either provide arbitrary object, use built-imputation method listed imputations create one using makeImputeMethod.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Impute and re-impute data — impute","text":"","code":"impute(   obj,   target = character(0L),   classes = list(),   cols = list(),   dummy.classes = character(0L),   dummy.cols = character(0L),   dummy.type = \"factor\",   force.dummies = FALSE,   impute.new.levels = TRUE,   recode.factor.levels = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Impute and re-impute data — impute","text":"obj (data.frame | Task) Input data. target (character) Name column(s) specifying response. Default character(0). classes (named list) Named list containing imputation techniques classes columns. E.g. list(numeric = imputeMedian()). cols (named list) Named list containing names imputation methods impute missing values data column referenced list element's name. Overrules imputation set via classes. dummy.classes (character) Classes columns create dummy columns . Default character(0). dummy.cols (character) Column names create dummy columns (containing binary missing indicator) . Default character(0). dummy.type (character(1)) dummy columns encoded. Either 0/1 type “numeric” “factor”. Default “factor”. force.dummies (logical(1)) Force dummy creation even respective data column contain NAs. Note () learners complain constant columns created way (b) feature set might stochastic turn . Default FALSE. impute.new.levels (logical(1)) new, unencountered factor level occur reimputation, handled NAs imputed way? Default TRUE. recode.factor.levels (logical(1)) Recode factor levels reimputation, match respective element lvls (description object) therefore match levels feature factor training data imputation?. Default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Impute and re-impute data — impute","text":"(list) data (data.frame) Imputed data. desc (ImputationDesc) Description object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/impute.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Impute and re-impute data — impute","text":"description object contains slots target (character) See argument. features (character) Feature names (column names data). classes (character) Feature classes (storage type data). lvls (named list) Mapping column names factor features levels, including newly created ones imputation. impute (named list) Mapping column names imputation functions. dummies (named list) Mapping column names imputation functions. impute.new.levels (logical(1)) See argument. recode.factor.levels (logical(1)) See argument.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Impute and re-impute data — impute","text":"","code":"df = data.frame(x = c(1, 1, NA), y = factor(c(\"a\", \"a\", \"b\")), z = 1:3) imputed = impute(df, target = character(0), cols = list(x = 99, y = imputeMode())) print(imputed$data) #>    x y z #> 1  1 a 1 #> 2  1 a 2 #> 3 99 b 3 reimpute(data.frame(x = NA_real_), imputed$desc) #>    x y  z #> 1 99 a NA"},{"path":"https://mlr.mlr-org.com/dev/reference/iris.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Iris classification task. — iris.task","title":"Iris classification task. — iris.task","text":"Contains task (iris.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/iris.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Iris classification task. — iris.task","text":"See datasets::iris.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/isFailureModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Is the model a FailureModel? — isFailureModel","title":"Is the model a FailureModel? — isFailureModel","text":"model created one sets corresponding option configureMlr. complex wrappers getter returns TRUE model contained failed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/isFailureModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Is the model a FailureModel? — isFailureModel","text":"","code":"isFailureModel(model)"},{"path":"https://mlr.mlr-org.com/dev/reference/isFailureModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Is the model a FailureModel? — isFailureModel","text":"model (WrappedModel) model.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/isFailureModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Is the model a FailureModel? — isFailureModel","text":"(logical(1)).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/joinClassLevels.html","id":null,"dir":"Reference","previous_headings":"","what":"Join some class existing levels to new, larger class levels for classification problems. — joinClassLevels","title":"Join some class existing levels to new, larger class levels for classification problems. — joinClassLevels","text":"Join class existing levels new, larger class levels classification problems.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/joinClassLevels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join some class existing levels to new, larger class levels for classification problems. — joinClassLevels","text":"","code":"joinClassLevels(task, new.levels)"},{"path":"https://mlr.mlr-org.com/dev/reference/joinClassLevels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join some class existing levels to new, larger class levels for classification problems. — joinClassLevels","text":"task (Task) task. new.levels (list character) Element names specify new class levels create, corresponding element character vector specifies existing class levels joined new one.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/joinClassLevels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join some class existing levels to new, larger class levels for classification problems. — joinClassLevels","text":"Task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/joinClassLevels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Join some class existing levels to new, larger class levels for classification problems. — joinClassLevels","text":"","code":"joinClassLevels(iris.task, new.levels = list(foo = c(\"setosa\", \"virginica\"))) #> Supervised task: iris-example #> Type: classif #> Target: Species #> Observations: 150 #> Features: #>    numerics     factors     ordered functionals  #>           4           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 2 #>        foo versicolor  #>        100         50  #> Positive class: foo"},{"path":"https://mlr.mlr-org.com/dev/reference/learnerArgsToControl.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert arguments to control structure. — learnerArgsToControl","title":"Convert arguments to control structure. — learnerArgsToControl","text":"Find elements ... missing call control .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/learnerArgsToControl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert arguments to control structure. — learnerArgsToControl","text":"","code":"learnerArgsToControl(control, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/learnerArgsToControl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert arguments to control structure. — learnerArgsToControl","text":"control (function) Function creates control structure. ... () Arguments control structure function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/learnerArgsToControl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert arguments to control structure. — learnerArgsToControl","text":"Control structure learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/learners.html","id":null,"dir":"Reference","previous_headings":"","what":"List of supported learning algorithms. — learners","title":"List of supported learning algorithms. — learners","text":"supported learners can found listLearners table tutorial appendix: https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterEnsembleMethods.html","id":null,"dir":"Reference","previous_headings":"","what":"List ensemble filter methods. — listFilterEnsembleMethods","title":"List ensemble filter methods. — listFilterEnsembleMethods","text":"Returns subset-able dataframe filter information.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterEnsembleMethods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List ensemble filter methods. — listFilterEnsembleMethods","text":"","code":"listFilterEnsembleMethods(desc = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterEnsembleMethods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List ensemble filter methods. — listFilterEnsembleMethods","text":"desc (logical(1)) Provide detailed information filters. Default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterEnsembleMethods.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List ensemble filter methods. — listFilterEnsembleMethods","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterMethods.html","id":null,"dir":"Reference","previous_headings":"","what":"List filter methods. — listFilterMethods","title":"List filter methods. — listFilterMethods","text":"Returns subset-able dataframe filter information.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterMethods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List filter methods. — listFilterMethods","text":"","code":"listFilterMethods(   desc = TRUE,   tasks = FALSE,   features = FALSE,   include.deprecated = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterMethods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List filter methods. — listFilterMethods","text":"desc (logical(1)) Provide detailed information filters. Default TRUE. tasks (logical(1)) Provide information supported tasks. Default FALSE. features (logical(1)) Provide information supported features. Default FALSE. include.deprecated (logical(1)) deprecated filter methods included list. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listFilterMethods.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List filter methods. — listFilterMethods","text":"(data.frame).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/listLearnerProperties.html","id":null,"dir":"Reference","previous_headings":"","what":"List the supported learner properties — listLearnerProperties","title":"List the supported learner properties — listLearnerProperties","text":"useful determining learner properties available.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listLearnerProperties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List the supported learner properties — listLearnerProperties","text":"","code":"listLearnerProperties(type = \"any\")"},{"path":"https://mlr.mlr-org.com/dev/reference/listLearnerProperties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List the supported learner properties — listLearnerProperties","text":"type (character(1)) return properties specified task type. Default “”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listLearnerProperties.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List the supported learner properties — listLearnerProperties","text":"(character).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listLearners.html","id":null,"dir":"Reference","previous_headings":"","what":"Find matching learning algorithms. — listLearners","title":"Find matching learning algorithms. — listLearners","text":"Returns learning algorithms specific characteristics, e.g. whether support missing values, case weights, etc. Note packages learners loaded search create . can lot. create inspect properties S3 classes. lot faster. Note general cost-sensitive learning, mlr currently supports mainly “wrapper” approaches like CostSensWeightedPairsWrapper, listed, basic R learning algorithms. applies many multilabel methods, see, e.g., makeMultilabelBinaryRelevanceWrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listLearners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find matching learning algorithms. — listLearners","text":"","code":"listLearners(   obj = NA_character_,   properties = character(0L),   quiet = TRUE,   warn.missing.packages = TRUE,   check.packages = FALSE,   create = FALSE )  # S3 method for default listLearners(   obj = NA_character_,   properties = character(0L),   quiet = TRUE,   warn.missing.packages = TRUE,   check.packages = FALSE,   create = FALSE )  # S3 method for character listLearners(   obj = NA_character_,   properties = character(0L),   quiet = TRUE,   warn.missing.packages = TRUE,   check.packages = FALSE,   create = FALSE )  # S3 method for Task listLearners(   obj = NA_character_,   properties = character(0L),   quiet = TRUE,   warn.missing.packages = TRUE,   check.packages = TRUE,   create = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/listLearners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find matching learning algorithms. — listLearners","text":"obj (character(1) | Task) Either character(1) task type task, latter case one : “classif” “regr” “surv” “costsens” “cluster” “multilabel”. Default NA matching types. properties (character) Set required properties filter . Default character(0). quiet (logical(1)) Construct learners quietly check properties, shows package startup messages. Turn suspect errors. Default TRUE. warn.missing.packages (logical(1)) learner constructed package missing, warning shown? Default TRUE. check.packages (logical(1)) Check required packages installed. Calls find.package(). create TRUE, done implicitly value parameter ignored. create FALSE check.packages TRUE returned table contains learners whose dependencies installed. check.packages set FALSE, learners actually constructed missing packages may returned. Default FALSE. create (logical(1)) Instantiate objects (return info table)? Packages loaded option TRUE. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listLearners.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find matching learning algorithms. — listLearners","text":"([data.frame|list` Learner). Either descriptive data.frame allows access properties learners list created learner objects (named ids listed learners).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listLearners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find matching learning algorithms. — listLearners","text":"","code":"if (FALSE) { listLearners(\"classif\", properties = c(\"multiclass\", \"prob\")) data = iris task = makeClassifTask(data = data, target = \"Species\") listLearners(task) }"},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasureProperties.html","id":null,"dir":"Reference","previous_headings":"","what":"List the supported measure properties. — listMeasureProperties","title":"List the supported measure properties. — listMeasureProperties","text":"useful determining measure properties available.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasureProperties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List the supported measure properties. — listMeasureProperties","text":"","code":"listMeasureProperties()"},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasureProperties.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List the supported measure properties. — listMeasureProperties","text":"(character).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasures.html","id":null,"dir":"Reference","previous_headings":"","what":"Find matching measures. — listMeasures","title":"Find matching measures. — listMeasures","text":"Returns matching measures specific characteristics, e.g. whether supports classification regression.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find matching measures. — listMeasures","text":"","code":"listMeasures(obj, properties = character(0L), create = FALSE)  # S3 method for default listMeasures(obj, properties = character(0L), create = FALSE)  # S3 method for character listMeasures(obj, properties = character(0L), create = FALSE)  # S3 method for Task listMeasures(obj, properties = character(0L), create = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find matching measures. — listMeasures","text":"obj (character(1) | Task) Either character(1) task type task, latter case one : “classif” “regr” “surv” “costsens” “cluster” “multilabel”. Default NA matching types. properties (character) Set required properties filter . See Measure standardized properties. Default character(0). create (logical(1)) Instantiate objects (return strings)? Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listMeasures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find matching measures. — listMeasures","text":"([character|list` Measure). Class names matching measures instantiated objects.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listTaskTypes.html","id":null,"dir":"Reference","previous_headings":"","what":"List the supported task types in mlr — listTaskTypes","title":"List the supported task types in mlr — listTaskTypes","text":"Returns character vector supported task types mlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/listTaskTypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List the supported task types in mlr — listTaskTypes","text":"","code":"listTaskTypes()"},{"path":"https://mlr.mlr-org.com/dev/reference/listTaskTypes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List the supported task types in mlr — listTaskTypes","text":"(character).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/lung.task.html","id":null,"dir":"Reference","previous_headings":"","what":"NCCTG Lung Cancer survival task. — lung.task","title":"NCCTG Lung Cancer survival task. — lung.task","text":"Contains task (lung.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/lung.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"NCCTG Lung Cancer survival task. — lung.task","text":"See survival::lung. Incomplete cases removed task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeAggregation.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify your own aggregation of measures. — makeAggregation","title":"Specify your own aggregation of measures. — makeAggregation","text":"advanced feature mlr. gives access inner workings result might compatible everything!","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeAggregation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify your own aggregation of measures. — makeAggregation","text":"","code":"makeAggregation(id, name = id, properties, fun)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeAggregation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify your own aggregation of measures. — makeAggregation","text":"id (character(1)) Name aggregation method (preferably name generated function). name (character(1)) Long name aggregation method. Default id. properties (character) Set aggregation properties. req.train prediction train sets required calculate aggregation? req.test prediction test sets required calculate aggregation? fun (function(task, perf.test, perf.train, measure, group, pred)) Calculates aggregated performance. cases need performances perf.test optionally perf.train test training data sets. task (Task) task. perf.test (numeric) performance results test data sets. perf.train (numeric) performance results training data sets. measure (Measure) Performance measure. group (factor) Grouping resampling iterations. encodes whether specific iterations 'belong together' (e.g. repeated CV). pred (Prediction) Prediction object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeAggregation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify your own aggregation of measures. — makeAggregation","text":"(Aggregation).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeAggregation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specify your own aggregation of measures. — makeAggregation","text":"","code":"# computes the interquartile range on all performance values test.iqr = makeAggregation(   id = \"test.iqr\", name = \"Test set interquartile range\",   properties = \"req.test\",   fun = function(task, perf.test, perf.train, measure, group, pred) IQR(perf.test) )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaggingWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with the bagging technique. — makeBaggingWrapper","title":"Fuse learner with the bagging technique. — makeBaggingWrapper","text":"Fuses learner bagging method (.e., similar randomForest ). Creates learner object, can used like learner object. Models can easily accessed via getLearnerModel. Bagging implemented follows: iteration random data subset sampled (without replacement) potentially number features also restricted random subset. Note usually handled slightly different way random forest features sampled tree split). Prediction works follows: classification majority voting create discrete label probabilities predicted considering proportions predicted labels. regression mean value standard deviations across predictions computed. Note passed base learner must always predict.type = 'response', BaggingWrapper can estimate probabilities standard errors, can set, e.g., predict.type = 'prob'. reason, call setPredictType, type set BaggingWrapper, passed inner learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaggingWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with the bagging technique. — makeBaggingWrapper","text":"","code":"makeBaggingWrapper(   learner,   bw.iters = 10L,   bw.replace = TRUE,   bw.size,   bw.feats = 1 )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaggingWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with the bagging technique. — makeBaggingWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. bw.iters (integer(1)) Iterations = number fitted models bagging. Default 10. bw.replace (logical(1)) Sample bags replacement (bootstrapping)? Default TRUE. bw.size (numeric(1)) Percentage size sampled bags. Default 1 bootstrapping 0.632 subsampling. bw.feats (numeric(1)) Percentage size randomly selected features bags. Default 1. least one feature always selected.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaggingWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with the bagging technique. — makeBaggingWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaseWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Exported for internal use only. — makeBaseWrapper","title":"Exported for internal use only. — makeBaseWrapper","text":"Exported internal use .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaseWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exported for internal use only. — makeBaseWrapper","text":"","code":"makeBaseWrapper(   id,   type,   next.learner,   package = character(0L),   par.set = makeParamSet(),   par.vals = list(),   learner.subclass,   model.subclass,   cache = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeBaseWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exported for internal use only. — makeBaseWrapper","text":"id (character(1)) Id string object. Used display object. type (character(1)) Learner type. next.learner (Learner) Learner wrap. package (character) Packages load loading learner. par.set (ParamSet) Parameter set. par.vals (list) Optional list named (hyper)parameter values. learner.subclass (character) Class assign new object. model.subclass (character) Class assign learner models.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeChainModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Only exported for internal use. — makeChainModel","title":"Only exported for internal use. — makeChainModel","text":"exported internal use.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeChainModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Only exported for internal use. — makeChainModel","text":"","code":"makeChainModel(next.model, cl)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeChainModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Only exported for internal use. — makeChainModel","text":"next.model (WrappedModel) next model. cl (character) Subclass assign resulting model.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeClassificationViaRegressionWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification via regression wrapper. — makeClassificationViaRegressionWrapper","title":"Classification via regression wrapper. — makeClassificationViaRegressionWrapper","text":"Builds regression models predict positive class whether particular example belongs (1) (-1). Probabilities generated transforming predictions softmax. Inspired WEKA's ClassificationViaRegression (http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/ClassificationViaRegression.html).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeClassificationViaRegressionWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification via regression wrapper. — makeClassificationViaRegressionWrapper","text":"","code":"makeClassificationViaRegressionWrapper(learner, predict.type = \"response\")"},{"path":"https://mlr.mlr-org.com/dev/reference/makeClassificationViaRegressionWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification via regression wrapper. — makeClassificationViaRegressionWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. predict.type (character(1)) “response” (= labels) “prob” (= probabilities labels selecting one maximal probability).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeClassificationViaRegressionWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classification via regression wrapper. — makeClassificationViaRegressionWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeClassificationViaRegressionWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classification via regression wrapper. — makeClassificationViaRegressionWrapper","text":"","code":"lrn = makeLearner(\"regr.rpart\") lrn = makeClassificationViaRegressionWrapper(lrn) mod = train(lrn, sonar.task, subset = 1:140) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions predictions = predict(mod, newdata = getTaskData(sonar.task)[141:208, 1:60]) #> Error in predict(mod, newdata = getTaskData(sonar.task)[141:208, 1:60]): object 'mod' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/makeConstantClassWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Wraps a classification learner to support problems where the class label is (almost) constant. — makeConstantClassWrapper","title":"Wraps a classification learner to support problems where the class label is (almost) constant. — makeConstantClassWrapper","text":"training data contains single class (almost single class), wrapper creates model always predicts constant class training data. cases, underlying learner trained resulting model used predictions. Probabilities can predicted 1 0 depending whether label matches majority class .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeConstantClassWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wraps a classification learner to support problems where the class label is (almost) constant. — makeConstantClassWrapper","text":"","code":"makeConstantClassWrapper(learner, frac = 0)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeConstantClassWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wraps a classification learner to support problems where the class label is (almost) constant. — makeConstantClassWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. frac numeric(1) fraction labels [0, 1) can different majority label. Default 0, means constant labels predicted exactly one label data.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeConstantClassWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wraps a classification learner to support problems where the class label is (almost) constant. — makeConstantClassWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostMeasure.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a measure for non-standard misclassification costs. — makeCostMeasure","title":"Creates a measure for non-standard misclassification costs. — makeCostMeasure","text":"Creates cost measure non-standard classification error costs.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostMeasure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a measure for non-standard misclassification costs. — makeCostMeasure","text":"","code":"makeCostMeasure(   id = \"costs\",   minimize = TRUE,   costs,   combine = mean,   best = NULL,   worst = NULL,   name = id,   note = \"\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostMeasure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a measure for non-standard misclassification costs. — makeCostMeasure","text":"id (character(1)) Name measure. Default “costs”. minimize (logical(1)) measure minimized? Otherwise effectively specifying benefits matrix. Default TRUE. costs (matrix) Matrix misclassification costs. Rows columns named class labels, order matter. Rows indicate true classes, columns predicted classes. combine (function) combine costs cases SINGLE test set? Note aggregate argument makeMeasure can set well via setAggregation, measure. Default mean. best (numeric(1)) Best obtainable value measure. Default -Inf Inf, depending minimize. worst (numeric(1)) Worst obtainable value measure. Default Inf -Inf, depending minimize. name (character)  Name measure. Default id. note (character)  Description additional notes measure. Default “”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostMeasure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates a measure for non-standard misclassification costs. — makeCostMeasure","text":"Measure.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensClassifWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Wraps a classification learner for use in cost-sensitive learning. — makeCostSensClassifWrapper","title":"Wraps a classification learner for use in cost-sensitive learning. — makeCostSensClassifWrapper","text":"Creates wrapper, can used like learner object. classification model can easily accessed via getLearnerModel. naive learner, costs transformed classification labels - label case name class minimal costs. (ties occur, label better average w.r.t. costs training data preferred.) classifier fitted data subsequently used prediction.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensClassifWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wraps a classification learner for use in cost-sensitive learning. — makeCostSensClassifWrapper","text":"","code":"makeCostSensClassifWrapper(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensClassifWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wraps a classification learner for use in cost-sensitive learning. — makeCostSensClassifWrapper","text":"learner (Learner | character(1)) classification learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensClassifWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wraps a classification learner for use in cost-sensitive learning. — makeCostSensClassifWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensRegrWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Wraps a regression learner for use in cost-sensitive learning. — makeCostSensRegrWrapper","title":"Wraps a regression learner for use in cost-sensitive learning. — makeCostSensRegrWrapper","text":"Creates wrapper, can used like learner object. Models can easily accessed via getLearnerModel. class task, individual regression model fitted costs class. prediction, class lowest predicted costs selected.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensRegrWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wraps a regression learner for use in cost-sensitive learning. — makeCostSensRegrWrapper","text":"","code":"makeCostSensRegrWrapper(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensRegrWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wraps a regression learner for use in cost-sensitive learning. — makeCostSensRegrWrapper","text":"learner (Learner | character(1)) regression learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensRegrWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wraps a regression learner for use in cost-sensitive learning. — makeCostSensRegrWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensWeightedPairsWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Wraps a classifier for cost-sensitive learning to produce a weighted pairs model. — makeCostSensWeightedPairsWrapper","title":"Wraps a classifier for cost-sensitive learning to produce a weighted pairs model. — makeCostSensWeightedPairsWrapper","text":"Creates wrapper, can used like learner object. Models can easily accessed via getLearnerModel. pair labels, fit binary classifier. observation define label element pair minimal costs. fitting, also weight observation absolute difference costs. Prediction performed simple voting. approach sometimes called cost-sensitive one-vs-one (CS-OVO), obviously similar one-vs-one approach one reduces normal multi-class problem multiple binary ones aggregates voting.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensWeightedPairsWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wraps a classifier for cost-sensitive learning to produce a weighted pairs model. — makeCostSensWeightedPairsWrapper","text":"","code":"makeCostSensWeightedPairsWrapper(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensWeightedPairsWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wraps a classifier for cost-sensitive learning to produce a weighted pairs model. — makeCostSensWeightedPairsWrapper","text":"learner (Learner | character(1)) classification learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensWeightedPairsWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wraps a classifier for cost-sensitive learning to produce a weighted pairs model. — makeCostSensWeightedPairsWrapper","text":"(Learner).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCostSensWeightedPairsWrapper.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wraps a classifier for cost-sensitive learning to produce a weighted pairs model. — makeCostSensWeightedPairsWrapper","text":"Lin, HT.: Reduction Cost-sensitive Multiclass Classification One-versus-one Binary Classification. : Proceedings Sixth Asian Conference Machine Learning. JMLR Workshop Conference Proceedings, vol 39, pp. 371-386. JMLR W&CP (2014). https://www.jmlr.org/proceedings/papers/v39/lin14.pdf","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeCustomResampledMeasure.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct your own resampled performance measure. — makeCustomResampledMeasure","title":"Construct your own resampled performance measure. — makeCustomResampledMeasure","text":"Construct performance measure, used resampling. Note individual training / test set performance values set NA, calculate aggregated value. can define function makes sense every single training / test set, implement Measure.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCustomResampledMeasure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct your own resampled performance measure. — makeCustomResampledMeasure","text":"","code":"makeCustomResampledMeasure(   measure.id,   aggregation.id,   minimize = TRUE,   properties = character(0L),   fun,   extra.args = list(),   best = NULL,   worst = NULL,   measure.name = measure.id,   aggregation.name = aggregation.id,   note = \"\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeCustomResampledMeasure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct your own resampled performance measure. — makeCustomResampledMeasure","text":"measure.id (character(1)) Short name measure. aggregation.id (character(1)) Short name aggregation. minimize (logical(1)) measure minimized? Default TRUE. properties (character) Set measure properties. list values see Measure. Default character(0). fun (function(task, group, pred, extra.args)) Calculates performance value ResamplePrediction object. rare cases can also use task, grouping extra arguments extra.args. - task (Task) task. - group (factor) Grouping resampling iterations. encodes whether specific iterations 'belong together' (e.g. repeated CV). - pred (Prediction) Prediction object. - extra.args (list) See . extra.args (list) List extra arguments always passed fun. Default empty list. best (numeric(1)) Best obtainable value measure. Default -Inf Inf, depending minimize. worst (numeric(1)) Worst obtainable value measure. Default Inf -Inf, depending minimize. measure.name (character(1)) Long name measure. Default measure.id. aggregation.name (character(1)) Long name aggregation. Default aggregation.id. note (character)  Description additional notes measure. Default “”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeCustomResampledMeasure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct your own resampled performance measure. — makeCustomResampledMeasure","text":"Measure.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeDownsampleWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with simple downsampling (subsampling). — makeDownsampleWrapper","title":"Fuse learner with simple downsampling (subsampling). — makeDownsampleWrapper","text":"Creates learner object, can used like learner object. trained subset original data save computational time.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeDownsampleWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with simple downsampling (subsampling). — makeDownsampleWrapper","text":"","code":"makeDownsampleWrapper(learner, dw.perc = 1, dw.stratify = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeDownsampleWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with simple downsampling (subsampling). — makeDownsampleWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. dw.perc (numeric(1)) See downsample. Default 1. dw.stratify (logical(1)) See downsample. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeDownsampleWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with simple downsampling (subsampling). — makeDownsampleWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeDummyFeaturesWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with dummy feature creator. — makeDummyFeaturesWrapper","title":"Fuse learner with dummy feature creator. — makeDummyFeaturesWrapper","text":"Fuses base learner dummy feature creator (see createDummyFeatures). Returns learner can used like learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeDummyFeaturesWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with dummy feature creator. — makeDummyFeaturesWrapper","text":"","code":"makeDummyFeaturesWrapper(learner, method = \"1-of-n\", cols = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeDummyFeaturesWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with dummy feature creator. — makeDummyFeaturesWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. method (character(1)) Available : \"1--n\": n factor levels n dummy variables. \"reference\": n-1 dummy variables leaving first factor level variable. Default “1--n”. cols (character) Columns create dummy features . Default use columns.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeDummyFeaturesWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with dummy feature creator. — makeDummyFeaturesWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatMethod.html","id":null,"dir":"Reference","previous_headings":"","what":"Constructor for FDA feature extraction methods. — makeExtractFDAFeatMethod","title":"Constructor for FDA feature extraction methods. — makeExtractFDAFeatMethod","text":"can used implement custom FDA feature extraction. Takes learn reextract function along optional parameters argument.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatMethod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constructor for FDA feature extraction methods. — makeExtractFDAFeatMethod","text":"","code":"makeExtractFDAFeatMethod(learn, reextract, args = list(), par.set = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatMethod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constructor for FDA feature extraction methods. — makeExtractFDAFeatMethod","text":"learn (function(data, target, col, ...)) Function learn extract information functional column col. Arguments : data data.frame Data.frame containing matricies one row per observation single functional time series one column per meahttps://github.com/mlr-org/mlr/pull/2005/conflict?name=R%252FextractFDAFeatures.R&ancestor_oid=bdc5d882cc86adac456842bebf1a2cf9bb0eb648&base_oid=55d472e23f5c3eb8099607bd9f539034d93e82a4&head_oid=4076800589c60b20acc926e5a545df9f73193b65surement time point. entries need numeric. target (character(1)) Name target variable. Default: “NULL”. variable set consistent API. col (character(1) | numeric(1)) column names indices, extraction performed . function return named list values. reextract (function(data, target, col, ...)) Function used reextracting data predict phase. Can equal learn. args (list) Named list arguments pass learn via .... par.set (ParamSet) Paramset added learner used conjunction makeExtractFDAFeatsWrapper. Can NULL.`","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatsWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with an extractFDAFeatures method. — makeExtractFDAFeatsWrapper","title":"Fuse learner with an extractFDAFeatures method. — makeExtractFDAFeatsWrapper","text":"Fuses base learner extractFDAFeatures method. Creates learner object, can used like learner object. Internally uses extractFDAFeatures training learner reextractFDAFeatures predicting.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatsWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with an extractFDAFeatures method. — makeExtractFDAFeatsWrapper","text":"","code":"makeExtractFDAFeatsWrapper(learner, feat.methods = list())"},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatsWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with an extractFDAFeatures method. — makeExtractFDAFeatsWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. feat.methods (named list) List functional features along desired methods functional feature. “” applies extractFDAFeatures method functional feature. Names feat.methods must match column names functional features. Available feature extraction methods available family fda_featextractor. Specifying functional feature multiple times different extraction methods allows extraction different features functional. Default list() nothing.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeExtractFDAFeatsWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with an extractFDAFeatures method. — makeExtractFDAFeatsWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeFeatSelWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with feature selection. — makeFeatSelWrapper","title":"Fuse learner with feature selection. — makeFeatSelWrapper","text":"Fuses base learner search strategy select variables. Creates learner object, can used like learner object, internally uses selectFeatures. train function called , search strategy resampling invoked select optimal set variables. Finally, model fitted complete training data variables returned. See selectFeatures details. training, optimal features (related information) can retrieved getFeatSelResult.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFeatSelWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with feature selection. — makeFeatSelWrapper","text":"","code":"makeFeatSelWrapper(   learner,   resampling,   measures,   bit.names,   bits.to.features,   control,   show.info = getMlrOption(\"show.info\") )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFeatSelWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with feature selection. — makeFeatSelWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. resampling (ResampleInstance | ResampleDesc) Resampling strategy feature selection. pass description, instantiated beginning default, points evaluated training/test sets. want change behavior, look FeatSelControl. measures (list Measure | Measure) Performance measures evaluate. first measure, aggregated first aggregation function optimized, others simply evaluated. Default default measure task, see getDefaultMeasure. bit.names character Names bits encoding solutions. Also defines total number bits encoding. Per default feature names task. used together bits..features. bits..features (function(x, task)) Function transforms integer-0-1 vector character vector selected features. Per default value 1 ith bit selects ith feature candidate solution. vector x correspond bit.names length. control [see FeatSelControl) Control object search method. Also selects optimization algorithm feature selection. show.info (logical(1)) Print verbose output console? Default set via configureMlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFeatSelWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with feature selection. — makeFeatSelWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeFeatSelWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fuse learner with feature selection. — makeFeatSelWrapper","text":"","code":"# nested resampling with feature selection (with a nonsense algorithm for selection) outer = makeResampleDesc(\"CV\", iters = 2L) inner = makeResampleDesc(\"Holdout\") ctrl = makeFeatSelControlRandom(maxit = 1) lrn = makeFeatSelWrapper(\"classif.ksvm\", resampling = inner, control = ctrl) # we also extract the selected features for all iteration here r = resample(lrn, iris.task, outer, extract = getFeatSelResult) #> Resampling: cross-validation #> Measures:             mmce       #> [FeatSel] Started selecting features for learner 'classif.ksvm' #> With control class: FeatSelControlRandom #> Imputation value: 1 #> [FeatSel-x] 1: 0101 (2 bits) #> [FeatSel-y] 1: mmce.test.mean=0.0800000; time: 0.0 min #> Warning: one argument not used by format '[FeatSel] Result: %s (%i bits)' #> [FeatSel] Result: Sepal.Width,Petal.Width (2 bits) #> [Resample] iter 1:    0.0666667  #> [FeatSel] Started selecting features for learner 'classif.ksvm' #> With control class: FeatSelControlRandom #> Imputation value: 1 #> [FeatSel-x] 1: 0010 (1 bits) #> [FeatSel-y] 1: mmce.test.mean=0.0400000; time: 0.0 min #> Warning: one argument not used by format '[FeatSel] Result: %s (%i bits)' #> [FeatSel] Result: Petal.Length (1 bits) #> [Resample] iter 2:    0.0266667  #>  #> Aggregated Result: mmce.test.mean=0.0466667 #>"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilter.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a feature filter. — makeFilter","title":"Create a feature filter. — makeFilter","text":"Creates registers custom feature filters. Implemented filters can listed listFilterMethods. Additional documentation fun parameter specific filter can found description.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a feature filter. — makeFilter","text":"","code":"makeFilter(name, desc, pkg, supported.tasks, supported.features, fun)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a feature filter. — makeFilter","text":"name (character(1)) Identifier filter. desc (character(1)) Short description filter. pkg (character(1)) Source package filter implemented. supported.tasks (character) Task types supported. supported.features (character) Feature types supported. fun (function(task, nselect, ...) Function takes task returns named numeric vector scores, one score feature task. Higher scores mean higher importance feature. least nselect features must calculated, remaining may set NA omitted, thus selected. original order restored necessary.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a feature filter. — makeFilter","text":"Object class “Filter”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilter.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a feature filter. — makeFilter","text":"Kira, Kenji Rendell, Larry (1992). Feature Selection Problem: Traditional Methods New Algorithm. AAAI-92 Proceedings. Kononenko, Igor et al. Overcoming myopia inductive learning algorithms RELIEFF (1997), Applied Intelligence, 7(1), p39-55.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterEnsemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ensemble feature filter. — makeFilterEnsemble","title":"Create an ensemble feature filter. — makeFilterEnsemble","text":"Creates registers custom ensemble feature filters. Implemented ensemble filters can listed listFilterEnsembleMethods. Additional documentation fun parameter specific filter can found description.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterEnsemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ensemble feature filter. — makeFilterEnsemble","text":"","code":"makeFilterEnsemble(name, base.methods, desc, fun)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterEnsemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ensemble feature filter. — makeFilterEnsemble","text":"name (character(1)) Identifier filter. base.methods base filter methods ensemble method use. desc (character(1)) Short description filter. fun (function(task, nselect, ...) Function takes task returns named numeric vector scores, one score feature task. Higher scores mean higher importance feature. least nselect features must calculated, remaining may set NA omitted, thus selected. original order restored necessary.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterEnsemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ensemble feature filter. — makeFilterEnsemble","text":"Object class “FilterEnsemble”.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with a feature filter method. — makeFilterWrapper","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"Fuses base learner filter method. Creates learner object, can used like learner object. Internally uses filterFeatures every model fit.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"","code":"makeFilterWrapper(   learner,   fw.method = \"randomForestSRC_importance\",   fw.base.methods = NULL,   fw.perc = NULL,   fw.abs = NULL,   fw.threshold = NULL,   fw.fun = NULL,   fw.fun.args = NULL,   fw.mandatory.feat = NULL,   cache = FALSE,   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. fw.method (character(1)) Filter method. See listFilterMethods. Default “randomForestSRC_importance”. fw.base.methods (character(1)) Simple Filter methods ensemble filters. See listFilterMethods. Can used combination ensemble filters. See listFilterEnsembleMethods. fw.perc (numeric(1)) set, select fw.perc*100 top scoring features. Mutually exclusive arguments fw.abs, fw.threshold `fw.fun. fw.abs (numeric(1)) set, select fw.abs top scoring features. Mutually exclusive arguments fw.perc, fw.threshold fw.fun. fw.threshold (numeric(1)) set, select features whose score exceeds fw.threshold. Mutually exclusive arguments fw.perc, fw.abs fw.fun. fw.fun (function)) set, select features via custom thresholding function, must return number top scoring features select. Mutually exclusive arguments fw.perc, fw.abs fw.threshold. fw.fun.args () Arguments passed custom thresholding function fw.mandatory.feat (character) Mandatory features always included regardless scores cache (character(1) | logical) Whether use caching filter value creation. See details. ... () Additional parameters passed filter. using one filter method, need pass arguments named list via .args. example .args = list(\"FSelectorRcpp_information.gain\" = list(equal = TRUE)).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"ensemble = TRUE, ensemble feature selection using methods specified fw.method performed. least two methods need selected. training, selected features can retrieved getFilteredFeatures. Note observation weights influence filtering simply passed next learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":"caching","dir":"Reference","previous_headings":"","what":"Caching","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"cache = TRUE, default mlr cache directory used cache filter values. directory operating system dependent can checked getCacheDir(). Alternatively custom directory can passed store cache. cache can cleared deleteCacheDir(). Caching disabled default. Care taken operating large clusters due possible write conflicts disk multiple workers try write cache time.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeFilterWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fuse learner with a feature filter method. — makeFilterWrapper","text":"","code":"# \\donttest{ task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.lda\") inner = makeResampleDesc(\"Holdout\") outer = makeResampleDesc(\"CV\", iters = 2) lrn = makeFilterWrapper(lrn, fw.perc = 0.5) mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions print(getFilteredFeatures(mod)) #> Error in getFilteredFeatures(mod): object 'mod' not found # now nested resampling, where we extract the features that the filter method selected r = resample(lrn, task, outer, extract = function(model) {   getFilteredFeatures(model) }) #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.0266667  #> [Resample] iter 2:    0.0533333  #>  #> Aggregated Result: mmce.test.mean=0.0400000 #>  print(r$extract) #> [[1]] #> [1] \"Petal.Length\" \"Petal.Width\"  #>  #> [[2]] #> [1] \"Petal.Length\" \"Petal.Width\"  #>   # usage of an ensemble filter lrn = makeLearner(\"classif.lda\") lrn = makeFilterWrapper(lrn, fw.method = \"E-Borda\",   fw.base.methods = c(\"FSelectorRcpp_gain.ratio\", \"FSelectorRcpp_information.gain\"),   fw.perc = 0.5) r = resample(lrn, task, outer, extract = function(model) {   getFilteredFeatures(model) }) #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.0666667  #> [Resample] iter 2:    0.0266667  #>  #> Aggregated Result: mmce.test.mean=0.0466667 #>  print(r$extract) #> [[1]] #> [1] \"Petal.Length\" \"Petal.Width\"  #>  #> [[2]] #> [1] \"Petal.Length\" \"Petal.Width\"  #>   # usage of a custom thresholding function biggest_gap = function(values, diff) {   gap_size = 0   gap_location = 0    for (i in (diff + 1):length(values)) {     gap = values[[i - diff]] - values[[i]]     if (gap > gap_size) {       gap_size = gap       gap_location = i - 1     }   }   return(gap_location) }  lrn = makeLearner(\"classif.lda\") lrn = makeFilterWrapper(lrn, fw.method = \"randomForestSRC_importance\",   fw.fun = biggest_gap, fw.fun.args = list(\"diff\" = 1)) r = resample(lrn, task, outer, extract = function(model) {   getFilteredFeatures(model) }) #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.0533333  #> [Resample] iter 2:    0.0400000  #>  #> Aggregated Result: mmce.test.mean=0.0466667 #>  print(r$extract) #> [[1]] #> [1] \"Petal.Length\" \"Petal.Width\"  #>  #> [[2]] #> [1] \"Petal.Length\" \"Petal.Width\"  #>  # }"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFixedHoldoutInstance.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a fixed holdout instance for resampling. — makeFixedHoldoutInstance","title":"Generate a fixed holdout instance for resampling. — makeFixedHoldoutInstance","text":"Generate fixed holdout instance resampling.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFixedHoldoutInstance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a fixed holdout instance for resampling. — makeFixedHoldoutInstance","text":"","code":"makeFixedHoldoutInstance(train.inds, test.inds, size)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFixedHoldoutInstance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a fixed holdout instance for resampling. — makeFixedHoldoutInstance","text":"train.inds (integer) Indices training set. test.inds (integer) Indices test set. size (integer(1)) Size data set resample. function needs know largest possible index whole data set.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFixedHoldoutInstance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a fixed holdout instance for resampling. — makeFixedHoldoutInstance","text":"(ResampleInstance).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFunctionalData.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a data.frame containing functional features from a normal data.frame. — makeFunctionalData","title":"Create a data.frame containing functional features from a normal data.frame. — makeFunctionalData","text":"work functional features, features need stored matrix column data.frame, mlr can automatically recognize functional features. function allows easy conversion data.frame numeric columns required format. data already contains matrix columns, left -specified otherwise fd.features. See Examples structure generated output.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFunctionalData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a data.frame containing functional features from a normal data.frame. — makeFunctionalData","text":"","code":"makeFunctionalData(data, fd.features = NULL, exclude.cols = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeFunctionalData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a data.frame containing functional features from a normal data.frame. — makeFunctionalData","text":"data (data.frame)  data.frame contains functional features numeric columns. fd.features (list)  Named list containing integer column indices character column names. element defines functional feature, given order indices column names. name list element defines name functional feature. selected columns correspond numeric data.frame entries. default NULL, means numeric features considered single functional “fd1”. exclude.cols (character | integer) Column names indices exclude conversion functionals, even included fd.features. Default exclude anything.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFunctionalData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a data.frame containing functional features from a normal data.frame. — makeFunctionalData","text":"(data.frame).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeFunctionalData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a data.frame containing functional features from a normal data.frame. — makeFunctionalData","text":"","code":"# data.frame where columns 1:6 and 8:10 belong to a functional feature d1 = data.frame(matrix(rnorm(100), nrow = 10), \"target\" = seq_len(10)) # Transform to functional data d2 = makeFunctionalData(d1, fd.features = list(\"fd1\" = 1:6, \"fd2\" = 8:10)) # Create a regression task makeRegrTask(data = d2, target = \"target\") #> Supervised task: d2 #> Type: regr #> Target: target #> Observations: 10 #> Features: #>    numerics     factors     ordered functionals  #>           1           0           0           2  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE"},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeMethod.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a custom imputation method. — makeImputeMethod","title":"Create a custom imputation method. — makeImputeMethod","text":"constructor create imputation methods.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeMethod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a custom imputation method. — makeImputeMethod","text":"","code":"makeImputeMethod(learn, impute, args = list())"},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeMethod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a custom imputation method. — makeImputeMethod","text":"learn (function(data, target, col, ...)) Function learn extract information column col data frame data. Argument target specifies target column learning task. function return named list values. impute (function(data, target, col, ...)) Function impute missing values col using information returned learn column. list elements return values o learn passed function .... args (list) Named list arguments pass learn via ....","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with an imputation method. — makeImputeWrapper","title":"Fuse learner with an imputation method. — makeImputeWrapper","text":"Fuses base learner imputation method. Creates learner object, can used like learner object. Internally uses impute training learner reimpute predicting.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with an imputation method. — makeImputeWrapper","text":"","code":"makeImputeWrapper(   learner,   classes = list(),   cols = list(),   dummy.classes = character(0L),   dummy.cols = character(0L),   dummy.type = \"factor\",   force.dummies = FALSE,   impute.new.levels = TRUE,   recode.factor.levels = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with an imputation method. — makeImputeWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. classes (named list) Named list containing imputation techniques classes columns. E.g. list(numeric = imputeMedian()). cols (named list) Named list containing names imputation methods impute missing values data column referenced list element's name. Overrules imputation set via classes. dummy.classes (character) Classes columns create dummy columns . Default character(0). dummy.cols (character) Column names create dummy columns (containing binary missing indicator) . Default character(0). dummy.type (character(1)) dummy columns encoded. Either 0/1 type “numeric” “factor”. Default “factor”. force.dummies (logical(1)) Force dummy creation even respective data column contain NAs. Note () learners complain constant columns created way (b) feature set might stochastic turn . Default FALSE. impute.new.levels (logical(1)) new, unencountered factor level occur reimputation, handled NAs imputed way? Default TRUE. recode.factor.levels (logical(1)) Recode factor levels reimputation, match respective element lvls (description object) therefore match levels feature factor training data imputation?. Default TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeImputeWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with an imputation method. — makeImputeWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Create learner object. — makeLearner","title":"Create learner object. — makeLearner","text":"classification learner predict.type can set “prob” predict probabilities maximum value selects label. threshold used assign label can later changed using setThreshold function. see possible properties learner, go : LearnerProperties.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create learner object. — makeLearner","text":"","code":"makeLearner(   cl,   id = cl,   predict.type = \"response\",   predict.threshold = NULL,   fix.factors.prediction = FALSE,   ...,   par.vals = list(),   config = list() )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create learner object. — makeLearner","text":"cl (character(1)) Class learner. convention, classification learners start “classif.” regression learners “regr.” survival learners start “surv.” clustering learners “cluster.” multilabel classification learners start “multilabel.”. list integrated learners available learners help page. id (character(1)) Id string object. Used display object. Default cl. predict.type (character(1)) Classification: “response” (= labels) “prob” (= probabilities labels selecting ones maximal probability). Regression: “response” (= mean response) “se” (= standard errors mean response). Survival: “response” (= sort orderable risk) “prob” (= time dependent probabilities). Clustering: “response” (= cluster IDS) “prob” (= fuzzy cluster membership probabilities), Multilabel: “response” (= logical matrix indicating predicted class labels) “prob” (= probabilities corresponding logical matrix indicating class labels). Default “response”. predict.threshold (numeric) Threshold produce class labels. named vector, names correspond class labels. binary classification can single numerical threshold positive class. See setThreshold details applied. Default NULL means 0.5 / equal threshold class. fix.factors.prediction (logical(1)) cases, problems occur underlying learners factor features prediction. new features LESS factor levels training (strict subset), learner might produce  error like “type predictors new data match training data”. case one can repair problem setting option TRUE. simply add missing factor levels missing test feature (present training) feature. Default FALSE. ... () Optional named (hyper)parameters. want set specific hyperparameters learner model creation, go . can get list available hyperparameters using getParamSet(<learner>). Alternatively hyperparameters can given using par.vals argument ... preferred! par.vals (list) Optional list named (hyper)parameters. arguments ... take precedence values list. strongly encourage use ... passing hyperparameters. config (named list) Named list config option overwrite global settings set via configureMlr specific learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create learner object. — makeLearner","text":"(Learner).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"par-vals-vs-","dir":"Reference","previous_headings":"","what":"par.vals vs. ...","title":"Create learner object. — makeLearner","text":"former aims specifying default hyperparameter settings mlr differ actual defaults underlying learner. example, respect.unordered.factors set order mlr default ranger::ranger depends argument splitrule. getHyperPars(<learner>) can used query hyperparameter defaults differ underlying learner. function also shows hyperparameters set user learner creation (differ learner defaults).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"regr-randomforest","dir":"Reference","previous_headings":"","what":"regr.randomForest","title":"Create learner object. — makeLearner","text":"learner added additional uncertainty estimation functionality (predict.type = \"se\") randomForest, provided underlying package. Currently implemented methods : se.method = \"jackknife\" standard error prediction estimated computing jackknife--bootstrap, mean-squared difference prediction made using trees contain said observation ensemble prediction. se.method = \"bootstrap\" standard error prediction estimated bootstrapping random forest, number bootstrap replicates number trees ensemble controlled se.boot se.ntree respectively, taking standard deviation bootstrap predictions. \"brute force\" bootstrap executed ntree = se.ntree, latter controls number trees individual random forests bootstrapped. \"noisy bootstrap\" executed se.ntree < ntree less computationally expensive. Monte-Carlo bias correction may make latter option preferable many cases. Defaults se.boot = 50 se.ntree = 100. se.method = \"sd\", default, standard deviation predictions across trees returned variance estimate. can computed quickly also naive estimator. “jackknife” “bootstrap”, Monte-Carlo bias correction applied , case results negative variance estimate, values truncated 0. Note using “jackknife” procedure se estimation, using small number trees can lead training data observations never --bag. current implementation ignores observations, original definition, resulting se estimation undefined. Please note mentioned se.method variants affect computation posterior mean “response” value. always underlying randomForest.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"regr-featureless","dir":"Reference","previous_headings":"","what":"regr.featureless","title":"Create learner object. — makeLearner","text":"basic baseline method useful model comparisons (beat , likely problem). consider features task uses target feature training data make predictions. Using observation weights currently supported. Methods “mean” “median” always predict constant value new observation corresponds observed mean median target feature training data, respectively. default method “mean” corresponds ZeroR algorithm WEKA.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"classif-featureless","dir":"Reference","previous_headings":"","what":"classif.featureless","title":"Create learner object. — makeLearner","text":"Method “majority” predicts always majority class new observation. case ties, one randomly sampled, constant class predicted observations test set. method used default. similar ZeroR classifier WEKA. difference ZeroR always predicts first class tied class values instead sampling randomly. Method “sample-prior” always samples random class individual test observation according prior probabilities observed training data. opt predict probabilities, class probabilities always correspond prior probabilities observed training data.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create learner object. — makeLearner","text":"","code":"makeLearner(\"classif.rpart\") #> Learner classif.rpart from package rpart #> Type: classif #> Name: Decision Tree; Short name: rpart #> Class: classif.rpart #> Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp #> Predict-Type: response #> Hyperparameters: xval=0 #>  makeLearner(\"classif.lda\", predict.type = \"prob\") #> Learner classif.lda from package MASS #> Type: classif #> Name: Linear Discriminant Analysis; Short name: lda #> Class: classif.lda #> Properties: twoclass,multiclass,numerics,factors,prob #> Predict-Type: prob #> Hyperparameters:  #>  lrn = makeLearner(\"classif.lda\", method = \"t\", nu = 10) getHyperPars(lrn) #> $method #> [1] \"t\" #>  #> $nu #> [1] 10 #>"},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearners.html","id":null,"dir":"Reference","previous_headings":"","what":"Create multiple learners at once. — makeLearners","title":"Create multiple learners at once. — makeLearners","text":"Small helper function can save typing creating mutiple learner objects. Calls makeLearner multiple times internally.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create multiple learners at once. — makeLearners","text":"","code":"makeLearners(cls, ids = NULL, type = NULL, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create multiple learners at once. — makeLearners","text":"cls (character) Classes learners. ids (character) Id strings. Must unique. Default cls. type (character(1)) Shortcut prepend type string cls one can set cls = \"rpart\". Default NULL, .e., used. ... () Optional named (hyper)parameters. want set specific hyperparameters learner model creation, go . can get list available hyperparameters using getParamSet(<learner>). Alternatively hyperparameters can given using par.vals argument ... preferred!","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearners.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create multiple learners at once. — makeLearners","text":"(named list Learner). Named ids.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeLearners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create multiple learners at once. — makeLearners","text":"","code":"makeLearners(c(\"rpart\", \"lda\"), type = \"classif\", predict.type = \"prob\") #> $classif.rpart #> Learner classif.rpart from package rpart #> Type: classif #> Name: Decision Tree; Short name: rpart #> Class: classif.rpart #> Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp #> Predict-Type: prob #> Hyperparameters: xval=0 #>  #>  #> $classif.lda #> Learner classif.lda from package MASS #> Type: classif #> Name: Linear Discriminant Analysis; Short name: lda #> Class: classif.lda #> Properties: twoclass,multiclass,numerics,factors,prob #> Predict-Type: prob #> Hyperparameters:  #>  #>"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMeasure.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct performance measure. — makeMeasure","title":"Construct performance measure. — makeMeasure","text":"measure object encapsulates function evaluate performance prediction. Information already implemented measures can obtained : measures. learner trained training set d1, results model m predicts another set d2 (may different one training set) resulting prediction. performance measure can now defined using information original task, fitted model prediction.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMeasure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct performance measure. — makeMeasure","text":"","code":"makeMeasure(   id,   minimize,   properties = character(0L),   fun,   extra.args = list(),   aggr = test.mean,   best = NULL,   worst = NULL,   name = id,   note = \"\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMeasure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct performance measure. — makeMeasure","text":"id (character(1)) Name measure. minimize (logical(1)) measure minimized? Default TRUE. properties (character) Set measure properties. standard property names include: - classif: measure applicable classification? - classif.multi: measure applicable multi-class classification? - multilabel: measure applicable multilabel classification? - regr: measure applicable regression? - surv: measure applicable survival? - cluster: measure applicable cluster? - costsens: measure applicable cost-sensitive learning? - req.pred: prediction object required calculation? Usually case. - req.truth: truth column required calculation? Usually case. - req.task: task object required calculation? Usually case - req.model: model object required calculation? Usually case. - req.feats: feature values required calculation? Usually case. - req.prob: predicted probabilities required calculation? Usually case, example AUC. Default character(0). fun (function(task, model, pred, feats, extra.args)) Calculates performance value. Usually need prediction object pred. - task (Task) task. - model (WrappedModel) fitted model. - pred (Prediction) Prediction object. - feats (data.frame) features. - extra.args (list) See . extra.args (list) List extra arguments always passed fun. Can changed construction via setMeasurePars(). Default empty list. aggr (Aggregation) Aggregation function, used aggregate values measured test / training sets measure single value. Default test.mean. best (numeric(1)) Best obtainable value measure. Default -Inf Inf, depending minimize. worst (numeric(1)) Worst obtainable value measure. Default Inf -Inf, depending minimize. name (character)  Name measure. Default id. note (character)  Description additional notes measure. Default “”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMeasure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct performance measure. — makeMeasure","text":"Measure.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMeasure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Construct performance measure. — makeMeasure","text":"","code":"f = function(task, model, pred, extra.args) {   sum((pred$data$response - pred$data$truth)^2) } makeMeasure(id = \"my.sse\", minimize = TRUE,   properties = c(\"regr\", \"response\"), fun = f) #> Name: my.sse #> Performance measure: my.sse #> Properties: regr,response #> Minimize: TRUE #> Best: -Inf; Worst: Inf #> Aggregated by: test.mean #> Arguments:  #> Note:"},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","title":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","text":"Combines multiple base learners dispatching hyperparameter “selected.learner” specific model class. allows tune model class (SVM, random forest, etc) also hyperparameters one go. Combine tuneParams makeTuneControlIrace powerful approach, see example . parameter set union (unique) base learners. order avoid name clashes parameter names prefixed base learner id, .e. learnerId.parameterName. predict.type Multiplexer inherited predict.type base learners. getter getLearnerProperties returns properties selected base learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","text":"","code":"makeModelMultiplexer(base.learners)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","text":"base.learners ([list` Learner) List Learners unique IDs.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","text":"(ModelMultiplexer). Learner specialized ModelMultiplexer.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexer.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","text":"Note logging output tuning somewhat shortened make readable. .e., artificial prefix parameter names suppressed.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create model multiplexer for model selection to tune over multiple\npossible models. — makeModelMultiplexer","text":"","code":"set.seed(123) # \\donttest{ library(BBmisc) #>  #> Attaching package: ‘BBmisc’ #> The following object is masked from ‘package:base’: #>  #>     isFALSE bls = list(   makeLearner(\"classif.ksvm\"),   makeLearner(\"classif.randomForest\") ) lrn = makeModelMultiplexer(bls) # simple way to contruct param set for tuning # parameter names are prefixed automatically and the 'requires' # element is set, too, to make all paramaters subordinate to 'selected.learner' ps = makeModelMultiplexerParamSet(lrn,   makeNumericParam(\"sigma\", lower = -10, upper = 10, trafo = function(x) 2^x),   makeIntegerParam(\"ntree\", lower = 1L, upper = 500L) ) print(ps) #>                                Type len Def                            Constr #> selected.learner           discrete   -   - classif.ksvm,classif.randomForest #> classif.ksvm.sigma          numeric   -   -                         -10 to 10 #> classif.randomForest.ntree  integer   -   -                          1 to 500 #>                            Req Tunable Trafo #> selected.learner             -    TRUE     - #> classif.ksvm.sigma           Y    TRUE     Y #> classif.randomForest.ntree   Y    TRUE     - rdesc = makeResampleDesc(\"CV\", iters = 2L) # to save some time we use random search. but you probably want something like this: # ctrl = makeTuneControlIrace(maxExperiments = 500L) ctrl = makeTuneControlRandom(maxit = 10L) res = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl) #> [Tune] Started tuning learner ModelMultiplexer for parameter set: #>                                Type len Def                            Constr #> selected.learner           discrete   -   - classif.ksvm,classif.randomForest #> classif.ksvm.sigma          numeric   -   -                         -10 to 10 #> classif.randomForest.ntree  integer   -   -                          1 to 500 #>                            Req Tunable Trafo #> selected.learner             -    TRUE     - #> classif.ksvm.sigma           Y    TRUE     Y #> classif.randomForest.ntree   Y    TRUE     - #> With control class: TuneControlRandom #> Imputation value: 1 #> [Tune-x] 1: selected.learner=classif.ksvm; sigma=0.0253 #> [Tune-y] 1: mmce.test.mean=0.1466667; time: 0.0 min #> [Tune-x] 2: selected.learner=classif.rand...; ntree=50 #> [Tune-y] 2: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune-x] 3: selected.learner=classif.ksvm; sigma=0.0494 #> [Tune-y] 3: mmce.test.mean=0.0933333; time: 0.0 min #> [Tune-x] 4: selected.learner=classif.rand...; ntree=434 #> [Tune-y] 4: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune-x] 5: selected.learner=classif.rand...; ntree=74 #> [Tune-y] 5: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune-x] 6: selected.learner=classif.rand...; ntree=262 #> [Tune-y] 6: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune-x] 7: selected.learner=classif.ksvm; sigma=0.0274 #> [Tune-y] 7: mmce.test.mean=0.1400000; time: 0.0 min #> [Tune-x] 8: selected.learner=classif.rand...; ntree=406 #> [Tune-y] 8: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune-x] 9: selected.learner=classif.rand...; ntree=63 #> [Tune-y] 9: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune-x] 10: selected.learner=classif.rand...; ntree=233 #> [Tune-y] 10: mmce.test.mean=0.0666667; time: 0.0 min #> [Tune] Result: selected.learner=classif.rand...; classif.randomForest.ntree=74 : mmce.test.mean=0.0666667 print(res) #> Tune result: #> Op. pars: selected.learner=classif.rand...; classif.randomForest.ntree=74 #> mmce.test.mean=0.0666667  df = as.data.frame(res$opt.path) print(head(df[, -ncol(df)])) #>       selected.learner classif.ksvm.sigma classif.randomForest.ntree #> 1         classif.ksvm          -5.303606                         NA #> 2 classif.randomForest                 NA                         50 #> 3         classif.ksvm          -4.340143                         NA #> 4 classif.randomForest                 NA                        434 #> 5 classif.randomForest                 NA                         74 #> 6 classif.randomForest                 NA                        262 #>   mmce.test.mean dob eol error.message #> 1     0.14666667   1  NA          <NA> #> 2     0.06666667   2  NA          <NA> #> 3     0.09333333   3  NA          <NA> #> 4     0.06666667   4  NA          <NA> #> 5     0.06666667   5  NA          <NA> #> 6     0.06666667   6  NA          <NA>  # more unique and reliable way to construct the param set ps = makeModelMultiplexerParamSet(lrn,   classif.ksvm = makeParamSet(     makeNumericParam(\"sigma\", lower = -10, upper = 10, trafo = function(x) 2^x)   ),   classif.randomForest = makeParamSet(     makeIntegerParam(\"ntree\", lower = 1L, upper = 500L)   ) )  # this is how you would construct the param set manually, works too ps = makeParamSet(   makeDiscreteParam(\"selected.learner\", values = extractSubList(bls, \"id\")),   makeNumericParam(\"classif.ksvm.sigma\", lower = -10, upper = 10, trafo = function(x) 2^x,     requires = quote(selected.learner == \"classif.ksvm\")),   makeIntegerParam(\"classif.randomForest.ntree\", lower = 1L, upper = 500L,     requires = quote(selected.learner == \"classif.randomForst\")) )  # all three ps-objects are exactly the same internally. # }"},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexerParamSet.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a parameter set for model multiplexer tuning. — makeModelMultiplexerParamSet","title":"Creates a parameter set for model multiplexer tuning. — makeModelMultiplexerParamSet","text":"Handy way create param set less typing. following done automatically: selected.learner param created Parameter names prefixed. requires field param set. makes parameters subordinate selected.learner","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexerParamSet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a parameter set for model multiplexer tuning. — makeModelMultiplexerParamSet","text":"","code":"makeModelMultiplexerParamSet(multiplexer, ..., .check = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexerParamSet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a parameter set for model multiplexer tuning. — makeModelMultiplexerParamSet","text":"multiplexer (ModelMultiplexer) muliplexer learner. ... (ParamHelpers::ParamSet | ParamHelpers::Param) () First option: Named param sets. Names must correspond base learners. need enter parameters want tune without reference selected.learner field way. (b) Second option. Just params enter param sets. Even shorter create. works can uniquely identified learner passed parameters belongs. .check (logical) Check param ... one param found base learners. Default TRUE","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexerParamSet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates a parameter set for model multiplexer tuning. — makeModelMultiplexerParamSet","text":"ParamSet.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeModelMultiplexerParamSet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates a parameter set for model multiplexer tuning. — makeModelMultiplexerParamSet","text":"","code":"# See makeModelMultiplexer"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMulticlassWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with multiclass method. — makeMulticlassWrapper","title":"Fuse learner with multiclass method. — makeMulticlassWrapper","text":"Fuses base learner multi-class method. Creates learner object, can used like learner object. way learners can handle binary classification able handle multi-class problems, . use multiclass--binary reduction principle, multiple binary problems created multiclass task. binary problems generated defined error-correcting-output-code (ECOC) code book. also allows simple well-known one-vs-one one-vs-rest approaches. Decoding currently done via Hamming decoding, see e.g. https://jmlr.org/papers/volume11/escalera10a/escalera10a.pdf. Currently, approach always operates discrete predicted labels binary base models (instead probabilities) created wrapper predict posterior probabilities.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMulticlassWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with multiclass method. — makeMulticlassWrapper","text":"","code":"makeMulticlassWrapper(learner, mcw.method = \"onevsrest\")"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMulticlassWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with multiclass method. — makeMulticlassWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. mcw.method (character(1) | function)  “onevsone” “onevsrest”. can also pass function, signature function(task) returns ECOC codematrix entries +1,-1,0. Columns define new binary problems, rows correspond classes (rows must named). 0 means class included binary problem. Default “onevsrest”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMulticlassWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with multiclass method. — makeMulticlassWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelBinaryRelevanceWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","title":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","text":"Every learner implemented mlr supports binary classification can converted wrapped binary relevance multilabel learner. multilabel classification problem converted simple binary classifications label/target binary learner applied. Models can easily accessed via getLearnerModel. Note make sense set threshold used base learner predict probabilities. hand, can make lot sense, call setThreshold MultilabelBinaryRelevanceWrapper label indvidually; tune thresholds tuneThreshold; especially face unabalanced class distributions binary label.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelBinaryRelevanceWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","text":"","code":"makeMultilabelBinaryRelevanceWrapper(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelBinaryRelevanceWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelBinaryRelevanceWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelBinaryRelevanceWrapper.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","text":"Tsoumakas, G., & Katakis, . (2006) Multi-label classification: overview. Dept. Informatics, Aristotle University Thessaloniki, Greece.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelBinaryRelevanceWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use binary relevance method to create a multilabel learner. — makeMultilabelBinaryRelevanceWrapper","text":"","code":"d = getTaskData(yeast.task) # drop some labels so example runs faster d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)] task = makeMultilabelTask(data = d, target = c(\"label1\", \"label2\")) lrn = makeLearner(\"classif.rpart\") lrn = makeMultilabelBinaryRelevanceWrapper(lrn) lrn = setPredictType(lrn, \"prob\") # train, predict and evaluate mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, task) #> Error in predict(mod, task): object 'mod' not found performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1)) #> Error in performance(pred, measure = list(multilabel.hamloss, multilabel.subset01,     multilabel.f1)): object 'pred' not found # the next call basically has the same structure for any multilabel meta wrapper getMultilabelBinaryPerformances(pred, measures = list(mmce, auc)) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found # above works also with predictions from resample!"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelClassifierChainsWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","title":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","text":"Every learner implemented mlr supports binary classification can converted wrapped classifier chains multilabel learner. CC trains binary classifier label following given order. training phase, feature space classifier extended true label information previous labels chain. prediction phase, true labels available, replaced predicted labels. Models can easily accessed via getLearnerModel.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelClassifierChainsWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","text":"","code":"makeMultilabelClassifierChainsWrapper(learner, order = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelClassifierChainsWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. order (character) Specifies chain order using names target labels. E.g. m target labels, must character vector length m contains permutation target label names. Default NULL uses random ordering target label names.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelClassifierChainsWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelClassifierChainsWrapper.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","text":"Montanes, E. et al. (2013) Dependent binary relevance models multi-label classification Artificial Intelligence Center, University Oviedo Gijon, Spain.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelClassifierChainsWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use classifier chains method (CC) to create a multilabel learner. — makeMultilabelClassifierChainsWrapper","text":"","code":"d = getTaskData(yeast.task) # drop some labels so example runs faster d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)] task = makeMultilabelTask(data = d, target = c(\"label1\", \"label2\")) lrn = makeLearner(\"classif.rpart\") lrn = makeMultilabelBinaryRelevanceWrapper(lrn) lrn = setPredictType(lrn, \"prob\") # train, predict and evaluate mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, task) #> Error in predict(mod, task): object 'mod' not found performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1)) #> Error in performance(pred, measure = list(multilabel.hamloss, multilabel.subset01,     multilabel.f1)): object 'pred' not found # the next call basically has the same structure for any multilabel meta wrapper getMultilabelBinaryPerformances(pred, measures = list(mmce, auc)) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found # above works also with predictions from resample!"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelDBRWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","title":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","text":"Every learner implemented mlr supports binary classification can converted wrapped DBR multilabel learner. multilabel classification problem converted simple binary classifications label/target binary learner applied. target, actual information binary labels (except target variable) used additional features. prediction labels need obtained binary relevance method using binary learner. Models can easily accessed via getLearnerModel.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelDBRWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","text":"","code":"makeMultilabelDBRWrapper(learner)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelDBRWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelDBRWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelDBRWrapper.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","text":"Montanes, E. et al. (2013) Dependent binary relevance models multi-label classification Artificial Intelligence Center, University Oviedo Gijon, Spain.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelDBRWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use dependent binary relevance method (DBR) to create a multilabel learner. — makeMultilabelDBRWrapper","text":"","code":"d = getTaskData(yeast.task) # drop some labels so example runs faster d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)] task = makeMultilabelTask(data = d, target = c(\"label1\", \"label2\")) lrn = makeLearner(\"classif.rpart\") lrn = makeMultilabelBinaryRelevanceWrapper(lrn) lrn = setPredictType(lrn, \"prob\") # train, predict and evaluate mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, task) #> Error in predict(mod, task): object 'mod' not found performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1)) #> Error in performance(pred, measure = list(multilabel.hamloss, multilabel.subset01,     multilabel.f1)): object 'pred' not found # the next call basically has the same structure for any multilabel meta wrapper getMultilabelBinaryPerformances(pred, measures = list(mmce, auc)) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found # above works also with predictions from resample!"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelNestedStackingWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","title":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","text":"Every learner implemented mlr supports binary classification can converted wrapped nested stacking multilabel learner. Nested stacking trains binary classifier label following given order. training phase, feature space classifier extended predicted label information (cross validation) previous labels chain. prediction phase, predicted labels obtained classifiers, learned training data. Models can easily accessed via getLearnerModel.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelNestedStackingWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","text":"","code":"makeMultilabelNestedStackingWrapper(learner, order = NULL, cv.folds = 2)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelNestedStackingWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. order (character) Specifies chain order using names target labels. E.g. m target labels, must character vector length m contains permutation target label names. Default NULL uses random ordering target label names. cv.folds (integer(1)) number folds inner cross validation method predict labels augmented feature space. Default 2.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelNestedStackingWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelNestedStackingWrapper.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","text":"Montanes, E. et al. (2013), Dependent binary relevance models multi-label classification Artificial Intelligence Center, University Oviedo Gijon, Spain.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelNestedStackingWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use nested stacking method to create a multilabel learner. — makeMultilabelNestedStackingWrapper","text":"","code":"d = getTaskData(yeast.task) # drop some labels so example runs faster d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)] task = makeMultilabelTask(data = d, target = c(\"label1\", \"label2\")) lrn = makeLearner(\"classif.rpart\") lrn = makeMultilabelBinaryRelevanceWrapper(lrn) lrn = setPredictType(lrn, \"prob\") # train, predict and evaluate mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, task) #> Error in predict(mod, task): object 'mod' not found performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1)) #> Error in performance(pred, measure = list(multilabel.hamloss, multilabel.subset01,     multilabel.f1)): object 'pred' not found # the next call basically has the same structure for any multilabel meta wrapper getMultilabelBinaryPerformances(pred, measures = list(mmce, auc)) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found # above works also with predictions from resample!"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelStackingWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","title":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","text":"Every learner implemented mlr supports binary classification can converted wrapped stacking multilabel learner. Stacking trains binary classifier label using predicted label information labels (including target label) additional features (cross validation). prediction labels need obtained binary relevance method using binary learner. Models can easily accessed via getLearnerModel.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelStackingWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","text":"","code":"makeMultilabelStackingWrapper(learner, cv.folds = 2)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelStackingWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. cv.folds (integer(1)) number folds inner cross validation method predict labels augmented feature space. Default 2.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelStackingWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelStackingWrapper.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","text":"Montanes, E. et al. (2013) Dependent binary relevance models multi-label classification Artificial Intelligence Center, University Oviedo Gijon, Spain.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeMultilabelStackingWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use stacking method (stacked generalization) to create a multilabel learner. — makeMultilabelStackingWrapper","text":"","code":"d = getTaskData(yeast.task) # drop some labels so example runs faster d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)] task = makeMultilabelTask(data = d, target = c(\"label1\", \"label2\")) lrn = makeLearner(\"classif.rpart\") lrn = makeMultilabelBinaryRelevanceWrapper(lrn) lrn = setPredictType(lrn, \"prob\") # train, predict and evaluate mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, task) #> Error in predict(mod, task): object 'mod' not found performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1)) #> Error in performance(pred, measure = list(multilabel.hamloss, multilabel.subset01,     multilabel.f1)): object 'pred' not found # the next call basically has the same structure for any multilabel meta wrapper getMultilabelBinaryPerformances(pred, measures = list(mmce, auc)) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found # above works also with predictions from resample!"},{"path":"https://mlr.mlr-org.com/dev/reference/makeOverBaggingWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with the bagging technique and oversampling for imbalancy correction. — makeOverBaggingWrapper","title":"Fuse learner with the bagging technique and oversampling for imbalancy correction. — makeOverBaggingWrapper","text":"Fuses classification learner binary classification -bagging method imbalancy correction strongly unequal class sizes. Creates learner object, can used like learner object. Models can easily accessed via getLearnerModel. OverBagging implemented follows: iteration random data subset sampled. Class examples oversampled replacement given rate. Members class either simply copied bag, bootstrapped replacement many majority class examples original training data. Features currently changed sampled. Prediction works follows: classification majority voting create discrete label probabilities predicted considering proportions predicted labels.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeOverBaggingWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with the bagging technique and oversampling for imbalancy correction. — makeOverBaggingWrapper","text":"","code":"makeOverBaggingWrapper(   learner,   obw.iters = 10L,   obw.rate = 1,   obw.maxcl = \"boot\",   obw.cl = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeOverBaggingWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with the bagging technique and oversampling for imbalancy correction. — makeOverBaggingWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. obw.iters (integer(1)) Number fitted models bagging. Default 10. obw.rate (numeric(1)) Factor upsample class bag. Must 1 Inf, 1 means oversampling 2 mean doubling class size. Default 1. obw.maxcl (character(1)) class (usually larger class) handled? “” means every instance class gets bag, “boot” means class instances bootstrapped iteration. Default “boot”. obw.cl (character(1)) class - undersampled. NULL, makeOverBaggingWrapper take smaller class.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeOverBaggingWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with the bagging technique and oversampling for imbalancy correction. — makeOverBaggingWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with preprocessing. — makePreprocWrapper","title":"Fuse learner with preprocessing. — makePreprocWrapper","text":"Fuses base learner preprocessing method. Creates learner object, can used like learner object, internally preprocesses data requested. train predict function called data / task, preprocessing always performed automatically.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with preprocessing. — makePreprocWrapper","text":"","code":"makePreprocWrapper(   learner,   train,   predict,   par.set = makeParamSet(),   par.vals = list() )"},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with preprocessing. — makePreprocWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. train (function(data, target, args)) Function preprocess data training. target string denotes target variable data. args list arguments parameters influence preprocessing. Must return list(data, control), data preprocessed data control stores information necessary preprocessing predictions. predict (function(data, target, args, control)) Function preprocess data prediction. target string denotes target variable data. args args passed train. control object returned train. Must return processed data. par.set (ParamHelpers::ParamSet) Parameter set ParamHelpers::LearnerParam objects describe parameters args. Default empty set. par.vals (list) Named list default values params args respectively par.set. Default empty list.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with preprocessing. — makePreprocWrapper","text":"(Learner).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapperCaret.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with preprocessing. — makePreprocWrapperCaret","title":"Fuse learner with preprocessing. — makePreprocWrapperCaret","text":"Fuses learner preprocessing methods provided caret::preProcess. training preprocessing performed preprocessing model stored. prediction preprocessing model transform test data according trained model. wrapped learner support missing values although case ppc.knnImpute, ppc.bagImpute ppc.medianImpute set TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapperCaret.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with preprocessing. — makePreprocWrapperCaret","text":"","code":"makePreprocWrapperCaret(learner, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapperCaret.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with preprocessing. — makePreprocWrapperCaret","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. ... () See caret::preProcess parameters listed . use might want define add.par.set can tuned.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makePreprocWrapperCaret.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with preprocessing. — makePreprocWrapperCaret","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeRLearner.classif.fdausc.glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification of functional data by Generalized Linear Models. — makeRLearner.classif.fdausc.glm","title":"Classification of functional data by Generalized Linear Models. — makeRLearner.classif.fdausc.glm","text":"Learner classification using Generalized Linear Models.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeRLearner.classif.fdausc.glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification of functional data by Generalized Linear Models. — makeRLearner.classif.fdausc.glm","text":"","code":"# S3 method for classif.fdausc.glm makeRLearner()"},{"path":"https://mlr.mlr-org.com/dev/reference/makeRLearner.classif.fdausc.kernel.html","id":null,"dir":"Reference","previous_headings":"","what":"Learner for kernel classification for functional data. — makeRLearner.classif.fdausc.kernel","title":"Learner for kernel classification for functional data. — makeRLearner.classif.fdausc.kernel","text":"Learner kernel Classification.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeRLearner.classif.fdausc.kernel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Learner for kernel classification for functional data. — makeRLearner.classif.fdausc.kernel","text":"","code":"# S3 method for classif.fdausc.kernel makeRLearner()"},{"path":"https://mlr.mlr-org.com/dev/reference/makeRLearner.classif.fdausc.np.html","id":null,"dir":"Reference","previous_headings":"","what":"Learner for nonparametric classification for functional data. — makeRLearner.classif.fdausc.np","title":"Learner for nonparametric classification for functional data. — makeRLearner.classif.fdausc.np","text":"Learner Nonparametric Supervised Classification.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeRLearner.classif.fdausc.np.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Learner for nonparametric classification for functional data. — makeRLearner.classif.fdausc.np","text":"","code":"# S3 method for classif.fdausc.np makeRLearner()"},{"path":"https://mlr.mlr-org.com/dev/reference/makeRemoveConstantFeaturesWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with removal of constant features preprocessing. — makeRemoveConstantFeaturesWrapper","title":"Fuse learner with removal of constant features preprocessing. — makeRemoveConstantFeaturesWrapper","text":"Fuses base learner preprocessing implemented removeConstantFeatures.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeRemoveConstantFeaturesWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with removal of constant features preprocessing. — makeRemoveConstantFeaturesWrapper","text":"","code":"makeRemoveConstantFeaturesWrapper(   learner,   perc = 0,   dont.rm = character(0L),   na.ignore = FALSE,   wrap.tol = .Machine$double.eps^0.5 )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeRemoveConstantFeaturesWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with removal of constant features preprocessing. — makeRemoveConstantFeaturesWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. perc (numeric(1)) percentage feature values [0, 1) must differ mode value. Default 0, means constant features exactly one observed level removed. dont.rm (character) Names columns must deleted. Default columns. na.ignore (logical(1)) NAs ignored percentage calculation? (treated single, extra level percentage calculation?) Note feature missing values, always removed. Default FALSE. wrap.tol (numeric(1)) Numerical tolerance treat two numbers equal. Variables stored double get rounded accordingly computing mode. Default sqrt(.Maschine$double.eps).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeRemoveConstantFeaturesWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with removal of constant features preprocessing. — makeRemoveConstantFeaturesWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a description object for a resampling strategy. — makeResampleDesc","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"description resampling algorithm contains necessary information create ResampleInstance, given size data set.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"","code":"makeResampleDesc(   method,   predict = \"test\",   ...,   stratify = FALSE,   stratify.cols = NULL,   fixed = FALSE,   blocking.cv = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"method (character(1)) “CV” cross-validation, “LOO” leave-one-, “RepCV” repeated cross-validation, “Bootstrap” --bag bootstrap, “Subsample” subsampling, “Holdout” holdout, “GrowingWindowCV” growing window cross-validation, “FixedWindowCV” fixed window cross validation. predict (character(1)) predict resampling: “train”, “test” “” sets. Default “test”. ... () parameters strategies. iters (integer(1)) Number iterations, “CV”, “Subsample” “Bootstrap”. split (numeric(1)) Proportion training cases “Holdout” “Subsample” 0 1. Default 2 / 3. reps (integer(1)) Repeats “RepCV”. iters = folds * reps. Default 10. folds (integer(1)) Folds repeated CV RepCV. iters = folds * reps. Default 10. horizon (numeric(1)) Number observations forecast test set “GrowingWindowCV” “FixedWindowCV”. horizon > 1 treated number observations forecast, else fraction initial window. IE, 100 observations, initial window .5, horizon .2, test set 10 observations. Default 1. initial.window (numeric(1)) Fraction observations start training set “GrowingWindowCV” “FixedWindowCV”. initial.window > 1 treated number observations initial window, else treated fraction observations initial window. Default 0.5. skip (numeric(1)) many resamples skip thin total amount “GrowingWindowCV” “FixedWindowCV”. passed “” argument seq(). skip > 1 treated increment sequence resampling indices, else fraction total training indices. IE 100 training sets value .2, increment resampling indices 20. Default “horizon” gives mutually exclusive chunks test indices. stratify (logical(1)) stratification done target variable? classification tasks, means resampling strategy applied classes individually resulting index sets joined make sure proportion observations training set original data set. Useful imbalanced class sizes. survival tasks stratification done events, resulting training sets comparable censoring rates. stratify.cols (character) Stratify specific columns referenced name. columns factor integer. Note ensure stratification possible, .e. strata contains enough observations. argument stratify mutually exclusive. fixed (logical(1)) Whether indices supplied via argument 'blocking' task used fully pre-defined indices. Default FALSE means used following 'blocking' approach. fixed works ResampleDesc CV supplied indices must match number observations. fixed = TRUE, iters argument ignored interally set number supplied factor levels blocking. blocking.cv (logical(1)) 'blocking' used CV? Default FALSE. different fixed = TRUE combined. Please check mlr online tutorial details.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"(ResampleDesc).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"notes special strategies: Repeated cross-validation Use “RepCV”. set aggregation function preferred performance measure “testgroup.mean” via setAggregation. B632 bootstrap Use “Bootstrap” bootstrap set predict “”. set aggregation function preferred performance measure “b632” via setAggregation. B632+ bootstrap Use “Bootstrap” bootstrap set predict “”. set aggregation function preferred performance measure “b632plus” via setAggregation. Fixed Holdout set Use makeFixedHoldoutInstance. Object slots: id (character(1)) Name resampling strategy. iters (integer(1)) Number iterations. Note always complete number generated train/test sets, 10-times repeated 5fold cross-validation 50. predict (character(1)) See argument. stratify (logical(1)) See argument. parameters passed ... respective argument name See arguments.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":"standard-resampledesc-objects","dir":"Reference","previous_headings":"","what":"Standard ResampleDesc objects","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"common resampling strategies can save typing using following description objects: hout holdout .k.. test sample estimation (two-thirds training set, one-third testing set) cv2 2-fold cross-validation cv3 3-fold cross-validation cv5 5-fold cross-validation cv10 10-fold cross-validation","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleDesc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a description object for a resampling strategy. — makeResampleDesc","text":"","code":"# Bootstraping makeResampleDesc(\"Bootstrap\", iters = 10) #> Resample description: OOB bootstrapping with 10 iterations. #> Predict: test #> Stratification: FALSE makeResampleDesc(\"Bootstrap\", iters = 10, predict = \"both\") #> Resample description: OOB bootstrapping with 10 iterations. #> Predict: both #> Stratification: FALSE  # Subsampling makeResampleDesc(\"Subsample\", iters = 10, split = 3 / 4) #> Resample description: subsampling with 10 iterations and 0.75 split rate. #> Predict: test #> Stratification: FALSE makeResampleDesc(\"Subsample\", iters = 10) #> Resample description: subsampling with 10 iterations and 0.67 split rate. #> Predict: test #> Stratification: FALSE  # Holdout a.k.a. test sample estimation makeResampleDesc(\"Holdout\") #> Resample description: holdout with 0.67 split rate. #> Predict: test #> Stratification: FALSE"},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleInstance.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates a resampling strategy object. — makeResampleInstance","title":"Instantiates a resampling strategy object. — makeResampleInstance","text":"class encapsulates training test sets generated data set number iterations. mainly stores set integer vectors indicating training test examples iteration.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleInstance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates a resampling strategy object. — makeResampleInstance","text":"","code":"makeResampleInstance(desc, task, size, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleInstance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates a resampling strategy object. — makeResampleInstance","text":"desc (ResampleDesc | character(1)) Resampling description object name resampling strategy. latter case makeResampleDesc called internally string. task (Task) Data task resample . Prefer pass instead size. size (integer) Size data set resample. Can used instead task. ... () Passed makeResampleDesc case passed string desc. Otherwise ignored.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleInstance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates a resampling strategy object. — makeResampleInstance","text":"(ResampleInstance).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleInstance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Instantiates a resampling strategy object. — makeResampleInstance","text":"Object slots: desc (ResampleDesc) See argument. size (integer(1)) See argument. train.inds (list integer) List training indices iterations. test.inds (list integer) List test indices iterations. group (factor) Optional grouping resampling iterations. encodes whether specific iterations 'belong together' (e.g. repeated CV), can later used aggregate performance values accordingly. Default 'factor()'.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeResampleInstance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Instantiates a resampling strategy object. — makeResampleInstance","text":"","code":"rdesc = makeResampleDesc(\"Bootstrap\", iters = 10) rin = makeResampleInstance(rdesc, task = iris.task)  rdesc = makeResampleDesc(\"CV\", iters = 50) rin = makeResampleInstance(rdesc, size = nrow(iris))  rin = makeResampleInstance(\"CV\", iters = 10, task = iris.task)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeSMOTEWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with SMOTE oversampling for imbalancy correction in binary classification. — makeSMOTEWrapper","title":"Fuse learner with SMOTE oversampling for imbalancy correction in binary classification. — makeSMOTEWrapper","text":"Creates learner object, can used like learner object. Internally uses smote every model fit. Note observation weights influence sampling simply passed next learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeSMOTEWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with SMOTE oversampling for imbalancy correction in binary classification. — makeSMOTEWrapper","text":"","code":"makeSMOTEWrapper(   learner,   sw.rate = 1,   sw.nn = 5L,   sw.standardize = TRUE,   sw.alt.logic = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeSMOTEWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with SMOTE oversampling for imbalancy correction in binary classification. — makeSMOTEWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. sw.rate (numeric(1)) Factor oversample smaller class. Must 1 Inf, 1 means oversampling 2 mean doubling class size. Default 1. sw.nn (integer(1)) Number nearest neighbors consider. Default 5. sw.standardize (logical(1)) Standardize input variables calculating nearest neighbors data sets numeric input variables . mixed variables (numeric factor) gower distance used variables standardized anyway. Default TRUE. sw.alt.logic (logical(1)) Use alternative logic selection minority class observations. Instead sampling minority class element one nearest neighbors, minority class element taken multiple times (depending rate) interpolation corresponding nearest neighbor sampled. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeSMOTEWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with SMOTE oversampling for imbalancy correction in binary classification. — makeSMOTEWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeStackedLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a stacked learner object. — makeStackedLearner","title":"Create a stacked learner object. — makeStackedLearner","text":"stacked learner uses predictions several base learners fits super learner using predictions features order predict outcome. following stacking methods available: average Averaging base learner predictions without weights. stack.nocv Fits super learner, -sample predictions base learners used. stack.cv Fits super learner, base learner predictions computed cross-validated predictions (resampling strategy can set via resampling argument). hill.climb Select subset base learner predictions hill climbing algorithm. compress Train neural network compress model collection base learners.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeStackedLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a stacked learner object. — makeStackedLearner","text":"","code":"makeStackedLearner(   base.learners,   super.learner = NULL,   predict.type = NULL,   method = \"stack.nocv\",   use.feat = FALSE,   resampling = NULL,   parset = list() )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeStackedLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a stacked learner object. — makeStackedLearner","text":"base.learners ((list ) Learner) list learners created makeLearner. super.learner (Learner | character(1)) super learner makes final prediction based base learners. pass string, super learner created via makeLearner. used method = 'average'. Default NULL. predict.type (character(1)) Sets type final prediction method = 'average'. methods, predict type set within super.learner. type base learner prediction, set within base.learners, \"prob\" predict.type = 'prob' use average base learner predictions predict.type = 'response' use class highest probability final prediction. \"response\" , classification tasks predict.type =    'prob', final prediction relative frequency based predicted base learner classes classification tasks predict.type    = 'response' use majority vote base learner predictions determine final prediction. regression tasks, final prediction average base learner predictions. method (character(1)) “average” averaging predictions base learners, “stack.nocv” building super learner using predictions base learners, “stack.cv” building super learner using cross-validated predictions base learners. “hill.climb” averaging predictions base learners, weights learned hill climbing algorithm “compress” compressing model mimic predictions collection base learners speeding predictions reducing size model. Default “stack.nocv”, use.feat (logical(1)) Whether original features also passed super learner. used method = 'average'. Default FALSE. resampling (ResampleDesc) Resampling strategy method = 'stack.cv'. Currently CV allowed resampling. default NULL uses 5-fold CV. parset parameters hill.climb method, including replace Whether base learner can selected . init Number best models included selection algorithm. bagprob proportion models considered one round selection. bagtime number rounds bagging selection. metric result evaluation metric function taking two parameters pred true, smaller score better. parameters compress method, including k size multiplier generated data prob probability exchange values s standard deviation numerical feature","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeStackedLearner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a stacked learner object. — makeStackedLearner","text":"","code":"# Classification data(iris) tsk = makeClassifTask(data = iris, target = \"Species\") base = c(\"classif.rpart\", \"classif.lda\", \"classif.svm\") lrns = lapply(base, makeLearner) lrns = lapply(lrns, setPredictType, \"prob\") m = makeStackedLearner(base.learners = lrns,   predict.type = \"prob\", method = \"hill.climb\") tmp = train(m, tsk) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions res = predict(tmp, tsk) #> Error in predict(tmp, tsk): object 'tmp' not found  # Regression data(BostonHousing, package = \"mlbench\") tsk = makeRegrTask(data = BostonHousing, target = \"medv\") base = c(\"regr.rpart\", \"regr.svm\") lrns = lapply(base, makeLearner) m = makeStackedLearner(base.learners = lrns,   predict.type = \"response\", method = \"compress\") tmp = train(m, tsk) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions res = predict(tmp, tsk) #> Error in predict(tmp, tsk): object 'tmp' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTaskDesc.html","id":null,"dir":"Reference","previous_headings":"","what":"Exported for internal use. — makeClassifTaskDesc","title":"Exported for internal use. — makeClassifTaskDesc","text":"Exported internal use.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTaskDesc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exported for internal use. — makeClassifTaskDesc","text":"","code":"makeClassifTaskDesc(id, data, target, weights, blocking, positive, coordinates)  makeClusterTaskDesc(id, data, weights, blocking, coordinates)  makeCostSensTaskDesc(id, data, target, blocking, costs, coordinates)  makeMultilabelTaskDesc(id, data, target, weights, blocking, coordinates)  makeRegrTaskDesc(id, data, target, weights, blocking, coordinates)  makeSurvTaskDesc(id, data, target, weights, blocking, coordinates)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTaskDesc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exported for internal use. — makeClassifTaskDesc","text":"id (character(1)) task id data (data.frame) data target (character) target columns weights (numeric) weights blocking ([numeric` task data blocking coordinates (data.frame) Coordinates spatial data set used spatial partitioning data spatial cross-validation resampling setting. Coordinates numeric values. Provided (data.frame) needs number rows data consist least two dimensions.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTaskDescInternal.html","id":null,"dir":"Reference","previous_headings":"","what":"Exported for internal use. — makeTaskDescInternal","title":"Exported for internal use. — makeTaskDescInternal","text":"Exported internal use.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTaskDescInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exported for internal use. — makeTaskDescInternal","text":"","code":"makeTaskDescInternal(type, id, data, target, weights, blocking, coordinates)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTaskDescInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exported for internal use. — makeTaskDescInternal","text":"type (character(1)) Task type. id (character(1)) task id data (data.frame) data target (character) target columns weights (numeric) weights blocking (numeric) task data blocking coordinates (logical(1)) whether spatial coordinates provided","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlCMAES.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with CMAES. — makeTuneControlCMAES","title":"Create control object for hyperparameter tuning with CMAES. — makeTuneControlCMAES","text":"CMA Evolution Strategy method cmaes::cma_es. Can handle numeric(vector) integer(vector) hyperparameters, dependencies. integers internally proposed numeric values automatically rounded. sigma variance parameter initialized 1/4 span box-constraints per parameter dimension.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlCMAES.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with CMAES. — makeTuneControlCMAES","text":"","code":"makeTuneControlCMAES(   same.resampling.instance = TRUE,   impute.val = NULL,   start = NULL,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL,   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlCMAES.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with CMAES. — makeTuneControlCMAES","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. start (list) Named list initial parameter values. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. budget corresponds product number generations (maxit) number offsprings per generation (lambda). ... () control parameters passed control arguments cmaes::cma_es GenSA::GenSA, well towards tunerConfig argument irace::irace.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlCMAES.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with CMAES. — makeTuneControlCMAES","text":"(TuneControlCMAES)","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlDesign.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with predefined design. — makeTuneControlDesign","title":"Create control object for hyperparameter tuning with predefined design. — makeTuneControlDesign","text":"Completely pre-specifiy data.frame design points evaluated tuning. kinds parameter types can handled.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlDesign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with predefined design. — makeTuneControlDesign","text":"","code":"makeTuneControlDesign(   same.resampling.instance = TRUE,   impute.val = NULL,   design = NULL,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlDesign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with predefined design. — makeTuneControlDesign","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. design (data.frame)data.frame containing different parameter settings evaluated. columns named according ParamSet used tune(). Proper designs can created ParamHelpers::generateDesign instance. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlDesign.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with predefined design. — makeTuneControlDesign","text":"(TuneControlDesign)","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGenSA.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with GenSA. — makeTuneControlGenSA","title":"Create control object for hyperparameter tuning with GenSA. — makeTuneControlGenSA","text":"Generalized simulated annealing method GenSA::GenSA. Can handle numeric(vector) integer(vector) hyperparameters, dependencies. integers internally proposed numeric values automatically rounded.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGenSA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with GenSA. — makeTuneControlGenSA","text":"","code":"makeTuneControlGenSA(   same.resampling.instance = TRUE,   impute.val = NULL,   start = NULL,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL,   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGenSA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with GenSA. — makeTuneControlGenSA","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. start (list) Named list initial parameter values. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. GenSA::GenSA defines budget via argument max.call. However, one note algorithm stop local search end. behavior might lead extension defined budget result warning. ... () control parameters passed control arguments cmaes::cma_es GenSA::GenSA, well towards tunerConfig argument irace::irace.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGenSA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with GenSA. — makeTuneControlGenSA","text":"(TuneControlGenSA).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGrid.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with grid search. — makeTuneControlGrid","title":"Create control object for hyperparameter tuning with grid search. — makeTuneControlGrid","text":"basic grid search can handle kinds parameter types. can either use correct param type resolution, discretize always using ParamHelpers::makeDiscreteParam par.set passed tuneParams.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGrid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with grid search. — makeTuneControlGrid","text":"","code":"makeTuneControlGrid(   same.resampling.instance = TRUE,   impute.val = NULL,   resolution = 10L,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGrid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with grid search. — makeTuneControlGrid","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. resolution (integer) Resolution grid numeric/integer parameter par.set. vector parameters, resolution per dimension. Either pass one resolution parameters, named vector. See ParamHelpers::generateGridDesign. Default 10. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. set, must equal size grid.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlGrid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with grid search. — makeTuneControlGrid","text":"(TuneControlGrid)","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlIrace.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with Irace. — makeTuneControlIrace","title":"Create control object for hyperparameter tuning with Irace. — makeTuneControlIrace","text":"Tuning iterated F-Racing method irace::irace. kinds parameter types can handled. return best final elite candidates found irace last race. estimated performance mean evaluations ever done candidate. information irace can found package vignette: vignette(\"irace-package\", package = \"irace\") resampling pass ResampleDesc, ResampleInstance. resampling strategy randomly instantiated n.instances times instances sense irace (instances element tunerConfig irace::irace). Also note irace always store tuning results file disk, see package documentation details change file path.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlIrace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with Irace. — makeTuneControlIrace","text":"","code":"makeTuneControlIrace(   impute.val = NULL,   n.instances = 100L,   show.irace.output = FALSE,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL,   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlIrace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with Irace. — makeTuneControlIrace","text":"impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. n.instances (integer(1)) Number random resampling instances irace, see details. Default 100. show.irace.output (logical(1)) Show console output irace tuning? Default FALSE. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. passed maxExperiments. ... () control parameters passed control arguments cmaes::cma_es GenSA::GenSA, well towards tunerConfig argument irace::irace.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlIrace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with Irace. — makeTuneControlIrace","text":"(TuneControlIrace)","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlMBO.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with MBO. — makeTuneControlMBO","title":"Create control object for hyperparameter tuning with MBO. — makeTuneControlMBO","text":"Model-based / Bayesian optimization function mlrMBO::mbo mlrMBO package. Please refer https://github.com/mlr-org/mlrMBO info.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlMBO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with MBO. — makeTuneControlMBO","text":"","code":"makeTuneControlMBO(   same.resampling.instance = TRUE,   impute.val = NULL,   learner = NULL,   mbo.control = NULL,   tune.threshold = FALSE,   tune.threshold.args = list(),   continue = FALSE,   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL,   mbo.design = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlMBO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with MBO. — makeTuneControlMBO","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. impute.val (numeric) something goes wrong optimization (e.g. learner crashes), value fed back tuner, tuning algorithm abort. Imputation active .learner.error configured stop configureMlr. stored optimization path, NA corresponding error message logged instead. Note value later multiplied -1 maximization measures internally, need enter larger positive value maximization well. Default worst obtainable value performance measure optimize aggregate mean value, Inf instead. multi-criteria optimization pass vector imputation values, one measures, order measures. learner (Learner | NULL) surrogate learner: regression learner model performance landscape. default, NULL, mlrMBO automatically create suitable learner based rules described mlrMBO::makeMBOLearner. mbo.control (mlrMBO::MBOControl | NULL) Control object model-based optimization tuning. default, NULL, control object created defaults described mlrMBO::makeMBOControl. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. continue (logical(1)) Resume calculation previous run using mlrMBO::mboContinue? Requires “save.file.path” set. Note ParamHelpers::OptPath mlrMBO::OptResult include evaluations continuation. complete OptPath found slot $mbo.result$opt.path. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. mbo.design (data.frame | NULL) Initial design data frame. parameters corresponding trafo functions, design must transformed passed! default, NULL, default design created like described mlrMBO::mbo.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlMBO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with MBO. — makeTuneControlMBO","text":"(TuneControlMBO)","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlMBO.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create control object for hyperparameter tuning with MBO. — makeTuneControlMBO","text":"Bernd Bischl, Jakob Richter, Jakob Bossek, Daniel Horn, Janek Thomas Michel Lang; mlrMBO: Modular Framework Model-Based Optimization Expensive Black-Box Functions, Preprint: https://arxiv.org/abs/1703.03373 (2017).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlRandom.html","id":null,"dir":"Reference","previous_headings":"","what":"Create control object for hyperparameter tuning with random search. — makeTuneControlRandom","title":"Create control object for hyperparameter tuning with random search. — makeTuneControlRandom","text":"Random search. kinds parameter types can handled.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlRandom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create control object for hyperparameter tuning with random search. — makeTuneControlRandom","text":"","code":"makeTuneControlRandom(   same.resampling.instance = TRUE,   maxit = NULL,   tune.threshold = FALSE,   tune.threshold.args = list(),   log.fun = \"default\",   final.dw.perc = NULL,   budget = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlRandom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create control object for hyperparameter tuning with random search. — makeTuneControlRandom","text":".resampling.instance (logical(1)) resampling instance used evaluations reduce variance? Default TRUE. maxit (integer(1) | NULL) Number iterations random search. Default 100. tune.threshold (logical(1)) threshold tuned measure hand, hyperparameter evaluation, via tuneThreshold? works classification predict type “prob”. Default FALSE. tune.threshold.args (list) arguments threshold tuning passed tuneThreshold. Default none. log.fun (function | character(1)) Function used logging. set “default” (default), evaluated design points, resulting performances, runtime reported. set “memory” memory usage evaluation also displayed, character(1) small increase run time. Otherwise character(1) function arguments learner, resampling, measures, par.set, control, opt.path, dob, x, y, remove.nas, stage prev.stage expected. default displays performance measures, time needed evaluating, currently used memory max memory ever used (latter two taken gc). See implementation details. final.dw.perc (boolean) Learner wrapped makeDownsampleWrapper used, can define value dw.perc used train Learner final parameter setting found tuning. Default NULL change anything. budget (integer(1)) Maximum budget tuning. value restricts number function evaluations. budget equals number iterations (maxit) performed random search algorithm.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneControlRandom.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create control object for hyperparameter tuning with random search. — makeTuneControlRandom","text":"(TuneControlRandom)","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with tuning. — makeTuneWrapper","title":"Fuse learner with tuning. — makeTuneWrapper","text":"Fuses base learner search strategy select hyperparameters. Creates learner object, can used like learner object, internally uses tuneParams. train function called , search strategy resampling invoked select optimal set hyperparameter values. Finally, model fitted complete training data optimal hyperparameters returned. See tuneParams details. training, optimal hyperparameters (related information) can retrieved getTuneResult.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with tuning. — makeTuneWrapper","text":"","code":"makeTuneWrapper(   learner,   resampling,   measures,   par.set,   control,   show.info = getMlrOption(\"show.info\") )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with tuning. — makeTuneWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. resampling (ResampleInstance | ResampleDesc) Resampling strategy evaluate points hyperparameter space. pass description, instantiated beginning default, points evaluated training/test sets. want change behavior, look TuneControl. measures (list Measure | Measure) Performance measures evaluate. first measure, aggregated first aggregation function optimized, others simply evaluated. Default default measure task, see getDefaultMeasure. par.set (ParamHelpers::ParamSet) Collection parameters constraints optimization. Dependent parameters requires field must use quote expression define . control (TuneControl) Control object search method. Also selects optimization algorithm tuning. show.info (logical(1)) Print verbose output console? Default set via configureMlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with tuning. — makeTuneWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeTuneWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fuse learner with tuning. — makeTuneWrapper","text":"","code":"# \\donttest{ task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.rpart\") # stupid mini grid ps = makeParamSet(   makeDiscreteParam(\"cp\", values = c(0.05, 0.1)),   makeDiscreteParam(\"minsplit\", values = c(10, 20)) ) ctrl = makeTuneControlGrid() inner = makeResampleDesc(\"Holdout\") outer = makeResampleDesc(\"CV\", iters = 2) lrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl) mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions print(getTuneResult(mod)) #> Error in checkClass(x, classes, ordered, null.ok): object 'mod' not found # nested resampling for evaluation # we also extract tuned hyper pars in each iteration r = resample(lrn, task, outer, extract = getTuneResult) #> Resampling: cross-validation #> Measures:             mmce       #> [Tune] Started tuning learner classif.rpart for parameter set: #>              Type len Def   Constr Req Tunable Trafo #> cp       discrete   -   - 0.05,0.1   -    TRUE     - #> minsplit discrete   -   -    10,20   -    TRUE     - #> With control class: TuneControlGrid #> Imputation value: 1 #> [Tune-x] 1: cp=0.05; minsplit=10 #> [Tune-y] 1: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune-x] 2: cp=0.1; minsplit=10 #> [Tune-y] 2: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune-x] 3: cp=0.05; minsplit=20 #> [Tune-y] 3: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune-x] 4: cp=0.1; minsplit=20 #> [Tune-y] 4: mmce.test.mean=0.0400000; time: 0.0 min #> [Tune] Result: cp=0.05; minsplit=20 : mmce.test.mean=0.0400000 #> [Resample] iter 1:    0.0533333  #> [Tune] Started tuning learner classif.rpart for parameter set: #>              Type len Def   Constr Req Tunable Trafo #> cp       discrete   -   - 0.05,0.1   -    TRUE     - #> minsplit discrete   -   -    10,20   -    TRUE     - #> With control class: TuneControlGrid #> Imputation value: 1 #> [Tune-x] 1: cp=0.05; minsplit=10 #> [Tune-y] 1: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune-x] 2: cp=0.1; minsplit=10 #> [Tune-y] 2: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune-x] 3: cp=0.05; minsplit=20 #> [Tune-y] 3: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune-x] 4: cp=0.1; minsplit=20 #> [Tune-y] 4: mmce.test.mean=0.0800000; time: 0.0 min #> [Tune] Result: cp=0.1; minsplit=10 : mmce.test.mean=0.0800000 #> [Resample] iter 2:    0.0800000  #>  #> Aggregated Result: mmce.test.mean=0.0666667 #>  print(r$extract) #> [[1]] #> Tune result: #> Op. pars: cp=0.05; minsplit=20 #> mmce.test.mean=0.0400000 #>  #> [[2]] #> Tune result: #> Op. pars: cp=0.1; minsplit=10 #> mmce.test.mean=0.0800000 #>  getNestedTuneResultsOptPathDf(r) #>     cp minsplit mmce.test.mean dob eol error.message exec.time iter #> 1 0.05       10           0.04   1  NA          <NA>     0.011    1 #> 2  0.1       10           0.04   2  NA          <NA>     0.012    1 #> 3 0.05       20           0.04   3  NA          <NA>     0.012    1 #> 4  0.1       20           0.04   4  NA          <NA>     0.012    1 #> 5 0.05       10           0.08   1  NA          <NA>     0.012    2 #> 6  0.1       10           0.08   2  NA          <NA>     0.023    2 #> 7 0.05       20           0.08   3  NA          <NA>     0.012    2 #> 8  0.1       20           0.08   4  NA          <NA>     0.011    2 getNestedTuneResultsX(r) #>     cp minsplit #> 1 0.05       20 #> 2 0.10       10 # }"},{"path":"https://mlr.mlr-org.com/dev/reference/makeUndersampleWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Fuse learner with simple ove/underrsampling for imbalancy correction in binary classification. — makeUndersampleWrapper","title":"Fuse learner with simple ove/underrsampling for imbalancy correction in binary classification. — makeUndersampleWrapper","text":"Creates learner object, can used like learner object. Internally uses oversample undersample every model fit. Note observation weights influence sampling simply passed next learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeUndersampleWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fuse learner with simple ove/underrsampling for imbalancy correction in binary classification. — makeUndersampleWrapper","text":"","code":"makeUndersampleWrapper(learner, usw.rate = 1, usw.cl = NULL)  makeOversampleWrapper(learner, osw.rate = 1, osw.cl = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeUndersampleWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fuse learner with simple ove/underrsampling for imbalancy correction in binary classification. — makeUndersampleWrapper","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. usw.rate (numeric(1)) Factor downsample class. Must 0 1, 1 means downsampling, 0.5 implies reduction 50 percent 0 imply reduction 0 observations. Default 1. usw.cl (character(1)) Class undersampled. Default NULL, means larger one. osw.rate (numeric(1)) Factor oversample class. Must 1 Inf, 1 means oversampling 2 mean doubling class size. Default 1. osw.cl (character(1)) Class oversampled. Default NULL, means smaller one.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeUndersampleWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fuse learner with simple ove/underrsampling for imbalancy correction in binary classification. — makeUndersampleWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeWeightedClassesWrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Wraps a classifier for weighted fitting where each class receives a weight. — makeWeightedClassesWrapper","title":"Wraps a classifier for weighted fitting where each class receives a weight. — makeWeightedClassesWrapper","text":"Creates wrapper, can used like learner object. Fitting performed weighted fashion observation receives weight, depending class belongs , see wcw.weight. might help mitigate problems caused imbalanced class distributions. weighted fitting can achieved two ways: ) learner already parameter class weighting, one weight can directly defined per class. Example: “classif.ksvm” parameter class.weights. case really anything fancy. convert wcw.weight bit, basically simply bind value class weighting param. wrapper case simply offers convenient, consistent fashion class weighting - tuning! See example . b) learner direct parameter support class weighting, supports observation weights, hasLearnerProperties(learner, 'weights') TRUE. means individual, arbitrary weight can set per observation training. set weight depending class internally wrapper. Basically introduce something like new “class.weights” parameter learner via observation weights.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeWeightedClassesWrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wraps a classifier for weighted fitting where each class receives a weight. — makeWeightedClassesWrapper","text":"","code":"makeWeightedClassesWrapper(learner, wcw.param = NULL, wcw.weight = 1)"},{"path":"https://mlr.mlr-org.com/dev/reference/makeWeightedClassesWrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wraps a classifier for weighted fitting where each class receives a weight. — makeWeightedClassesWrapper","text":"learner (Learner | character(1)) classification learner. pass string learner created via makeLearner. wcw.param (character(1)) Name already existing learner parameter, allows class weighting. default (wcw.param = NULL) use parameter defined learner (class.weights.param). training, parameter must accept named vector class weights, length equals number classes. wcw.weight (numeric) Weight class. Must vector number elements classes task, must also order class levels getTaskDesc(task)$class.levels. convenience, one must pass single number case binary classification, taken weight positive class, negative class receives weight 1. Default 1.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeWeightedClassesWrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wraps a classifier for weighted fitting where each class receives a weight. — makeWeightedClassesWrapper","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/makeWeightedClassesWrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wraps a classifier for weighted fitting where each class receives a weight. — makeWeightedClassesWrapper","text":"","code":"# \\donttest{ set.seed(123) # using the direct parameter of the SVM (which is already defined in the learner) lrn = makeWeightedClassesWrapper(\"classif.ksvm\", wcw.weight = 0.01) res = holdout(lrn, sonar.task) #> Resampling: holdout #> Measures:             mmce       #> [Resample] iter 1:    0.5428571  #>  #> Aggregated Result: mmce.test.mean=0.5428571 #>  print(calculateConfusionMatrix(res$pred)) #>         predicted #> true     M  R -err.- #>   M      0 38     38 #>   R      0 32      0 #>   -err.- 0 38     38  # using the observation weights of logreg lrn = makeWeightedClassesWrapper(\"classif.logreg\", wcw.weight = 0.01) res = holdout(lrn, sonar.task) #> Resampling: holdout #> Measures:             mmce       #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> [Resample] iter 1:    0.3285714  #>  #> Aggregated Result: mmce.test.mean=0.3285714 #>  print(calculateConfusionMatrix(res$pred)) #>         predicted #> true      M  R -err.- #>   M      28  7      7 #>   R      16 19     16 #>   -err.- 16  7     23  # tuning the imbalancy param and the SVM param in one go lrn = makeWeightedClassesWrapper(\"classif.ksvm\", wcw.param = \"class.weights\") ps = makeParamSet(   makeNumericParam(\"wcw.weight\", lower = 1, upper = 10),   makeNumericParam(\"C\", lower = -12, upper = 12, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x) ) ctrl = makeTuneControlRandom(maxit = 3L) rdesc = makeResampleDesc(\"CV\", iters = 2L, stratify = TRUE) res = tuneParams(lrn, sonar.task, rdesc, par.set = ps, control = ctrl) #> [Tune] Started tuning learner weightedclasses.classif.ksvm for parameter set: #>               Type len Def    Constr Req Tunable Trafo #> wcw.weight numeric   -   -   1 to 10   -    TRUE     - #> C          numeric   -   - -12 to 12   -    TRUE     Y #> sigma      numeric   -   - -12 to 12   -    TRUE     Y #> With control class: TuneControlRandom #> Imputation value: 1 #> [Tune-x] 1: wcw.weight=1.11; C=441; sigma=0.0013 #> [Tune-y] 1: mmce.test.mean=0.2644231; time: 0.0 min #> [Tune-x] 2: wcw.weight=3.81; C=0.336; sigma=6.86 #> [Tune-y] 2: mmce.test.mean=0.4663462; time: 0.0 min #> [Tune-x] 3: wcw.weight=4.05; C=3.78; sigma=241 #> [Tune-y] 3: mmce.test.mean=0.4663462; time: 0.0 min #> [Tune] Result: wcw.weight=1.11; C=441; sigma=0.0013 : mmce.test.mean=0.2644231 print(res) #> Tune result: #> Op. pars: wcw.weight=1.11; C=441; sigma=0.0013 #> mmce.test.mean=0.2644231 # print(res$opt.path) # }"},{"path":"https://mlr.mlr-org.com/dev/reference/makeWrappedModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Induced model of learner. — makeWrappedModel","title":"Induced model of learner. — makeWrappedModel","text":"Result train. internally stores underlying fitted model, subset used training, features used training, levels factors data set computation time spent training. Object members: See arguments. constructor makeWrappedModel mainly internal use.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeWrappedModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Induced model of learner. — makeWrappedModel","text":"","code":"makeWrappedModel(   learner,   learner.model,   task.desc,   subset,   features,   factor.levels,   time )"},{"path":"https://mlr.mlr-org.com/dev/reference/makeWrappedModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Induced model of learner. — makeWrappedModel","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. learner.model () Underlying model. task.desc TaskDesc Task description object. subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used. features (character) Features used training. factor.levels (named list character) Levels factor variables (features potentially target) training data. Named variable name, non-factors occur list. time (numeric(1)) Computation time model fit seconds.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/makeWrappedModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Induced model of learner. — makeWrappedModel","text":"WrappedModel.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Performance measures. — measures","title":"Performance measures. — measures","text":"performance measure evaluated single train/predict step returns single number assess quality prediction (maybe model, think AIC). measure knows whether wants minimized maximized tasks applicable. supported measures can found listMeasures table tutorial appendix: https://mlr.mlr-org.com/articles/tutorial/measures.html. want measure misclassification cost matrix, look makeCostMeasure. want implement measure, look makeMeasure. measures can directly accessed via function named scheme measureX (e.g. measureSSE). clustering measures, compact predicted cluster IDs form continuous series starting 1. case, measures generate warnings. measure parameters. defaults set constructor makeMeasure can overwritten using setMeasurePars.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performance measures. — measures","text":"","code":"measureSSE(truth, response)  measureMSE(truth, response)  measureRMSE(truth, response)  measureMEDSE(truth, response)  measureSAE(truth, response)  measureMAE(truth, response)  measureMEDAE(truth, response)  measureRSQ(truth, response)  measureEXPVAR(truth, response)  measureRRSE(truth, response)  measureRAE(truth, response)  measureMAPE(truth, response)  measureMSLE(truth, response)  measureRMSLE(truth, response)  measureKendallTau(truth, response)  measureSpearmanRho(truth, response)  measureMMCE(truth, response)  measureACC(truth, response)  measureBER(truth, response)  measureAUNU(probabilities, truth)  measureAUNP(probabilities, truth)  measureAU1U(probabilities, truth)  measureAU1P(probabilities, truth)  measureMulticlassBrier(probabilities, truth)  measureLogloss(probabilities, truth)  measureSSR(probabilities, truth)  measureQSR(probabilities, truth)  measureLSR(probabilities, truth)  measureKAPPA(truth, response)  measureWKAPPA(truth, response)  measureAUC(probabilities, truth, negative, positive)  measureBrier(probabilities, truth, negative, positive)  measureBrierScaled(probabilities, truth, negative, positive)  measureBAC(truth, response)  measureTP(truth, response, positive)  measureTN(truth, response, negative)  measureFP(truth, response, positive)  measureFN(truth, response, negative)  measureTPR(truth, response, positive)  measureTNR(truth, response, negative)  measureFPR(truth, response, negative, positive)  measureFNR(truth, response, negative, positive)  measurePPV(truth, response, positive, probabilities = NULL)  measureNPV(truth, response, negative)  measureFDR(truth, response, positive)  measureMCC(truth, response, negative, positive)  measureF1(truth, response, positive)  measureGMEAN(truth, response, negative, positive)  measureGPR(truth, response, positive)  measureMultilabelHamloss(truth, response)  measureMultilabelSubset01(truth, response)  measureMultilabelF1(truth, response)  measureMultilabelACC(truth, response)  measureMultilabelPPV(truth, response)  measureMultilabelTPR(truth, response)"},{"path":"https://mlr.mlr-org.com/dev/reference/measures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performance measures. — measures","text":"truth (factor) Vector true class. response (factor) Vector predicted class. probabilities (numeric | matrix) ) purely binary classification measures: predicted probabilities positive class numeric vector. b) multiclass classification measures: predicted probabilities classes, always numeric matrix, columns named class labels. negative (character(1)) name negative class. positive (character(1)) name positive class.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/measures.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Performance measures. — measures","text":", H. & Garcia, E. . (2009) Learning Imbalanced Data. IEEE Transactions Knowledge Data Engineering, vol. 21, . 9. pp. 1263-1284. H. Uno et al. C-statistics Evaluating Overall Adequacy Risk Prediction Procedures Censored Survival Data Statistics medicine. 2011;30(10):1105-1117. doi: 10.1002/sim.4154 . H. Uno et al. Evaluating Prediction Rules T-Year Survivors Censored Regression Models Journal American Statistical Association 102, . 478 (2007): 527-37.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/mergeBenchmarkResults.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge different BenchmarkResult objects. — mergeBenchmarkResults","title":"Merge different BenchmarkResult objects. — mergeBenchmarkResults","text":"function automatically combines list BenchmarkResult objects single BenchmarkResult object long full crossproduct task-learner combinations available.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mergeBenchmarkResults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge different BenchmarkResult objects. — mergeBenchmarkResults","text":"","code":"mergeBenchmarkResults(bmrs)"},{"path":"https://mlr.mlr-org.com/dev/reference/mergeBenchmarkResults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge different BenchmarkResult objects. — mergeBenchmarkResults","text":"bmrs (list BenchmarkResult)BenchmarkResult objects merged.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mergeBenchmarkResults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge different BenchmarkResult objects. — mergeBenchmarkResults","text":"BenchmarkResult","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mergeBenchmarkResults.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge different BenchmarkResult objects. — mergeBenchmarkResults","text":"Note want merge several BenchmarkResult objects, must ensure possible learner task combinations contained returned object. Otherwise, user notified task-learner combinations missing duplicated. merging BenchmarkResult objects different measures, missing measures automatically recomputed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mergeSmallFactorLevels.html","id":null,"dir":"Reference","previous_headings":"","what":"Merges small levels of factors into new level. — mergeSmallFactorLevels","title":"Merges small levels of factors into new level. — mergeSmallFactorLevels","text":"Merges factor levels occur infrequently combined levels higher frequency.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mergeSmallFactorLevels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merges small levels of factors into new level. — mergeSmallFactorLevels","text":"","code":"mergeSmallFactorLevels(   task,   cols = NULL,   min.perc = 0.01,   new.level = \".merged\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/mergeSmallFactorLevels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merges small levels of factors into new level. — mergeSmallFactorLevels","text":"task (Task) task. cols (character) columns convert. Default factor character columns. min.perc (numeric(1)) smallest levels factor merged combined proportion w.r.t. length factor exceeds min.perc. Must 0 1. Default 0.01. new.level (character(1)) New name merged level. Default “.merged”","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mergeSmallFactorLevels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merges small levels of factors into new level. — mergeSmallFactorLevels","text":"Task, merged levels combined new level name new.level.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/mlr-package.html","id":null,"dir":"Reference","previous_headings":"","what":"mlr: Machine Learning in R — mlr-package","title":"mlr: Machine Learning in R — mlr-package","text":"Interface large number classification regression techniques, including machine-readable parameter descriptions. also experimental extension survival analysis, clustering general, example-specific cost-sensitive learning. Generic resampling, including cross-validation, bootstrapping subsampling. Hyperparameter tuning modern optimization techniques, single- multi-objective problems. Filter wrapper methods feature selection. Extension basic learners additional operations common machine learning, also allowing easy nested resampling. operations can parallelized.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/mlr-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mlr: Machine Learning in R — mlr-package","text":"Maintainer: Patrick Schratz patrick.schratz@gmail.com (ORCID) Authors: Bernd Bischl bernd_bischl@gmx.net (ORCID) Michel Lang michellang@gmail.com (ORCID) Lars Kotthoff larsko@uwyo.edu Julia Schiffner schiffner@math.uni-duesseldorf.de Jakob Richter code@jakob-r.de Zachary Jones zmj@zmjones.com Giuseppe Casalicchio giuseppe.casalicchio@stat.uni-muenchen.de (ORCID) Mason Gallo masonagallo@gmail.com contributors: Jakob Bossek jakob.bossek@tu-dortmund.de (ORCID) [contributor] Erich Studerus erich.studerus@upkbs.ch (ORCID) [contributor] Leonard Judt leonard.judt@tu-dortmund.de [contributor] Tobias Kuehn tobi.kuehn@gmx.de [contributor] Pascal Kerschke kerschke@uni-muenster.de (ORCID) [contributor] Florian Fendt flo_fendt@gmx.de [contributor] Philipp Probst philipp_probst@gmx.de (ORCID) [contributor] Xudong Sun xudong.sun@stat.uni-muenchen.de (ORCID) [contributor] Janek Thomas janek.thomas@stat.uni-muenchen.de (ORCID) [contributor] Bruno Vieira bruno.hebling.vieira@usp.br [contributor] Laura Beggel laura.beggel@web.de (ORCID) [contributor] Quay Au quay.au@stat.uni-muenchen.de (ORCID) [contributor] Martin Binder ma.binder@campus.lmu.de [contributor] Florian Pfisterer pfistererf@googlemail.com [contributor] Stefan Coors stefan.coors@gmx.net [contributor] Steve Bronder sab2287@columbia.edu [contributor] Alexander Engelhardt alexander.w.engelhardt@gmail.com [contributor] Christoph Molnar christoph.molnar@stat.uni-muenchen.de [contributor] Annette Spooner .spooner@unsw.edu.au [contributor]","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mlrFamilies.html","id":null,"dir":"Reference","previous_headings":"","what":"mlr documentation families — mlrFamilies","title":"mlr documentation families — mlrFamilies","text":"List mlr documentation families members.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mlrFamilies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mlr documentation families — mlrFamilies","text":"benchmark batchmark, reduceBatchmarkResults, benchmark, benchmarkParallel, getBMRTaskIds, getBMRLearners, getBMRLearnerIds, getBMRLearnerShortNames, getBMRMeasures, getBMRMeasureIds, getBMRPredictions, getBMRPerformances, getBMRAggrPerformances, getBMRTuneResults, getBMRFeatSelResults, getBMRFilteredFeatures, getBMRModels, getBMRTaskDescs, convertBMRToRankMatrix, friedmanPostHocTestBMR, friedmanTestBMR, plotBMRBoxplots, plotBMRRanksAsBarChart, generateCritDifferencesData, plotCritDifferences calibration generateCalibrationData, plotCalibration configure configureMlr, getMlrOptions costsens makeCostSensTask, makeCostSensWeightedPairsWrapper debug predictFailureModel, getPredictionDump, getRRDump, print.ResampleResult downsample downsample eda_and_preprocess capLargeValues, createDummyFeatures, dropFeatures, mergeSmallFactorLevels, normalizeFeatures, removeConstantFeatures, summarizeColumns, summarizeLevels extractFDAFeatures reextractFDAFeatures fda_featextractor extractFDAFourier, extractFDAWavelets, extractFDAFPCA, extractFDAMultiResFeatures fda makeExtractFDAFeatMethod, extractFDAFeatures featsel analyzeFeatSelResult, makeFeatSelControl, getFeatSelResult, selectFeatures filter filterFeatures, makeFilter, listFilterMethods, getFilteredFeatures, generateFilterValuesData, getFilterValues generate_plot_data generateFeatureImportanceData, plotFilterValues, generatePartialDependenceData help helpLearner, helpLearnerParam imbalancy oversample, smote impute makeImputeMethod, imputeConstant, impute, reimpute learner getClassWeightParam, getHyperPars, getParamSet.Learner, getLearnerType, getLearnerId, getLearnerPredictType, getLearnerPackages, getLearnerParamSet, getLearnerParVals, setLearnerId, getLearnerShortName, getLearnerProperties, makeLearner, makeLearners, removeHyperPars, setHyperPars, setId, setPredictThreshold, setPredictType learning_curve generateLearningCurveData multilabel getMultilabelBinaryPerformances, makeMultilabelBinaryRelevanceWrapper, makeMultilabelClassifierChainsWrapper, makeMultilabelDBRWrapper, makeMultilabelNestedStackingWrapper, makeMultilabelStackingWrapper performance calculateConfusionMatrix, calculateROCMeasures, makeCustomResampledMeasure, makeCostMeasure, setMeasurePars, setAggregation, makeMeasure, featperc, performance, estimateRelativeOverfitting plot createSpatialResamplingPlots, plotLearningCurve, plotPartialDependence, plotBMRSummary, plotResiduals predict asROCRPrediction, getPredictionProbabilities, getPredictionTaskDesc, getPredictionResponse, predict.WrappedModel resample makeResampleDesc, makeResampleInstance, makeResamplePrediction, resample, getRRPredictions, getRRTaskDescription, getRRTaskDesc, getRRPredictionList, addRRMeasure task getTaskDesc, getTaskType, getTaskId, getTaskTargetNames, getTaskClassLevels, getTaskFeatureNames, getTaskNFeats, getTaskSize, getTaskFormula, getTaskTargets, getTaskData, getTaskCosts, subsetTask thresh_vs_perf generateThreshVsPerfData, plotThreshVsPerf, plotROCCurves tune getNestedTuneResultsX, getNestedTuneResultsOptPathDf, getResamplingIndices, getTuneResult, makeModelMultiplexerParamSet, makeModelMultiplexer, makeTuneControlCMAES, makeTuneControlDesign, makeTuneControlGenSA, makeTuneControlGrid, makeTuneControlIrace, makeTuneControlMBO, makeTuneControl, makeTuneControlRandom, tuneParams, tuneThreshold tune_multicrit plotTuneMultiCritResult, makeTuneMultiCritControl, tuneParamsMultiCrit wrapper makeBaggingWrapper, makeClassificationViaRegressionWrapper, makeConstantClassWrapper, makeCostSensClassifWrapper, makeCostSensRegrWrapper, makeDownsampleWrapper, makeDummyFeaturesWrapper, makeExtractFDAFeatsWrapper, makeFeatSelWrapper, makeFilterWrapper, makeImputeWrapper, makeMulticlassWrapper, makeOverBaggingWrapper, makeUndersampleWrapper, makePreprocWrapperCaret, makePreprocWrapper, makeRemoveConstantFeaturesWrapper, makeSMOTEWrapper, makeTuneWrapper, makeWeightedClassesWrapper","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mtcars.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Motor Trend Car Road Tests clustering task. — mtcars.task","title":"Motor Trend Car Road Tests clustering task. — mtcars.task","text":"Contains task (mtcars.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/mtcars.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Motor Trend Car Road Tests clustering task. — mtcars.task","text":"See datasets::mtcars.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/normalizeFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize features. — normalizeFeatures","title":"Normalize features. — normalizeFeatures","text":"Normalize features different methods. Internally BBmisc::normalize used every feature column. Non numerical features left untouched passed result. constant features methods fail, special behaviour case implemented.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/normalizeFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize features. — normalizeFeatures","text":"","code":"normalizeFeatures(   obj,   target = character(0L),   method = \"standardize\",   cols = NULL,   range = c(0, 1),   on.constant = \"quiet\" )"},{"path":"https://mlr.mlr-org.com/dev/reference/normalizeFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize features. — normalizeFeatures","text":"obj (data.frame | Task) Input data. target (character(1) | character(2) | character(n.classes)) Name(s) target variable(s). used obj data.frame, otherwise ignored. survival analysis applicable, names survival time event columns, length 2. multilabel classification names logical columns indicate whether class label present number target variables corresponds number classes. method (character(1)) Normalizing method. Available : “center”: Subtract mean. “scale”: Divide standard deviation. “standardize”: Center scale. “range”: Scale given range. cols (character) Columns normalize. Default use numeric columns. range (numeric(2)) Range method “range”. Default c(0,1). .constant (character(1)) constant vectors treated? used, “method != center”, since methods fail constant vectors. Possible actions : “quiet”: Depending method, treat quietly: “scale”: division standard deviation done, input values. returned untouched. “standardize”: mean subtracted, division done. “range”: values mapped mean given range. “warn”: behaviour “quiet”, print warning message. “stop”: Stop error.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/normalizeFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize features. — normalizeFeatures","text":"data.frame | Task. type obj.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/oversample.html","id":null,"dir":"Reference","previous_headings":"","what":"Over- or undersample binary classification task to handle class imbalancy. — oversample","title":"Over- or undersample binary classification task to handle class imbalancy. — oversample","text":"Oversampling: given class (usually smaller one) existing observations taken copied extra observations added randomly sampling replacement class. Undersampling: given class (usually larger one) number observations reduced (downsampled) randomly sampling without replacement class.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/oversample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Over- or undersample binary classification task to handle class imbalancy. — oversample","text":"","code":"oversample(task, rate, cl = NULL)  undersample(task, rate, cl = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/oversample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Over- or undersample binary classification task to handle class imbalancy. — oversample","text":"task (Task) task. rate (numeric(1)) Factor upsample downsample class. undersampling: Must 0 1, 1 means downsampling, 0.5 implies reduction 50 percent 0 imply reduction 0 observations. oversampling: Must 1 Inf, 1 means oversampling 2 mean doubling class size. cl (character(1)) class - undersampled. NULL, oversample select smaller undersample larger class.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/oversample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Over- or undersample binary classification task to handle class imbalancy. — oversample","text":"Task.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/parallelization.html","id":null,"dir":"Reference","previous_headings":"","what":"Supported parallelization methods — parallelization","title":"Supported parallelization methods — parallelization","text":"mlr supports different methods activate parallel computing capabilities integration parallelMap::parallelMap package, supports major parallelization backends R. can start parallelization parallelStart*, * replaced chosen backend. parallelMap::parallelStop used stop parallelization backends. Parallelization divided different levels automatically carried first level occurs, e.g. call resample() parallelMap::parallelStart, resampling iteration parallel job possible underlying calls like parameter tuning parallelized . supported levels parallelization : \"mlr.resample\" resampling iteration (train/test step) parallel job. \"mlr.benchmark\" experiment \"run learner data set\" parallel job. \"mlr.tuneParams\" evaluation hyperparameter space \"resample parameter settings\" parallel job. many can run independently parallel depends tuning algorithm. grid search random search limit, tuners depends many points evaluate produced iteration optimization. tuner works purely sequential fashion, work magic hyperparameter evaluation also run sequentially. note can still parallelize underlying resampling. \"mlr.selectFeatures\" evaluation feature space \"resample feature subset\" parallel job. comments \"mlr.tuneParams\" apply . \"mlr.ensemble\" ensemble methods, training prediction individual learner parallel job. Supported ensemble methods makeBaggingWrapper, makeCostSensRegrWrapper, makeMulticlassWrapper, makeMultilabelBinaryRelevanceWrapper makeOverBaggingWrapper.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/performance.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance of prediction. — performance","title":"Measure performance of prediction. — performance","text":"Measures quality prediction w.r.t. performance measure.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/performance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance of prediction. — performance","text":"","code":"performance(   pred,   measures,   task = NULL,   model = NULL,   feats = NULL,   simpleaggr = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/performance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance of prediction. — performance","text":"pred (Prediction) Prediction object. measures (Measure | list Measure) Performance measure(s) evaluate. Default default measure task, see getDefaultMeasure. task (Task) Learning task, might requested performance measure, usually needed except clustering survival. model (WrappedModel) Model built training data, might requested performance measure, usually needed except survival. feats (data.frame) Features predicted data, usually needed except clustering. prediction generated task, can also pass instead features extracted . simpleaggr (logical) TRUE, aggregation ResamplePrediction objects skipped. used internally threshold tuning. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/performance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure performance of prediction. — performance","text":"(named numeric). Performance value(s), named measure(s).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/performance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance of prediction. — performance","text":"","code":"training.set = seq(1, nrow(iris), by = 2) test.set = seq(2, nrow(iris), by = 2)  task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.lda\") mod = train(lrn, task, subset = training.set) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, newdata = iris[test.set, ]) #> Error in predict(mod, newdata = iris[test.set, ]): object 'mod' not found performance(pred, measures = mmce) #> Error in performance(pred, measures = mmce): object 'pred' not found  # Compute multiple performance measures at once ms = list(\"mmce\" = mmce, \"acc\" = acc, \"timetrain\" = timetrain) performance(pred, measures = ms, task, mod) #> Error in performance(pred, measures = ms, task, mod): object 'pred' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/phoneme.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Phoneme functional data multilabel classification task. — phoneme.task","title":"Phoneme functional data multilabel classification task. — phoneme.task","text":"Contains task (phoneme.task). task contains single functional covariate 5 equally big classes (aa, ao, dcl, iy, sh). aim predict class phoneme functional. dataset contained package fda.usc.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/phoneme.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Phoneme functional data multilabel classification task. — phoneme.task","text":"F. Ferraty P. Vieu (2003) \"Curve discrimination: nonparametric functional approach\", Computational Statistics Data Analysis, 44(1-2), 161-173. F. Ferraty P. Vieu (2006) Nonparametric functional data analysis, New York: Springer. T. Hastie R. Tibshirani J. Friedman (2009) elements statistical learning: Data mining, inference prediction, 2nd edn, New York: Springer.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/pid.task.html","id":null,"dir":"Reference","previous_headings":"","what":"PimaIndiansDiabetes classification task. — pid.task","title":"PimaIndiansDiabetes classification task. — pid.task","text":"Contains task (pid.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/pid.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"PimaIndiansDiabetes classification task. — pid.task","text":"See mlbench::PimaIndiansDiabetes. Note uncorrected version mlbench.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRBoxplots.html","id":null,"dir":"Reference","previous_headings":"","what":"Create box or violin plots for a BenchmarkResult. — plotBMRBoxplots","title":"Create box or violin plots for a BenchmarkResult. — plotBMRBoxplots","text":"Plots box violin plots selected measure across iterations resampling strategy, faceted task.id.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRBoxplots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create box or violin plots for a BenchmarkResult. — plotBMRBoxplots","text":"","code":"plotBMRBoxplots(   bmr,   measure = NULL,   style = \"box\",   order.lrns = NULL,   order.tsks = NULL,   pretty.names = TRUE,   facet.wrap.nrow = NULL,   facet.wrap.ncol = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRBoxplots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create box or violin plots for a BenchmarkResult. — plotBMRBoxplots","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. style (character(1)) Type plot, can “box” boxplot “violin” violin plot. Default “box”. order.lrns (character(n.learners)) Character vector learner.ids new order. order.tsks (character(n.tasks)) Character vector task.ids new order. pretty.names (logical(1)) Whether use Measure name Learner short name instead id. Default TRUE. facet.wrap.nrow, facet.wrap.ncol (integer) Number rows columns facetting. Default NULL. case ggplot's facet_wrap choose layout .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRBoxplots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create box or violin plots for a BenchmarkResult. — plotBMRBoxplots","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRBoxplots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create box or violin plots for a BenchmarkResult. — plotBMRBoxplots","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRRanksAsBarChart.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a bar chart for ranks in a BenchmarkResult. — plotBMRRanksAsBarChart","title":"Create a bar chart for ranks in a BenchmarkResult. — plotBMRRanksAsBarChart","text":"Plots bar chart ranks algorithms. Alternatively, tiles can plotted every rank-task combination, see pos details. plot variants ranks learning algorithms displayed x-axis. Areas always colored according learner.id.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRRanksAsBarChart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a bar chart for ranks in a BenchmarkResult. — plotBMRRanksAsBarChart","text":"","code":"plotBMRRanksAsBarChart(   bmr,   measure = NULL,   ties.method = \"average\",   aggregation = \"default\",   pos = \"stack\",   order.lrns = NULL,   order.tsks = NULL,   pretty.names = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRRanksAsBarChart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a bar chart for ranks in a BenchmarkResult. — plotBMRRanksAsBarChart","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. ties.method (character(1)) See rank details. aggregation (character(1))  “mean” “default”. See getBMRAggrPerformances details “default”. pos (character(1)) Optionally set bars positioned ggplot2. Ranks plotted x-axis. “tile” plots heat map task y-axis. Allows identification performance special task. “stack” plots stacked bar plot. Allows comparison learners within across ranks. “dodge” plots bar plot bars next instead stacked bars. order.lrns (character(n.learners)) Character vector learner.ids new order. order.tsks (character(n.tasks)) Character vector task.ids new order. pretty.names (logical(1)) Whether use short name learner instead ID labels. Defaults TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRRanksAsBarChart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a bar chart for ranks in a BenchmarkResult. — plotBMRRanksAsBarChart","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRRanksAsBarChart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a bar chart for ranks in a BenchmarkResult. — plotBMRRanksAsBarChart","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a benchmark summary. — plotBMRSummary","title":"Plot a benchmark summary. — plotBMRSummary","text":"Creates scatter plot, line refers task. line aggregated scores learners plotted, task. Optionally, can apply rank transformation just use one ggplot2's transformations like ggplot2::scale_x_log10.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a benchmark summary. — plotBMRSummary","text":"","code":"plotBMRSummary(   bmr,   measure = NULL,   trafo = \"none\",   order.tsks = NULL,   pointsize = 4L,   jitter = 0.05,   pretty.names = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a benchmark summary. — plotBMRSummary","text":"bmr (BenchmarkResult) Benchmark result. measure (Measure) Performance measure. Default first measure used benchmark experiment. trafo (character(1)) Currently either “none” “rank”, latter performing rank transformation (average handling ties) scores per task. NB: can add always add ggplot2::scale_x_log10 result put scores log scale. Default “none”. order.tsks (character(n.tasks)) Character vector task.ids new order. pointsize (numeric(1)) Point size ggplot2 ggplot2::geom_point data points. Default 4. jitter (numeric(1)) Small vertical jitter deal overplotting case equal scores. Default 0.05. pretty.names (logical(1)) Whether use short name learner instead ID labels. Defaults TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a benchmark summary. — plotBMRSummary","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotBMRSummary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot a benchmark summary. — plotBMRSummary","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/plotCalibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot calibration data using ggplot2. — plotCalibration","title":"Plot calibration data using ggplot2. — plotCalibration","text":"Plots calibration data generateCalibrationData.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotCalibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot calibration data using ggplot2. — plotCalibration","text":"","code":"plotCalibration(   obj,   smooth = FALSE,   reference = TRUE,   rag = TRUE,   facet.wrap.nrow = NULL,   facet.wrap.ncol = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotCalibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot calibration data using ggplot2. — plotCalibration","text":"obj (CalibrationData) Result generateCalibrationData. smooth (logical(1)) Whether use loess smoother. Default FALSE. reference (logical(1)) Whether plot reference line showing perfect calibration. Default TRUE. rag (logical(1)) Whether include rag plot shows rug plot top pertains positive cases bottom pertains negative cases. Default TRUE. facet.wrap.nrow, facet.wrap.ncol (integer) Number rows columns facetting. Default NULL. case ggplot's facet_wrap choose layout .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotCalibration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot calibration data using ggplot2. — plotCalibration","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotCalibration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot calibration data using ggplot2. — plotCalibration","text":"","code":"if (FALSE) { lrns = list(makeLearner(\"classif.rpart\", predict.type = \"prob\"),   makeLearner(\"classif.nnet\", predict.type = \"prob\")) fit = lapply(lrns, train, task = iris.task) pred = lapply(fit, predict, task = iris.task) names(pred) = c(\"rpart\", \"nnet\") out = generateCalibrationData(pred, groups = 3) plotCalibration(out)  fit = lapply(lrns, train, task = sonar.task) pred = lapply(fit, predict, task = sonar.task) names(pred) = c(\"rpart\", \"lda\") out = generateCalibrationData(pred) plotCalibration(out) }"},{"path":"https://mlr.mlr-org.com/dev/reference/plotCritDifferences.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot critical differences for a selected measure. — plotCritDifferences","title":"Plot critical differences for a selected measure. — plotCritDifferences","text":"Plots critical-differences diagram classifiers selected measure. baseline selected Bonferroni-Dunn test, critical difference interval positioned around baseline. , best performing algorithm chosen baseline. positioning descriptive elements can moved modifying generated data.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotCritDifferences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot critical differences for a selected measure. — plotCritDifferences","text":"","code":"plotCritDifferences(obj, baseline = NULL, pretty.names = TRUE)"},{"path":"https://mlr.mlr-org.com/dev/reference/plotCritDifferences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot critical differences for a selected measure. — plotCritDifferences","text":"obj (critDifferencesData) Result generateCritDifferencesData(). baseline (character(1)): (learner.id) Overwrites baseline generateCritDifferencesData()! Select learner.id baseline critical difference diagram, critical difference positioned around learner. Defaults best performing algorithm. pretty.names (logical(1)) Whether use short name learner instead ID labels. Defaults TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotCritDifferences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot critical differences for a selected measure. — plotCritDifferences","text":"ggplot2 plot object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotCritDifferences.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot critical differences for a selected measure. — plotCritDifferences","text":"Janez Demsar, Statistical Comparisons Classifiers Multiple Data Sets, JMLR, 2006","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotCritDifferences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot critical differences for a selected measure. — plotCritDifferences","text":"","code":"# see benchmark"},{"path":"https://mlr.mlr-org.com/dev/reference/plotFilterValues.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot filter values using ggplot2. — plotFilterValues","title":"Plot filter values using ggplot2. — plotFilterValues","text":"Plot filter values using ggplot2.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotFilterValues.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot filter values using ggplot2. — plotFilterValues","text":"","code":"plotFilterValues(   fvalues,   sort = \"dec\",   n.show = nrow(fvalues$data),   filter = NULL,   feat.type.cols = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotFilterValues.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot filter values using ggplot2. — plotFilterValues","text":"fvalues (FilterValues) Filter values. sort (character(1)) Available options : \"dec\"-> descending \"inc\" -> increasing \"none\" -> sorting Default decreasing. n.show (integer(1)) Number features (maximal) show. Default plot features. filter (character(1)) case fvalues contains multiple filter methods, method plotted? feat.type.cols (logical(1)) Whether color different feature types (e.g. numeric | factor). Default use colors (feat.type.cols = FALSE).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotFilterValues.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot filter values using ggplot2. — plotFilterValues","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotFilterValues.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot filter values using ggplot2. — plotFilterValues","text":"","code":"fv = generateFilterValuesData(iris.task, method = \"variance\") plotFilterValues(fv)"},{"path":"https://mlr.mlr-org.com/dev/reference/plotHyperParsEffect.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the hyperparameter effects data — plotHyperParsEffect","title":"Plot the hyperparameter effects data — plotHyperParsEffect","text":"Plot hyperparameter validation path. Automated plotting method HyperParsEffectData object. Useful determining importance effect particular hyperparameter performance measure /optimizer.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotHyperParsEffect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the hyperparameter effects data — plotHyperParsEffect","text":"","code":"plotHyperParsEffect(   hyperpars.effect.data,   x = NULL,   y = NULL,   z = NULL,   plot.type = \"scatter\",   loess.smooth = FALSE,   facet = NULL,   global.only = TRUE,   interpolate = NULL,   show.experiments = FALSE,   show.interpolated = FALSE,   nested.agg = mean,   partial.dep.learn = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotHyperParsEffect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the hyperparameter effects data — plotHyperParsEffect","text":"hyperpars.effect.data (HyperParsEffectData) Result generateHyperParsEffectData x (character(1)) Specify plotted x axis. Must column HyperParsEffectData$data. partial dependence, assumed hyperparameter. y (character(1)) Specify plotted y axis. Must column HyperParsEffectData$data z (character(1)) Specify used extra axis particular geom. fill heatmap color aesthetic line. Must column HyperParsEffectData$data. Default NULL. plot.type (character(1)) Specify type plot: “scatter” scatterplot, “heatmap” heatmap, “line” scatterplot connecting line, “contour” contour plot layered ontop heatmap. Default “scatter”. loess.smooth (logical(1)) TRUE, add loess smoothing line plots possible. Note probably useful plot.type set either “scatter” “line”. Must column HyperParsEffectData$data. used partial dependence. Default FALSE. facet (character(1)) Specify used facet axis particular geom. using nested cross validation, set “nested_cv_run” obtain facet outer loop. Must column HyperParsEffectData$data. Please note facetting supported partial dependence plots! Default NULL. global.(logical(1)) TRUE, plot current global optima setting x = \"iteration\" y performance measure HyperParsEffectData$measures. Set FALSE always plot performance every iteration, even improvement. used partial dependence. Default TRUE. interpolate (Learner | character(1)) NULL, interpolate non-complete grids order visualize complete path. meaningful attempting plot heatmap contour. fill “empty” cells heatmap contour plot. Note cases irregular hyperparameter paths, likely need use meaningful visualization. Accepts either regression Learner object learner string interpolation. used partial dependence. Default NULL. show.experiments (logical(1)) TRUE, overlay plot points indicating experiment ran. useful creating heatmap contour plot interpolation can see points actually original path. Note: learner crashes occurred within path, become TRUE. used partial dependence. Default FALSE. show.interpolated (logical(1)) TRUE, overlay plot points indicating interpolation ran. useful creating heatmap contour plot interpolation can see points interpolated. used partial dependence. Default FALSE. nested.agg (function) function used aggregate nested cross validation runs plotting 2 hyperparameters. also used nested aggregation partial dependence. Default mean. partial.dep.learn (Learner | character(1)) regression learner used learn partial dependence. Must specified “partial.dep” set TRUE generateHyperParsEffectData. Accepts either Learner object learner string learning partial dependence. Default NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotHyperParsEffect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the hyperparameter effects data — plotHyperParsEffect","text":"ggplot2 plot object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotHyperParsEffect.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Plot the hyperparameter effects data — plotHyperParsEffect","text":"NAs incurred learning algorithm crashes indicated plot (except case partial dependence) NA values replaced column min/max depending optimal values respective measure. Execution time replaced max. Interpolation nature result predicted values performance measure. Use interpolation caution. “partial.dep” set TRUE generateHyperParsEffectData, partial dependence plotted. Since ggplot2 plot object returned, user can change axis labels aspects plot using appropriate ggplot2 syntax.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotHyperParsEffect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the hyperparameter effects data — plotHyperParsEffect","text":"","code":"# see generateHyperParsEffectData"},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearnerPrediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualizes a learning algorithm on a 1D or 2D data set. — plotLearnerPrediction","title":"Visualizes a learning algorithm on a 1D or 2D data set. — plotLearnerPrediction","text":"Trains model 1 2 selected features, displays via ggplot2::ggplot. Good teaching exploring models. classification clustering, 2D plots supported. data points, classification potentially color alpha blending posterior probabilities shown. regression, 1D 2D plots supported. 1D shows data, estimated mean potentially estimated standard error. 2D show estimated standard error, estimated mean via background color. plot title displays model id, parameters, training performance cross-validation performance.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearnerPrediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualizes a learning algorithm on a 1D or 2D data set. — plotLearnerPrediction","text":"","code":"plotLearnerPrediction(   learner,   task,   features = NULL,   measures,   cv = 10L,   ...,   gridsize,   pointsize = 2,   prob.alpha = TRUE,   se.band = TRUE,   err.mark = \"train\",   bg.cols = c(\"darkblue\", \"green\", \"darkred\"),   err.col = \"white\",   err.size = pointsize,   greyscale = FALSE,   pretty.names = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearnerPrediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualizes a learning algorithm on a 1D or 2D data set. — plotLearnerPrediction","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. task (Task) task. features (character) Selected features model. default first 2 features used. measures (Measure | list Measure) Performance measure(s) evaluate. Default default measure task, see getDefaultMeasure. cv (integer(1)) cross-validation display plot title? Number folds. 0 means CV. Default 10. ... () Parameters learner. gridsize (integer(1)) Grid resolution per axis background predictions. Default 500 1D 100 2D. pointsize (numeric(1)) Pointsize ggplot2 ggplot2::geom_point data points. Default 2. prob.alpha (logical(1)) classification: Set alpha value background probability predicted class? Allows visualization “confidence” prediction. , constant color displayed background predicted label. Default TRUE. se.band (logical(1)) regression 1D: Show band standard error estimation? Default TRUE. err.mark (character(1)): classification: Either mark error model training data (“train”) cross-validation (“cv”) “none”. Default “train”. bg.cols (character(3)) Background colors classification regression. Sorted low, medium high. Default TRUE. err.col (character(1)) classification: Color misclassified data points. Default “white” err.size (integer(1)) classification: Size misclassified data points. Default pointsize. greyscale (logical(1)) plot greyscale completely? Default FALSE. pretty.names (logical(1)) Whether use short name learner instead ID labels. Defaults TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearnerPrediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualizes a learning algorithm on a 1D or 2D data set. — plotLearnerPrediction","text":"ggplot2 object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearningCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot learning curve data using ggplot2. — plotLearningCurve","title":"Plot learning curve data using ggplot2. — plotLearningCurve","text":"Visualizes data size (percentage used model) vs. performance measure(s).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearningCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot learning curve data using ggplot2. — plotLearningCurve","text":"","code":"plotLearningCurve(   obj,   facet = \"measure\",   pretty.names = TRUE,   facet.wrap.nrow = NULL,   facet.wrap.ncol = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearningCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot learning curve data using ggplot2. — plotLearningCurve","text":"obj (LearningCurveData) Result generateLearningCurveData, class LearningCurveData. facet (character(1)) Selects “measure” “learner” facetting variable. variable mapped facet must one unique value, otherwise ignored. variable chosen mapped color one unique value. default “measure”. pretty.names (logical(1)) Whether use Measure name instead id plot. Default TRUE. facet.wrap.nrow, facet.wrap.ncol (integer) Number rows columns facetting. Default NULL. case ggplot's facet_wrap choose layout .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotLearningCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot learning curve data using ggplot2. — plotLearningCurve","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotPartialDependence.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a partial dependence with ggplot2. — plotPartialDependence","title":"Plot a partial dependence with ggplot2. — plotPartialDependence","text":"Plot partial dependence generatePartialDependenceData using ggplot2.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotPartialDependence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a partial dependence with ggplot2. — plotPartialDependence","text":"","code":"plotPartialDependence(   obj,   geom = \"line\",   facet = NULL,   facet.wrap.nrow = NULL,   facet.wrap.ncol = NULL,   p = 1,   data = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotPartialDependence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a partial dependence with ggplot2. — plotPartialDependence","text":"obj PartialDependenceData Generated generatePartialDependenceData. geom (charater(1)) type geom use display data. Can “line” “tile”. tiling least two features must used interaction = TRUE call generatePartialDependenceData. may used conjuction facet argument three features specified call generatePartialDependenceData. Default “line”. facet (character(1)) name feature used facetting. feature must element features argument generatePartialDependenceData applicable said argument length greater 1. feature must factor integer. generatePartialDependenceData called interaction argument FALSE (default) argument features length greater one, facet ignored feature plotted facet. Default NULL. facet.wrap.nrow, facet.wrap.ncol (integer) Number rows columns facetting. Default NULL. case ggplot's facet_wrap choose layout . p (numeric(1)) individual = TRUE sample allows user sample without replacement output make display readable. row sampled probability p. Default 1. data (data.frame) Data points plot. Usually training data. survival binary classification tasks rug plot wherein ticks represent failures instances positive class shown. regression tasks points shown. multiclass classification tasks ticks shown colored according class. features target must included. Default NULL.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotPartialDependence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a partial dependence with ggplot2. — plotPartialDependence","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotROCCurves.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a ROC curve using ggplot2. — plotROCCurves","title":"Plots a ROC curve using ggplot2. — plotROCCurves","text":"Plots ROC curve predictions.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotROCCurves.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a ROC curve using ggplot2. — plotROCCurves","text":"","code":"plotROCCurves(   obj,   measures,   diagonal = TRUE,   pretty.names = TRUE,   facet.learner = FALSE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotROCCurves.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a ROC curve using ggplot2. — plotROCCurves","text":"obj (ThreshVsPerfData) Result generateThreshVsPerfData. measures ([list(2)` Measure) Default first 2 measures passed generateThreshVsPerfData. diagonal (logical(1)) Whether plot dashed diagonal line. Default TRUE. pretty.names (logical(1)) Whether use Measure name instead id plot. Default TRUE. facet.learner (logical(1)) Weather use facetting different colors compare multiple learners. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotROCCurves.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a ROC curve using ggplot2. — plotROCCurves","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotROCCurves.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plots a ROC curve using ggplot2. — plotROCCurves","text":"","code":"# \\donttest{ lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") fit = train(lrn, sonar.task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(fit, task = sonar.task) #> Error in predict(fit, task = sonar.task): object 'fit' not found roc = generateThreshVsPerfData(pred, list(fpr, tpr)) #> Error in generateThreshVsPerfData(pred, list(fpr, tpr)): object 'pred' not found plotROCCurves(roc) #> Error in checkClass(x, classes, ordered, null.ok): object 'roc' not found  r = bootstrapB632plus(lrn, sonar.task, iters = 3) #> Resampling: OOB bootstrapping #> Measures:             mmce.train   mmce.test     #> [Resample] iter 1:    0.0817308    0.2820513     #> [Resample] iter 2:    0.0865385    0.2318841     #> [Resample] iter 3:    0.0673077    0.2638889     #>  #> Aggregated Result: mmce.b632plus=0.2148920 #>  roc_r = generateThreshVsPerfData(r, list(fpr, tpr), aggregate = FALSE) plotROCCurves(roc_r)   r2 = crossval(lrn, sonar.task, iters = 3) #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.3913043  #> [Resample] iter 2:    0.2571429  #> [Resample] iter 3:    0.2898551  #>  #> Aggregated Result: mmce.test.mean=0.3127674 #>  roc_l = generateThreshVsPerfData(list(boot = r, cv = r2), list(fpr, tpr), aggregate = FALSE) plotROCCurves(roc_l)  # }"},{"path":"https://mlr.mlr-org.com/dev/reference/plotResiduals.html","id":null,"dir":"Reference","previous_headings":"","what":"Create residual plots for prediction objects or benchmark results. — plotResiduals","title":"Create residual plots for prediction objects or benchmark results. — plotResiduals","text":"Plots model diagnostics. Provides scatterplots true vs. predicted values histograms model's residuals.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotResiduals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create residual plots for prediction objects or benchmark results. — plotResiduals","text":"","code":"plotResiduals(   obj,   type = \"scatterplot\",   loess.smooth = TRUE,   rug = TRUE,   pretty.names = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotResiduals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create residual plots for prediction objects or benchmark results. — plotResiduals","text":"obj (Prediction | BenchmarkResult) Input data. type Type plot. Can “scatterplot”, default. “hist”, histogram, case classification problems barplot, displaying residuals. loess.smooth (logical(1)) loess smoother added plot? Defaults TRUE. applicable regression tasks type set scatterplot. rug (logical(1)) marginal distributions added plot? Defaults TRUE. applicable regression tasks type set scatterplot. pretty.names (logical(1)) Whether use short name learner instead ID labels. Defaults TRUE.  applicable BenchmarkResult passed obj function call, ignored otherwise.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotResiduals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create residual plots for prediction objects or benchmark results. — plotResiduals","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotThreshVsPerf.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot threshold vs. performance(s) for 2-class classification using ggplot2. — plotThreshVsPerf","title":"Plot threshold vs. performance(s) for 2-class classification using ggplot2. — plotThreshVsPerf","text":"Plots threshold vs. performance(s) data generated generateThreshVsPerfData.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotThreshVsPerf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot threshold vs. performance(s) for 2-class classification using ggplot2. — plotThreshVsPerf","text":"","code":"plotThreshVsPerf(   obj,   measures = obj$measures,   facet = \"measure\",   mark.th = NA_real_,   pretty.names = TRUE,   facet.wrap.nrow = NULL,   facet.wrap.ncol = NULL )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotThreshVsPerf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot threshold vs. performance(s) for 2-class classification using ggplot2. — plotThreshVsPerf","text":"obj (ThreshVsPerfData) Result generateThreshVsPerfData. measures (Measure | list Measure) Performance measure(s) plot. Must subset used generateThreshVsPerfData. Default measures stored obj generated generateThreshVsPerfData. facet (character(1)) Selects “measure” “learner” facetting variable. variable mapped facet must one unique value, otherwise ignored. variable chosen mapped color one unique value. default “measure”. mark.th (numeric(1)) Mark given threshold vertical line? Default NA means . pretty.names (logical(1)) Whether use Measure name instead id plot. Default TRUE. facet.wrap.nrow, facet.wrap.ncol (integer) Number rows columns facetting. Default NULL. case ggplot's facet_wrap choose layout .","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotThreshVsPerf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot threshold vs. performance(s) for 2-class classification using ggplot2. — plotThreshVsPerf","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotThreshVsPerf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot threshold vs. performance(s) for 2-class classification using ggplot2. — plotThreshVsPerf","text":"","code":"lrn = makeLearner(\"classif.rpart\", predict.type = \"prob\") mod = train(lrn, sonar.task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions pred = predict(mod, sonar.task) #> Error in predict(mod, sonar.task): object 'mod' not found pvs = generateThreshVsPerfData(pred, list(acc, setAggregation(acc, train.mean))) #> Error in generateThreshVsPerfData(pred, list(acc, setAggregation(acc,     train.mean))): object 'pred' not found plotThreshVsPerf(pvs) #> Error in checkClass(x, classes, ordered, null.ok): object 'pvs' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/plotTuneMultiCritResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots multi-criteria results after tuning using ggplot2. — plotTuneMultiCritResult","title":"Plots multi-criteria results after tuning using ggplot2. — plotTuneMultiCritResult","text":"Visualizes pareto front possibly dominated points.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotTuneMultiCritResult.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots multi-criteria results after tuning using ggplot2. — plotTuneMultiCritResult","text":"","code":"plotTuneMultiCritResult(   res,   path = TRUE,   col = NULL,   shape = NULL,   pointsize = 2,   pretty.names = TRUE )"},{"path":"https://mlr.mlr-org.com/dev/reference/plotTuneMultiCritResult.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots multi-criteria results after tuning using ggplot2. — plotTuneMultiCritResult","text":"res (TuneMultiCritResult) Result tuneParamsMultiCrit. path (logical(1)) Visualize evaluated points (non-dominated pareto front)? full path, size points front slightly increased. Default TRUE. col (character(1)) column res$opt.path mapped ggplot2 color? Default NULL, means none. shape (character(1)) column res$opt.path mapped ggplot2 shape? Default NULL, means none. pointsize (numeric(1)) Point size ggplot2 ggplot2::geom_point data points. Default 2. pretty.names (logical(1)) Whether use ID measures instead name labels. Defaults TRUE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/plotTuneMultiCritResult.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots multi-criteria results after tuning using ggplot2. — plotTuneMultiCritResult","text":"ggplot2 plot object.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/plotTuneMultiCritResult.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plots multi-criteria results after tuning using ggplot2. — plotTuneMultiCritResult","text":"","code":"# see tuneParamsMultiCrit"},{"path":"https://mlr.mlr-org.com/dev/reference/predict.WrappedModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict new data. — predict.WrappedModel","title":"Predict new data. — predict.WrappedModel","text":"Predict target variable new data using fitted model. stored exactly (Prediction) object depends predict.type setting Learner. predict.type set “prob” probability thresholding can done calling setThreshold function prediction object. row names input task newdata preserved output.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/predict.WrappedModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict new data. — predict.WrappedModel","text":"","code":"# S3 method for WrappedModel predict(object, task, newdata, subset = NULL, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/predict.WrappedModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict new data. — predict.WrappedModel","text":"object (WrappedModel) Wrapped model, result train. task (Task) task. passed, data task predicted. newdata (data.frame) New observations predicted. Pass alternatively instead task. subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used. ... () Currently ignored.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/predict.WrappedModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict new data. — predict.WrappedModel","text":"(Prediction).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/predict.WrappedModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict new data. — predict.WrappedModel","text":"","code":"# train and predict train.set = seq(1, 150, 2) test.set = seq(2, 150, 2) model = train(\"classif.lda\", iris.task, subset = train.set) #> Error: Please use column names for `x` p = predict(model, newdata = iris, subset = test.set) #> Error in predict(model, newdata = iris, subset = test.set): object 'model' not found print(p) #> Error in print(p): object 'p' not found predict(model, task = iris.task, subset = test.set) #> Error in predict(model, task = iris.task, subset = test.set): object 'model' not found  # predict now probabiliies instead of class labels lrn = makeLearner(\"classif.lda\", predict.type = \"prob\") model = train(lrn, iris.task, subset = train.set) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions p = predict(model, task = iris.task, subset = test.set) #> Error in predict(model, task = iris.task, subset = test.set): object 'model' not found print(p) #> Error in print(p): object 'p' not found getPredictionProbabilities(p) #> Error in checkClass(x, classes, ordered, null.ok): object 'p' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/predictLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict new data with an R learner. — predictLearner","title":"Predict new data with an R learner. — predictLearner","text":"Mainly internal use. Predict new data fitted model. implement method want add another learner package.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/predictLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict new data with an R learner. — predictLearner","text":"","code":"predictLearner(.learner, .model, .newdata, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/predictLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict new data with an R learner. — predictLearner","text":".learner (RLearner) Wrapped learner. .model (WrappedModel) Model produced training. .newdata (data.frame) New data predict. include target column. ... () Additional parameters, need passed underlying predict function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/predictLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict new data with an R learner. — predictLearner","text":"classification: Either factor class labels type “response” , learner supports , matrix class probabilities type “prob”. latter case columns must named class labels. regression: Either numeric vector type “response” , learner supports , matrix two columns type “se”. latter case first column contains estimated response (mean value) second column estimated standard errors. survival: Either numeric vector sort orderable risk type “response” , supported, numeric vector time dependent probabilities type “prob”. clustering: Either integer cluster IDs type “response” , supported, matrix membership probabilities type “prob”. multilabel: logical matrix indicates predicted class labels type “response” , supported, matrix class probabilities type “prob”. columns must named class labels.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/predictLearner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict new data with an R learner. — predictLearner","text":"implementation must adhere following: Predictions observations .newdata must made based fitted model (.model$learner.model). parameters ... must passed underlying predict function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reduceBatchmarkResults.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce results of a batch-distributed benchmark. — reduceBatchmarkResults","title":"Reduce results of a batch-distributed benchmark. — reduceBatchmarkResults","text":"creates BenchmarkResult batchtools::ExperimentRegistry. setup benchmark look batchmark.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reduceBatchmarkResults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce results of a batch-distributed benchmark. — reduceBatchmarkResults","text":"","code":"reduceBatchmarkResults(   ids = NULL,   keep.pred = TRUE,   keep.extract = FALSE,   show.info = getMlrOption(\"show.info\"),   reg = batchtools::getDefaultRegistry() )"},{"path":"https://mlr.mlr-org.com/dev/reference/reduceBatchmarkResults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce results of a batch-distributed benchmark. — reduceBatchmarkResults","text":"ids (data.frame integer) base::data.frame (data.table::data.table) column named “job.id”. Alternatively, may also pass vector integerish job ids. set, defaults successfully terminated jobs (return value batchtools::findDone. keep.pred (logical(1)) Keep prediction data pred slot result object. many experiments (larger data sets) objects might unnecessarily increase object size / mem usage, really need . default set TRUE. keep.extract (logical(1)) Keep extract slot result object. creating lot benchmark results extensive tuning, resulting R objects can become large size. tuning results stored extract slot removed default (keep.extract = FALSE). Note keep.extract = FALSE able conduct analysis tuning results. show.info (logical(1)) Print verbose output console? Default set via configureMlr. reg (batchtools::ExperimentRegistry) Registry, created batchtools::makeExperimentRegistry. explicitly passed, uses last created registry.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reduceBatchmarkResults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce results of a batch-distributed benchmark. — reduceBatchmarkResults","text":"(BenchmarkResult).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/reextractFDAFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Re-extract features from a data set — reextractFDAFeatures","title":"Re-extract features from a data set — reextractFDAFeatures","text":"function accepts data frame task extractFDAFeatDesc (FDA feature extraction description) returned extractFDAFeatures extract features previously unseen data.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reextractFDAFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Re-extract features from a data set — reextractFDAFeatures","text":"","code":"reextractFDAFeatures(obj, desc, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/reextractFDAFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Re-extract features from a data set — reextractFDAFeatures","text":"obj (Task | data.frame) Task data.frame extract functional features . Must contain functional features matrix columns. desc (extractFDAFeatDesc) FDAFeature extraction description returned extractFDAFeatures ... () args passed methods.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reextractFDAFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Re-extract features from a data set — reextractFDAFeatures","text":"data.frame Task containing extracted Features","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reimpute.html","id":null,"dir":"Reference","previous_headings":"","what":"Re-impute a data set — reimpute","title":"Re-impute a data set — reimpute","text":"function accepts data frame task imputation description returned impute perform following actions: Restore dropped columns, setting NA Add dummy variables columns specified impute Optionally check factors new levels treat NAs Reorder factor levels ensure identical integer representation Impute missing values using previously collected data","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reimpute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Re-impute a data set — reimpute","text":"","code":"reimpute(obj, desc)"},{"path":"https://mlr.mlr-org.com/dev/reference/reimpute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Re-impute a data set — reimpute","text":"obj (data.frame | Task) Input data. desc (ImputationDesc) Imputation description returned impute.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/reimpute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Re-impute a data set — reimpute","text":"Imputated data.frame task imputed data.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/removeConstantFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove constant features from a data set. — removeConstantFeatures","title":"Remove constant features from a data set. — removeConstantFeatures","text":"Constant features can lead errors models obviously provide information training set can learned . argument “perc”, possibility also remove features less “perc” percent observations differ mode value.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/removeConstantFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove constant features from a data set. — removeConstantFeatures","text":"","code":"removeConstantFeatures(   obj,   perc = 0,   dont.rm = character(0L),   na.ignore = FALSE,   wrap.tol = .Machine$double.eps^0.5,   show.info = getMlrOption(\"show.info\"),   ... )"},{"path":"https://mlr.mlr-org.com/dev/reference/removeConstantFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove constant features from a data set. — removeConstantFeatures","text":"obj (data.frame | Task) Input data. perc (numeric(1)) percentage feature values [0, 1) must differ mode value. Default 0, means constant features exactly one observed level removed. dont.rm (character) Names columns must deleted. Default columns. na.ignore (logical(1)) NAs ignored percentage calculation? (treated single, extra level percentage calculation?) Note feature missing values, always removed. Default FALSE. wrap.tol (numeric(1)) Numerical tolerance treat two numbers equal. Variables stored double get rounded accordingly computing mode. Default sqrt(.Maschine$double.eps). show.info (logical(1)) Print verbose output console? Default set via configureMlr. ... ensure backward compatibility old argument tol","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/removeConstantFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove constant features from a data set. — removeConstantFeatures","text":"data.frame | Task. type obj.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/removeHyperPars.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove hyperparameters settings of a learner. — removeHyperPars","title":"Remove hyperparameters settings of a learner. — removeHyperPars","text":"Remove settings (previously set mlr) parameters. means default behavior param now used.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/removeHyperPars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove hyperparameters settings of a learner. — removeHyperPars","text":"","code":"removeHyperPars(learner, ids = character(0L))"},{"path":"https://mlr.mlr-org.com/dev/reference/removeHyperPars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove hyperparameters settings of a learner. — removeHyperPars","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. ids (character) Parameter names remove settings . Default character(0L).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/removeHyperPars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove hyperparameters settings of a learner. — removeHyperPars","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/resample.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit models according to a resampling strategy. — resample","title":"Fit models according to a resampling strategy. — resample","text":"function resample fits model specified Learner Task calculates predictions performance measures training test sets specified either resampling description (ResampleDesc) resampling instance (ResampleInstance). able return fitted models (parameter models) extract specific parts models (parameter extract) returning completely might memory intensive. remaining functions page convenience wrappers various existing resampling strategies. Note need work precomputed training test splits (.e., resampling instances), stick resample.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/resample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit models according to a resampling strategy. — resample","text":"","code":"resample(   learner,   task,   resampling,   measures,   weights = NULL,   models = FALSE,   extract,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  crossval(   learner,   task,   iters = 10L,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  repcv(   learner,   task,   folds = 10L,   reps = 10L,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  holdout(   learner,   task,   split = 2/3,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  subsample(   learner,   task,   iters = 30,   split = 2/3,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  bootstrapOOB(   learner,   task,   iters = 30,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  bootstrapB632(   learner,   task,   iters = 30,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  bootstrapB632plus(   learner,   task,   iters = 30,   stratify = FALSE,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  growingcv(   learner,   task,   horizon = 1,   initial.window = 0.5,   skip = 0,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )  fixedcv(   learner,   task,   horizon = 1L,   initial.window = 0.5,   skip = 0,   measures,   models = FALSE,   keep.pred = TRUE,   ...,   show.info = getMlrOption(\"show.info\") )"},{"path":"https://mlr.mlr-org.com/dev/reference/resample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit models according to a resampling strategy. — resample","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. task (Task) task. resampling (ResampleDesc ResampleInstance) Resampling strategy. description passed, instantiated automatically. measures (Measure | list Measure) Performance measure(s) evaluate. Default default measure task, see getDefaultMeasure. weights (numeric) Optional, non-negative case weight vector used fitting. given, must length observations task corresponding order. Overwrites weights specified task. default NULL means weights used unless specified task. models (logical(1)) fitted models returned? Default FALSE. extract (function) Function used extract information fitted model resampling. applied every WrappedModel resulting calls train resampling. Default extract nothing. keep.pred (logical(1)) Keep prediction data pred slot result object. many experiments (larger data sets) objects might unnecessarily increase object size / mem usage, really need . default set TRUE. ... () hyperparameters passed learner. show.info (logical(1)) Print verbose output console? Default set via configureMlr. iters (integer(1)) See ResampleDesc. stratify (logical(1)) See ResampleDesc. folds (integer(1)) See ResampleDesc. reps (integer(1)) See ResampleDesc. split (numeric(1)) See ResampleDesc. horizon (numeric(1)) See ResampleDesc. initial.window (numeric(1)) See ResampleDesc. skip (integer(1)) See ResampleDesc.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/resample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit models according to a resampling strategy. — resample","text":"(ResampleResult).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/resample.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fit models according to a resampling strategy. — resample","text":"like include results training data set, make sure appropriately adjust resampling strategy aggregation measure. See example code .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/resample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit models according to a resampling strategy. — resample","text":"","code":"task = makeClassifTask(data = iris, target = \"Species\") rdesc = makeResampleDesc(\"CV\", iters = 2) r = resample(makeLearner(\"classif.qda\"), task, rdesc) #> Resampling: cross-validation #> Measures:             mmce       #> [Resample] iter 1:    0.0133333  #> [Resample] iter 2:    0.0266667  #>  #> Aggregated Result: mmce.test.mean=0.0200000 #>  print(r$aggr) #> mmce.test.mean  #>           0.02  print(r$measures.test) #>   iter       mmce #> 1    1 0.01333333 #> 2    2 0.02666667 print(r$pred) #> Resampled Prediction for: #> Resample description: cross-validation with 2 iterations. #> Predict: test #> Stratification: FALSE #> predict.type: response #> threshold:  #> time (mean): 0.00 #>   id  truth response iter  set #> 1  1 setosa   setosa    1 test #> 2  4 setosa   setosa    1 test #> 3  6 setosa   setosa    1 test #> 4  7 setosa   setosa    1 test #> 5 10 setosa   setosa    1 test #> 6 13 setosa   setosa    1 test #> ... (#rows: 150, #cols: 5)  # include the training set performance as well rdesc = makeResampleDesc(\"CV\", iters = 2, predict = \"both\") r = resample(makeLearner(\"classif.qda\"), task, rdesc,   measures = list(mmce, setAggregation(mmce, train.mean))) #> Resampling: cross-validation #> Measures:             mmce.train   mmce.test     #> [Resample] iter 1:    0.0000000    0.0533333     #> [Resample] iter 2:    0.0133333    0.0133333     #>  #> Aggregated Result: mmce.test.mean=0.0333333,mmce.train.mean=0.0066667 #>  print(r$aggr) #>  mmce.test.mean mmce.train.mean  #>     0.033333333     0.006666667"},{"path":"https://mlr.mlr-org.com/dev/reference/selectFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature selection by wrapper approach. — selectFeatures","title":"Feature selection by wrapper approach. — selectFeatures","text":"Optimizes features classification regression problem choosing variable selection wrapper approach. Allows different optimization methods, forward search genetic algorithm. can select algorithm (settings) passing corresponding control object. complete list implemented algorithms look subclasses (FeatSelControl). algorithms operate 0-1-bit encoding candidate solutions. Per default single bit corresponds single feature, able change using arguments bit.names bits..features. Thus allowing switch whole groups features single bit.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/selectFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature selection by wrapper approach. — selectFeatures","text":"","code":"selectFeatures(   learner,   task,   resampling,   measures,   bit.names,   bits.to.features,   control,   show.info = getMlrOption(\"show.info\") )"},{"path":"https://mlr.mlr-org.com/dev/reference/selectFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature selection by wrapper approach. — selectFeatures","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. task (Task) task. resampling (ResampleInstance | ResampleDesc) Resampling strategy feature selection. pass description, instantiated beginning default, points evaluated training/test sets. want change behavior, look FeatSelControl. measures (list Measure | Measure) Performance measures evaluate. first measure, aggregated first aggregation function optimized, others simply evaluated. Default default measure task, see getDefaultMeasure. bit.names character Names bits encoding solutions. Also defines total number bits encoding. Per default feature names task. used together bits..features. bits..features (function(x, task)) Function transforms integer-0-1 vector character vector selected features. Per default value 1 ith bit selects ith feature candidate solution. vector x correspond bit.names length. control [see FeatSelControl) Control object search method. Also selects optimization algorithm feature selection. show.info (logical(1)) Print verbose output console? Default set via configureMlr.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/selectFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature selection by wrapper approach. — selectFeatures","text":"(FeatSelResult).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/selectFeatures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Feature selection by wrapper approach. — selectFeatures","text":"","code":"# \\donttest{ rdesc = makeResampleDesc(\"Holdout\") ctrl = makeFeatSelControlSequential(method = \"sfs\", maxit = NA) res = selectFeatures(\"classif.rpart\", iris.task, rdesc, control = ctrl) #> [FeatSel] Started selecting features for learner 'classif.rpart' #> With control class: FeatSelControlSequential #> Imputation value: 1 #> [FeatSel-x] 1: 0000 (0 bits) #> [FeatSel-y] 1: mmce.test.mean=0.6800000; time: 0.0 min #> [FeatSel-x] 2: 1000 (1 bits) #> [FeatSel-y] 2: mmce.test.mean=0.2800000; time: 0.0 min #> [FeatSel-x] 2: 0100 (1 bits) #> [FeatSel-y] 2: mmce.test.mean=0.4600000; time: 0.0 min #> [FeatSel-x] 2: 0010 (1 bits) #> [FeatSel-y] 2: mmce.test.mean=0.0400000; time: 0.0 min #> [FeatSel-x] 2: 0001 (1 bits) #> [FeatSel-y] 2: mmce.test.mean=0.0400000; time: 0.0 min #> [FeatSel-x] 3: 1010 (2 bits) #> [FeatSel-y] 3: mmce.test.mean=0.0400000; time: 0.0 min #> [FeatSel-x] 3: 0110 (2 bits) #> [FeatSel-y] 3: mmce.test.mean=0.0400000; time: 0.0 min #> [FeatSel-x] 3: 0011 (2 bits) #> [FeatSel-y] 3: mmce.test.mean=0.0400000; time: 0.0 min #> Warning: one argument not used by format '[FeatSel] Result: %s (%i bits)' #> [FeatSel] Result: Petal.Length (1 bits) analyzeFeatSelResult(res) #> Features         : 1 #> Performance      : mmce.test.mean=0.0400000 #> Petal.Length #>  #> Path to optimum: #> - Features:    0  Init   :                       Perf = 0.68  Diff: NA  * #> - Features:    1  Add    : Petal.Length          Perf = 0.04  Diff: 0.64  * #>  #> Stopped, because no improving feature was found. # }"},{"path":"https://mlr.mlr-org.com/dev/reference/setAggregation.html","id":null,"dir":"Reference","previous_headings":"","what":"Set aggregation function of measure. — setAggregation","title":"Set aggregation function of measure. — setAggregation","text":"Set measure aggregated resampling. see possible aggregation functions: aggregations.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setAggregation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set aggregation function of measure. — setAggregation","text":"","code":"setAggregation(measure, aggr)"},{"path":"https://mlr.mlr-org.com/dev/reference/setAggregation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set aggregation function of measure. — setAggregation","text":"measure (Measure) Performance measure. aggr (Aggregation) Aggregation function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setAggregation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set aggregation function of measure. — setAggregation","text":"(Measure) changed aggregation behaviour.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the hyperparameters of a learner object. — setHyperPars","title":"Set the hyperparameters of a learner object. — setHyperPars","text":"Set hyperparameters learner object.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the hyperparameters of a learner object. — setHyperPars","text":"","code":"setHyperPars(learner, ..., par.vals = list())"},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the hyperparameters of a learner object. — setHyperPars","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. ... () Optional named (hyper)parameters. want set specific hyperparameters learner model creation, go . can get list available hyperparameters using getParamSet(<learner>). Alternatively hyperparameters can given using par.vals argument ... preferred! par.vals (list) Optional list named (hyper)parameters. arguments ... take precedence values list. strongly encourage use ... passing hyperparameters.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the hyperparameters of a learner object. — setHyperPars","text":"Learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Set the hyperparameters of a learner object. — setHyperPars","text":"named (hyper)parameter found given learner, 3 closest (hyper)parameter names output case user mistyped.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the hyperparameters of a learner object. — setHyperPars","text":"","code":"cl1 = makeLearner(\"classif.ksvm\", sigma = 1) cl2 = setHyperPars(cl1, sigma = 10, par.vals = list(C = 2)) print(cl1) #> Learner classif.ksvm from package kernlab #> Type: classif #> Name: Support Vector Machines; Short name: ksvm #> Class: classif.ksvm #> Properties: twoclass,multiclass,numerics,factors,prob,class.weights #> Predict-Type: response #> Hyperparameters: fit=FALSE,sigma=1 #>  # note the now set and altered hyperparameters: print(cl2) #> Learner classif.ksvm from package kernlab #> Type: classif #> Name: Support Vector Machines; Short name: ksvm #> Class: classif.ksvm #> Properties: twoclass,multiclass,numerics,factors,prob,class.weights #> Predict-Type: response #> Hyperparameters: fit=FALSE,sigma=10,C=2 #>"},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars2.html","id":null,"dir":"Reference","previous_headings":"","what":"Only exported for internal use. — setHyperPars2","title":"Only exported for internal use. — setHyperPars2","text":"exported internal use.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Only exported for internal use. — setHyperPars2","text":"","code":"setHyperPars2(learner, par.vals)"},{"path":"https://mlr.mlr-org.com/dev/reference/setHyperPars2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Only exported for internal use. — setHyperPars2","text":"learner (Learner) learner. par.vals (list) List named (hyper)parameter settings.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setId.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the id of a learner object. — setId","title":"Set the id of a learner object. — setId","text":"Deprecated, use setLearnerId instead.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setId.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the id of a learner object. — setId","text":"","code":"setId(learner, id)"},{"path":"https://mlr.mlr-org.com/dev/reference/setId.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the id of a learner object. — setId","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. id (character(1)) New id learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setId.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the id of a learner object. — setId","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setLearnerId.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the ID of a learner object. — setLearnerId","title":"Set the ID of a learner object. — setLearnerId","text":"Set ID learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setLearnerId.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the ID of a learner object. — setLearnerId","text":"","code":"setLearnerId(learner, id)"},{"path":"https://mlr.mlr-org.com/dev/reference/setLearnerId.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the ID of a learner object. — setLearnerId","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. id (character(1)) New ID learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setLearnerId.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the ID of a learner object. — setLearnerId","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setMeasurePars.html","id":null,"dir":"Reference","previous_headings":"","what":"Set parameters of performance measures — setMeasurePars","title":"Set parameters of performance measures — setMeasurePars","text":"Sets hyperparameters measures.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setMeasurePars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set parameters of performance measures — setMeasurePars","text":"","code":"setMeasurePars(measure, ..., par.vals = list())"},{"path":"https://mlr.mlr-org.com/dev/reference/setMeasurePars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set parameters of performance measures — setMeasurePars","text":"measure (Measure) Performance measure. ... () Named (hyper)parameters new settings. Alternatively can passed using par.vals argument. par.vals (list) Optional list named (hyper)parameter settings. arguments ... take precedence values list.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setMeasurePars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set parameters of performance measures — setMeasurePars","text":"Measure.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictThreshold.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the probability threshold the learner should use. — setPredictThreshold","title":"Set the probability threshold the learner should use. — setPredictThreshold","text":"See predict.threshold makeLearner setThreshold. complex wrappers top-level predict.type currently set.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictThreshold.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the probability threshold the learner should use. — setPredictThreshold","text":"","code":"setPredictThreshold(learner, predict.threshold)"},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictThreshold.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the probability threshold the learner should use. — setPredictThreshold","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. predict.threshold (numeric) Threshold produce class labels. named vector, names correspond class labels. binary classification can single numerical threshold positive class. See setThreshold details applied. Default NULL means 0.5 / equal threshold class.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictThreshold.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the probability threshold the learner should use. — setPredictThreshold","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictType.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the type of predictions the learner should return. — setPredictType","title":"Set the type of predictions the learner should return. — setPredictType","text":"Possible prediction types : Classification: Labels class probabilities (including labels). Regression: Numeric response standard errors (including numeric response). Survival: Linear predictor survival probability. complex wrappers predict type usually also passed encapsulated learner recursive fashion.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the type of predictions the learner should return. — setPredictType","text":"","code":"setPredictType(learner, predict.type)"},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the type of predictions the learner should return. — setPredictType","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. predict.type (character(1)) Classification: “response” “prob”. Regression: “response” “se”. Survival: “response” (linear predictor) “prob”. Clustering: “response” “prob”. Default “response”.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setPredictType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the type of predictions the learner should return. — setPredictType","text":"Learner.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setThreshold.html","id":null,"dir":"Reference","previous_headings":"","what":"Set threshold of prediction object. — setThreshold","title":"Set threshold of prediction object. — setThreshold","text":"Set threshold prediction object classification multilabel classification. Creates corresponding discrete class response newly set threshold. binary classification: positive class predicted probability value exceeds threshold. multiclass: Probabilities divided corresponding thresholds class maximum resulting value selected. result equivalent multi-threshold case values greater 0 sum 1. multilabel classification: label predicted (entry TRUE) probability matrix entry exceeds threshold corresponding label.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setThreshold.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set threshold of prediction object. — setThreshold","text":"","code":"setThreshold(pred, threshold)"},{"path":"https://mlr.mlr-org.com/dev/reference/setThreshold.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set threshold of prediction object. — setThreshold","text":"pred (Prediction) Prediction object. threshold (numeric) Threshold produce class labels. named vector, names correspond class labels. binary classification can single numerical threshold positive class.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/setThreshold.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set threshold of prediction object. — setThreshold","text":"(Prediction) changed threshold corresponding response.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/setThreshold.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set threshold of prediction object. — setThreshold","text":"","code":"# create task and train learner (LDA) task = makeClassifTask(data = iris, target = \"Species\") lrn = makeLearner(\"classif.lda\", predict.type = \"prob\") mod = train(lrn, task) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions  # predict probabilities and compute performance pred = predict(mod, newdata = iris) #> Error in predict(mod, newdata = iris): object 'mod' not found performance(pred, measures = mmce) #> Error in performance(pred, measures = mmce): object 'pred' not found head(as.data.frame(pred)) #> Error in as.data.frame(pred): object 'pred' not found  # adjust threshold and predict probabilities again threshold = c(setosa = 0.4, versicolor = 0.3, virginica = 0.3) pred = setThreshold(pred, threshold = threshold) #> Error in checkClass(x, classes, ordered, null.ok): object 'pred' not found performance(pred, measures = mmce) #> Error in performance(pred, measures = mmce): object 'pred' not found head(as.data.frame(pred)) #> Error in as.data.frame(pred): object 'pred' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/simplifyMeasureNames.html","id":null,"dir":"Reference","previous_headings":"","what":"Simplify measure names. — simplifyMeasureNames","title":"Simplify measure names. — simplifyMeasureNames","text":"Clips aggregation names character vector. E.g: 'mmce.test.mean' becomes 'mmce'. Elements contain measure name ignored returned unchanged.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/simplifyMeasureNames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simplify measure names. — simplifyMeasureNames","text":"","code":"simplifyMeasureNames(xs)"},{"path":"https://mlr.mlr-org.com/dev/reference/simplifyMeasureNames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simplify measure names. — simplifyMeasureNames","text":"xs (character) Character vector (possibly) contains aggregated measure names.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/simplifyMeasureNames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simplify measure names. — simplifyMeasureNames","text":"(character).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/smote.html","id":null,"dir":"Reference","previous_headings":"","what":"Synthetic Minority Oversampling Technique to handle class imbalancy in binary classification. — smote","title":"Synthetic Minority Oversampling Technique to handle class imbalancy in binary classification. — smote","text":"iteration, samples one minority class element x1, one x1's nearest neighbors: x2. points now interpolated / convex-combined, resulting new virtual data point x3 minority class. method handles factor features, . gower distance used nearest neighbor calculation, see cluster::daisy. interpolation, new factor level x3 sampled two given levels x1 x2 per feature.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/smote.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synthetic Minority Oversampling Technique to handle class imbalancy in binary classification. — smote","text":"","code":"smote(task, rate, nn = 5L, standardize = TRUE, alt.logic = FALSE)"},{"path":"https://mlr.mlr-org.com/dev/reference/smote.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synthetic Minority Oversampling Technique to handle class imbalancy in binary classification. — smote","text":"task (Task) task. rate (numeric(1)) Factor upsample smaller class. Must 1 Inf, 1 means oversampling 2 mean doubling class size. nn (integer(1)) Number nearest neighbors consider. Default 5. standardize (integer(1)) Standardize input variables calculating nearest neighbors data sets numeric input variables . mixed variables (numeric factor) gower distance used variables standardized anyway. Default TRUE. alt.logic (integer(1)) Use alternative logic selection minority class observations. Instead sampling minority class element one nearest neighbors, minority class element taken multiple times (depending rate) interpolation corresponding nearest neighbor sampled. Default FALSE.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/smote.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Synthetic Minority Oversampling Technique to handle class imbalancy in binary classification. — smote","text":"Task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/smote.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Synthetic Minority Oversampling Technique to handle class imbalancy in binary classification. — smote","text":"Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, P. (2000) SMOTE: Synthetic Minority -sampling TEchnique. International Conference Knowledge Based Computer Systems, pp. 46-57. National Center Software Technology, Mumbai, India, Allied Press.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/sonar.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Sonar classification task. — sonar.task","title":"Sonar classification task. — sonar.task","text":"Contains task (sonar.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/sonar.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sonar classification task. — sonar.task","text":"See mlbench::Sonar.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/spam.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Spam classification task. — spam.task","title":"Spam classification task. — spam.task","text":"Contains task (spam.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/spam.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Spam classification task. — spam.task","text":"See kernlab::spam.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/spatial.task.html","id":null,"dir":"Reference","previous_headings":"","what":"J. Muenchow's Ecuador landslide data set — spatial.task","title":"J. Muenchow's Ecuador landslide data set — spatial.task","text":"Data set created Jannes Muenchow, University Erlangen-Nuremberg, Germany. data cited Muenchow et al. (2012) (see reference ). publication also contains additional information data collection geomorphology area. data set provded (subset ) one 'natural' part RBSF area corresponds landslide distribution year 2000.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/spatial.task.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"J. Muenchow's Ecuador landslide data set — spatial.task","text":"data.frame point samples landslide non-landslide locations study area Andes southern Ecuador.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/spatial.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"J. Muenchow's Ecuador landslide data set — spatial.task","text":"Muenchow, J., Brenning, ., Richter, M., 2012. Geomorphic process rates landslides along humidity gradient tropical Andes. Geomorphology, 139-140: 271-284. Brenning, ., 2005. Spatial prediction models landslide hazards: review, comparison evaluation. Natural Hazards Earth System Sciences, 5(6): 853-862.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/subsetTask.html","id":null,"dir":"Reference","previous_headings":"","what":"Subset data in task. — subsetTask","title":"Subset data in task. — subsetTask","text":"See title.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/subsetTask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subset data in task. — subsetTask","text":"","code":"subsetTask(task, subset = NULL, features)"},{"path":"https://mlr.mlr-org.com/dev/reference/subsetTask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subset data in task. — subsetTask","text":"task (Task) task. subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used. features (character | integer | logical) Vector selected inputs. can either pass character vector feature names, vector indices, logical vector. case index vector element denotes position feature name returned getTaskFeatureNames. Note target feature always included resulting task, pass . Default use features.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/subsetTask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subset data in task. — subsetTask","text":"(Task). Task subsetted data.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/subsetTask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subset data in task. — subsetTask","text":"","code":"task = makeClassifTask(data = iris, target = \"Species\") subsetTask(task, subset = 1:100) #> Supervised task: iris #> Type: classif #> Target: Species #> Observations: 100 #> Features: #>    numerics     factors     ordered functionals  #>           4           0           0           0  #> Missings: FALSE #> Has weights: FALSE #> Has blocking: FALSE #> Has coordinates: FALSE #> Classes: 3 #>     setosa versicolor  virginica  #>         50         50          0  #> Positive class: NA"},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeColumns.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize columns of data.frame or task. — summarizeColumns","title":"Summarize columns of data.frame or task. — summarizeColumns","text":"Summarizes data.frame, somewhat differently normal summary function R. function mainly useful basic EDA tool data.frames converted tasks, can used tasks well. Columns can type numeric, integer, logical, factor, character. Characters logicals treated factors.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeColumns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize columns of data.frame or task. — summarizeColumns","text":"","code":"summarizeColumns(obj)"},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeColumns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize columns of data.frame or task. — summarizeColumns","text":"obj (data.frame | Task) Input data.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeColumns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize columns of data.frame or task. — summarizeColumns","text":"(data.frame). columns: name Name column. type Data type column. na Number NAs column. disp Measure dispersion, numerics integers sd used, categorical columns qualitative variation. mean Mean value column, NA categorical columns. median Median value column, NA categorical columns. mad MAD column, NA categorical columns. min Minimal value column, categorical columns size smallest category. max Maximal value column, categorical columns size largest category. nlevs categorical columns, number factor levels, NA else.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeColumns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize columns of data.frame or task. — summarizeColumns","text":"","code":"summarizeColumns(iris) #>           name    type na     mean      disp median     mad  min  max nlevs #> 1 Sepal.Length numeric  0 5.843333 0.8280661   5.80 1.03782  4.3  7.9     0 #> 2  Sepal.Width numeric  0 3.057333 0.4358663   3.00 0.44478  2.0  4.4     0 #> 3 Petal.Length numeric  0 3.758000 1.7652982   4.35 1.85325  1.0  6.9     0 #> 4  Petal.Width numeric  0 1.199333 0.7622377   1.30 1.03782  0.1  2.5     0 #> 5      Species  factor  0       NA 0.6666667     NA      NA 50.0 50.0     3"},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeLevels.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizes factors of a data.frame by tabling them. — summarizeLevels","title":"Summarizes factors of a data.frame by tabling them. — summarizeLevels","text":"Characters logicals treated factors.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeLevels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizes factors of a data.frame by tabling them. — summarizeLevels","text":"","code":"summarizeLevels(obj, cols = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeLevels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizes factors of a data.frame by tabling them. — summarizeLevels","text":"obj (data.frame | Task) Input data. cols (character) Restrict result columns cols. Default factor, character logical columns obj.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeLevels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarizes factors of a data.frame by tabling them. — summarizeLevels","text":"(list). Named list tables.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/summarizeLevels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizes factors of a data.frame by tabling them. — summarizeLevels","text":"","code":"summarizeLevels(iris) #> $Species #>  #>     setosa versicolor  virginica  #>         50         50         50  #>"},{"path":"https://mlr.mlr-org.com/dev/reference/train.html","id":null,"dir":"Reference","previous_headings":"","what":"Train a learning algorithm. — train","title":"Train a learning algorithm. — train","text":"Given Task, creates model learning machine can used predictions new data.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train a learning algorithm. — train","text":"","code":"train(learner, task, subset = NULL, weights = NULL)"},{"path":"https://mlr.mlr-org.com/dev/reference/train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train a learning algorithm. — train","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. task (Task) task. subset (integer | logical | NULL) Selected cases. Either logical index vector. default NULL observations used. weights (numeric) Optional, non-negative case weight vector used fitting. given, must length subset corresponding order. default NULL means weights used unless specified task (Task). Weights task overwritten.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train a learning algorithm. — train","text":"(WrappedModel).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train a learning algorithm. — train","text":"","code":"training.set = sample(seq_len(nrow(iris)), nrow(iris) / 2)  ## use linear discriminant analysis to classify iris data task = makeClassifTask(data = iris, target = \"Species\") learner = makeLearner(\"classif.lda\", method = \"mle\") mod = train(learner, task, subset = training.set) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions print(mod) #> Error in print(mod): object 'mod' not found  ## use random forest to classify iris data task = makeClassifTask(data = iris, target = \"Species\") learner = makeLearner(\"classif.rpart\", minsplit = 7, predict.type = \"prob\") mod = train(learner, task, subset = training.set) #> Error in x[0, , drop = FALSE]: incorrect number of dimensions print(mod) #> Error in print(mod): object 'mod' not found"},{"path":"https://mlr.mlr-org.com/dev/reference/trainLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Train an R learner. — trainLearner","title":"Train an R learner. — trainLearner","text":"Mainly internal use. Trains wrapped learner given training set. implement method want add another learner package.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/trainLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train an R learner. — trainLearner","text":"","code":"trainLearner(.learner, .task, .subset, .weights = NULL, ...)"},{"path":"https://mlr.mlr-org.com/dev/reference/trainLearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train an R learner. — trainLearner","text":".learner (RLearner) Wrapped learner. .task (Task) Task train learner . .subset (integer) Subset cases training set, index task . probably want use getTaskData purpose. .weights (numeric) Weights observation. ... () Additional (hyper)parameters, need passed underlying train function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/trainLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train an R learner. — trainLearner","text":"(). Model underlying learner.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/trainLearner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Train an R learner. — trainLearner","text":"implementation must adhere following: model must fitted subset .task given .subset. parameters ... must passed underlying training function.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParams.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter tuning. — tuneParams","title":"Hyperparameter tuning. — tuneParams","text":"Optimizes hyperparameters learner. Allows different optimization methods, grid search, evolutionary strategies, iterated F-race, etc. can select algorithm (settings) passing corresponding control object. complete list implemented algorithms look TuneControl. Multi-criteria tuning can done tuneParamsMultiCrit.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter tuning. — tuneParams","text":"","code":"tuneParams(   learner,   task,   resampling,   measures,   par.set,   control,   show.info = getMlrOption(\"show.info\"),   resample.fun = resample )"},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter tuning. — tuneParams","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. task (Task) task. resampling (ResampleInstance | ResampleDesc) Resampling strategy evaluate points hyperparameter space. pass description, instantiated beginning default, points evaluated training/test sets. want change behavior, look TuneControl. measures (list Measure | Measure) Performance measures evaluate. first measure, aggregated first aggregation function optimized, others simply evaluated. Default default measure task, see getDefaultMeasure. par.set (ParamHelpers::ParamSet) Collection parameters constraints optimization. Dependent parameters requires field must use quote expression define . control (TuneControl) Control object search method. Also selects optimization algorithm tuning. show.info (logical(1)) Print verbose output console? Default set via configureMlr. resample.fun (closure) function use resampling. Defaults resample. user-given function used instead, take arguments “learner”, “task”, “resampling”, “measures”, “show.info”; see resample. Within function, easiest call resample possibly modify result. However, possible return list following essential slots: “aggr” slot general tuning, additionally “pred” slot threshold tuning performed (see TuneControl), “err.msgs” “err.dumps” slots error reporting. parameter must default mbo tuning performed.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hyperparameter tuning. — tuneParams","text":"(TuneResult).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParams.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Hyperparameter tuning. — tuneParams","text":"like include results training data set, make sure appropriately adjust resampling strategy aggregation measure. See example code .","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParams.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter tuning. — tuneParams","text":"","code":"set.seed(123) # a grid search for an SVM (with a tiny number of points...) # note how easily we can optimize on a log-scale ps = makeParamSet(   makeNumericParam(\"C\", lower = -12, upper = 12, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x) ) ctrl = makeTuneControlGrid(resolution = 2L) rdesc = makeResampleDesc(\"CV\", iters = 2L) res = tuneParams(\"classif.ksvm\", iris.task, rdesc, par.set = ps, control = ctrl) #> [Tune] Started tuning learner classif.ksvm for parameter set: #>          Type len Def    Constr Req Tunable Trafo #> C     numeric   -   - -12 to 12   -    TRUE     Y #> sigma numeric   -   - -12 to 12   -    TRUE     Y #> With control class: TuneControlGrid #> Imputation value: 1 #> [Tune-x] 1: C=0.000244; sigma=0.000244 #> [Tune-y] 1: mmce.test.mean=0.7333333; time: 0.0 min #> [Tune-x] 2: C=4.1e+03; sigma=0.000244 #> [Tune-y] 2: mmce.test.mean=0.0533333; time: 0.0 min #> [Tune-x] 3: C=0.000244; sigma=4.1e+03 #> [Tune-y] 3: mmce.test.mean=0.7333333; time: 0.0 min #> [Tune-x] 4: C=4.1e+03; sigma=4.1e+03 #> [Tune-y] 4: mmce.test.mean=0.7333333; time: 0.0 min #> [Tune] Result: C=4.1e+03; sigma=0.000244 : mmce.test.mean=0.0533333 print(res) #> Tune result: #> Op. pars: C=4.1e+03; sigma=0.000244 #> mmce.test.mean=0.0533333 # access data for all evaluated points df = as.data.frame(res$opt.path) df1 = as.data.frame(res$opt.path, trafo = TRUE) print(head(df[, -ncol(df)])) #>     C sigma mmce.test.mean dob eol error.message #> 1 -12   -12     0.73333333   1  NA          <NA> #> 2  12   -12     0.05333333   2  NA          <NA> #> 3 -12    12     0.73333333   3  NA          <NA> #> 4  12    12     0.73333333   4  NA          <NA> print(head(df1[, -ncol(df)])) #>     C sigma mmce.test.mean dob eol error.message #> 1 -12   -12     0.73333333   1  NA          <NA> #> 2  12   -12     0.05333333   2  NA          <NA> #> 3 -12    12     0.73333333   3  NA          <NA> #> 4  12    12     0.73333333   4  NA          <NA> # access data for all evaluated points - alternative df2 = generateHyperParsEffectData(res) df3 = generateHyperParsEffectData(res, trafo = TRUE) print(head(df2$data[, -ncol(df2$data)])) #>     C sigma mmce.test.mean iteration #> 1 -12   -12     0.73333333         1 #> 2  12   -12     0.05333333         2 #> 3 -12    12     0.73333333         3 #> 4  12    12     0.73333333         4 print(head(df3$data[, -ncol(df3$data)])) #>              C        sigma mmce.test.mean iteration #> 1 2.441406e-04 2.441406e-04     0.73333333         1 #> 2 4.096000e+03 2.441406e-04     0.05333333         2 #> 3 2.441406e-04 4.096000e+03     0.73333333         3 #> 4 4.096000e+03 4.096000e+03     0.73333333         4 if (FALSE) { # we optimize the SVM over 3 kernels simultanously # note how we use dependent params (requires = ...) and iterated F-racing here ps = makeParamSet(   makeNumericParam(\"C\", lower = -12, upper = 12, trafo = function(x) 2^x),   makeDiscreteParam(\"kernel\", values = c(\"vanilladot\", \"polydot\", \"rbfdot\")),   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x,     requires = quote(kernel == \"rbfdot\")),   makeIntegerParam(\"degree\", lower = 2L, upper = 5L,     requires = quote(kernel == \"polydot\")) ) print(ps) ctrl = makeTuneControlIrace(maxExperiments = 5, nbIterations = 1, minNbSurvival = 1) rdesc = makeResampleDesc(\"Holdout\") res = tuneParams(\"classif.ksvm\", iris.task, rdesc, par.set = ps, control = ctrl) print(res) df = as.data.frame(res$opt.path) print(head(df[, -ncol(df)]))  # include the training set performance as well rdesc = makeResampleDesc(\"Holdout\", predict = \"both\") res = tuneParams(\"classif.ksvm\", iris.task, rdesc, par.set = ps,   control = ctrl, measures = list(mmce, setAggregation(mmce, train.mean))) print(res) df2 = as.data.frame(res$opt.path) print(head(df2[, -ncol(df2)])) }"},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParamsMultiCrit.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter tuning for multiple measures at once. — tuneParamsMultiCrit","title":"Hyperparameter tuning for multiple measures at once. — tuneParamsMultiCrit","text":"Optimizes hyperparameters learner multi-criteria fashion. Allows different optimization methods, grid search, evolutionary strategies, etc. can select algorithm (settings) passing corresponding control object. complete list implemented algorithms look TuneMultiCritControl.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParamsMultiCrit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter tuning for multiple measures at once. — tuneParamsMultiCrit","text":"","code":"tuneParamsMultiCrit(   learner,   task,   resampling,   measures,   par.set,   control,   show.info = getMlrOption(\"show.info\"),   resample.fun = resample )"},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParamsMultiCrit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter tuning for multiple measures at once. — tuneParamsMultiCrit","text":"learner (Learner | character(1)) learner. pass string learner created via makeLearner. task (Task) task. resampling (ResampleInstance | ResampleDesc) Resampling strategy evaluate points hyperparameter space. pass description, instantiated beginning default, points evaluated training/test sets. want change behavior, look TuneMultiCritControl. measures [list Measure) Performance measures optimize simultaneously. par.set (ParamHelpers::ParamSet) Collection parameters constraints optimization. Dependent parameters requires field must use quote expression define . control (TuneMultiCritControl) Control object search method. Also selects optimization algorithm tuning. show.info (logical(1)) Print verbose output console? Default set via configureMlr. resample.fun (closure) function use resampling. Defaults resample take arguments , return result type , resample.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParamsMultiCrit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hyperparameter tuning for multiple measures at once. — tuneParamsMultiCrit","text":"(TuneMultiCritResult).","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/tuneParamsMultiCrit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter tuning for multiple measures at once. — tuneParamsMultiCrit","text":"","code":"# \\donttest{ # multi-criteria optimization of (tpr, fpr) with NGSA-II lrn = makeLearner(\"classif.ksvm\") rdesc = makeResampleDesc(\"Holdout\") ps = makeParamSet(   makeNumericParam(\"C\", lower = -12, upper = 12, trafo = function(x) 2^x),   makeNumericParam(\"sigma\", lower = -12, upper = 12, trafo = function(x) 2^x) ) ctrl = makeTuneMultiCritControlNSGA2(popsize = 4L, generations = 1L) res = tuneParamsMultiCrit(lrn, sonar.task, rdesc, par.set = ps,   measures = list(tpr, fpr), control = ctrl) #> [Tune] Started tuning learner classif.ksvm for parameter set: #>          Type len Def    Constr Req Tunable Trafo #> C     numeric   -   - -12 to 12   -    TRUE     Y #> sigma numeric   -   - -12 to 12   -    TRUE     Y #> With control class: TuneMultiCritControlNSGA2 #> Imputation value: -0Imputation value: 1 #> [Tune-x] 1: C=0.0036; sigma=0.207 #> [Tune-y] 1: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 2: C=0.072; sigma=0.244 #> [Tune-y] 2: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 3: C=0.0384; sigma=2.73 #> [Tune-y] 3: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 4: C=0.00326; sigma=1.99e+03 #> [Tune-y] 4: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 5: C=0.338; sigma=0.207 #> [Tune-y] 5: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 6: C=0.00335; sigma=0.244 #> [Tune-y] 6: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 7: C=0.0882; sigma=0.145 #> [Tune-y] 7: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune-x] 8: C=0.00267; sigma=1.99e+03 #> [Tune-y] 8: tpr.test.mean=1.0000000,fpr.test.mean=1.0000000; time: 0.0 min #> [Tune] Result: Points on front : 8 plotTuneMultiCritResult(res, path = TRUE)  # }"},{"path":"https://mlr.mlr-org.com/dev/reference/tuneThreshold.html","id":null,"dir":"Reference","previous_headings":"","what":"Tune prediction threshold. — tuneThreshold","title":"Tune prediction threshold. — tuneThreshold","text":"Optimizes threshold predictions based probabilities. Works classification multilabel tasks. Uses BBmisc::optimizeSubInts normal binary class problems GenSA::GenSA multiclass multilabel problems.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneThreshold.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tune prediction threshold. — tuneThreshold","text":"","code":"tuneThreshold(pred, measure, task, model, nsub = 20L, control = list())"},{"path":"https://mlr.mlr-org.com/dev/reference/tuneThreshold.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tune prediction threshold. — tuneThreshold","text":"pred (Prediction) Prediction object. measure (Measure) Performance measure optimize. Default default measure task. task (Task) Learning task. Rarely neeeded, required performance measure. model (WrappedModel) Fitted model. Rarely neeeded, required performance measure. nsub (integer(1)) Passed BBmisc::optimizeSubInts 2class problems. Default 20. control (list) Control object GenSA::GenSA used. Default empty list.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/tuneThreshold.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tune prediction threshold. — tuneThreshold","text":"(list). named list following components: th optimal threshold, perf performance value.","code":""},{"path":[]},{"path":"https://mlr.mlr-org.com/dev/reference/wpbc.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Wisonsin Prognostic Breast Cancer (WPBC) survival task. — wpbc.task","title":"Wisonsin Prognostic Breast Cancer (WPBC) survival task. — wpbc.task","text":"Contains task (wpbc.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/wpbc.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wisonsin Prognostic Breast Cancer (WPBC) survival task. — wpbc.task","text":"See TH.data::wpbc. Incomplete cases removed task.","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/yeast.task.html","id":null,"dir":"Reference","previous_headings":"","what":"Yeast multilabel classification task. — yeast.task","title":"Yeast multilabel classification task. — yeast.task","text":"Contains task (yeast.task).","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/yeast.task.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Yeast multilabel classification task. — yeast.task","text":"https://archive.ics.uci.edu/ml/datasets/Yeast (long instead wide format)","code":""},{"path":"https://mlr.mlr-org.com/dev/reference/yeast.task.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Yeast multilabel classification task. — yeast.task","text":"Elisseeff, ., & Weston, J. (2001): kernel method multi-labelled classification. Advances neural information processing systems (pp. 681-687).","code":""}]
